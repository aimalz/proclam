{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hideme"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hideme"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "# print(mpl.rcParams.items)\n",
    "mpl.use('Agg')\n",
    "mpl.rcParams['text.usetex'] = False\n",
    "mpl.rcParams['mathtext.rm'] = 'serif'\n",
    "mpl.rcParams['font.family'] = ['serif']\n",
    "mpl.rcParams['font.serif'] = ['Times New Roman']\n",
    "# mpl.rcParams['font.family'] = ['Times New Roman']\n",
    "mpl.rcParams['axes.titlesize'] = 16\n",
    "mpl.rcParams['axes.labelsize'] = 14\n",
    "mpl.rcParams['savefig.dpi'] = 250\n",
    "mpl.rcParams['figure.dpi'] = 250\n",
    "mpl.rcParams['savefig.format'] = 'pdf'\n",
    "mpl.rcParams['savefig.bbox'] = 'tight'\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# print(mpl.rcParams.items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ![](./header.png) -->\n",
    "<img src=\"./header.png\",width=100%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Photometric LSST Astronomical Time-series Classification Challenge (PLAsTiCC): Selection of a performance metric\n",
    "\n",
    "*Alex Malz (NYU)*, *Renee Hlozek (U. Toronto)*, *Tarek Alam (UCL)*, *Anita Bahmanyar (U. Toronto)*, *Rahul Biswas (U. Stockholm)*, *Rafael Martinez-Galarza (Harvard)*, *Gautham Narayan (STScI)*\n",
    "\n",
    "We describe and illustrate the process by which a global performance metric was chosen for Photometric LSST Astronomical Time-series Classification Challenge (PLAsTiCC), a Kaggle competition aiming to identify promising transient and variable classifiers for LSST by involving the broader community outside astronomy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction\n",
    "============\n",
    "\n",
    "The metric of this note is for the first version of the Kaggle competition, though there are future plans for an early classification challenge and identification of class-specific metrics for different science goals.  \n",
    "\n",
    "* The metric must return a single scalar value.\n",
    "* The metric must be well-defined for non-binary classes.\n",
    "* The metric must balance diverse science use cases in the presence of heavily nonuniform class prevalence.\n",
    "* The metric must respect the information content of probabilistic classifications.\n",
    "* The metric must be able to evaluate deterministic classifications.\n",
    "* The metric must be interpretable, meaning it gives a more optimal value for \"good\" mock classifiers and a less optimal value for mock classifiers plagued by anticipated systematic errors; in other words, it must pass basic tests of intuition.\n",
    "* The metric must be reliable, giving consistent results for different instantiations of the same test case.\n",
    "\n",
    "The Probabilistic Classification Metric (ProClaM) code used in this exploration of performance metrics is publicly available on [GitHub](https://github.com/aimalz/proclam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hideme"
    ]
   },
   "outputs": [],
   "source": [
    "import proclam\n",
    "from proclam import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data\n",
    "====\n",
    "\n",
    "We confirm the behavior of the metrics on mock data with well-understood systematics as well as real data from past classification challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mock classifier systematics\n",
    "\n",
    "* guessing: random classifications across all classes\n",
    "* uncertain: uniform probabilities across all classes\n",
    "* perfect: perfectly accurate on all classes\n",
    "* almost: a slight perturbation of the perfect classifier\n",
    "* noisy: a large perturbation of the perfect classifier\n",
    "* tunnel vision: classifies one class well and others randomly\n",
    "* cruise control: classifies all objects as a single class\n",
    "* subsumed: consistently misclassifies one class as one other class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hideme"
    ]
   },
   "outputs": [],
   "source": [
    "plasticc = {}\n",
    "plasticc['label'] = 'ProClaM'\n",
    "plasticc['names'] = []\n",
    "plasticc['cm'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hideme"
    ]
   },
   "outputs": [],
   "source": [
    "M_classes = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hideme"
    ]
   },
   "outputs": [],
   "source": [
    "chosen = np.random.randint(0, M_classes)\n",
    "print(chosen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hideme"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_cm_from_cm(cm, text):\n",
    "    plt.matshow(cm, vmin=0., vmax=1.)\n",
    "    plt.title(text)\n",
    "    plt.xlabel('predicted class')\n",
    "    plt.ylabel('true class')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "def wrap_up_classifier(cm, testname, info_dict):\n",
    "    cm = cm / np.sum(cm, axis=1)[:, np.newaxis]\n",
    "    plot_cm_from_cm(cm, testname)\n",
    "    info_dict['names'].append(testname)\n",
    "    info_dict['cm'][testname] = cm\n",
    "    return info_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guessing classifier\n",
    "\n",
    "Totally random CM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hidein"
    ]
   },
   "outputs": [],
   "source": [
    "cm = np.random.uniform(size=(M_classes, M_classes))\n",
    "plasticc = wrap_up_classifier(cm, 'Guessing', plasticc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncertain\n",
    "\n",
    "Totally uniform CM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hidein"
    ]
   },
   "outputs": [],
   "source": [
    "cm = np.ones((M_classes, M_classes))\n",
    "plasticc = wrap_up_classifier(cm, 'Uncertain', plasticc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perfect classifier\n",
    "\n",
    "Identity matrix CM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hidein"
    ]
   },
   "outputs": [],
   "source": [
    "cm = np.eye(M_classes)\n",
    "plasticc = wrap_up_classifier(cm, 'Perfect', plasticc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Almost perfect classifier\n",
    "\n",
    "Identity matrix CM plus low-amplitude uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hidein"
    ]
   },
   "outputs": [],
   "source": [
    "cm = np.eye(M_classes) + 0.1 * np.ones((M_classes, M_classes))\n",
    "plasticc = wrap_up_classifier(cm, 'Almost', plasticc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noisy classifier\n",
    "\n",
    "Identity matrix CM plus high-amplitude uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hidein"
    ]
   },
   "outputs": [],
   "source": [
    "cm = np.eye(M_classes) + 0.5 * np.ones((M_classes, M_classes))\n",
    "plasticc = wrap_up_classifier(cm, 'Noisy', plasticc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tunnel vision classifier\n",
    "\n",
    "accurate predictions on one class and uniform on others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hidein"
    ]
   },
   "outputs": [],
   "source": [
    "cm = np.ones((M_classes, M_classes))\n",
    "cm = cm * 0.1\n",
    "cm[:, chosen] = np.zeros((M_classes))[np.newaxis, :]\n",
    "cm[chosen][chosen] = 1.\n",
    "plasticc = wrap_up_classifier(cm, 'Tunnel', plasticc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cruise control classifier\n",
    "\n",
    "always predict one class regardless of true class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hidein"
    ]
   },
   "outputs": [],
   "source": [
    "cm = np.ones((M_classes, M_classes))\n",
    "cm = cm * 0.1\n",
    "cm[:, chosen] = 1.\n",
    "plasticc = wrap_up_classifier(cm, 'Cruise', plasticc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsumed classifiers\n",
    "\n",
    "Subsumed to: the chosen class is consistently misclassified as a different class\n",
    "\n",
    "Subsumed from: another class is consistently misclassified as the chosen class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hidein"
    ]
   },
   "outputs": [],
   "source": [
    "cm = plasticc['cm']['Almost'].copy()\n",
    "cm[chosen] = cm[chosen-1]\n",
    "plasticc = wrap_up_classifier(cm, 'SubsumedTo', plasticc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hidein"
    ]
   },
   "outputs": [],
   "source": [
    "cm = plasticc['cm']['Almost'].copy()\n",
    "cm[chosen-1] = cm[chosen]\n",
    "plasticc = wrap_up_classifier(cm, 'SubsumedFrom', plasticc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real classification results\n",
    "\n",
    "* SNPhotCC \\[from Michelle?\\]\n",
    "* \\[Ashish's data?\\]\n",
    "* \\[Renee's data?\\]\n",
    "\n",
    "*show confusion matrices*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hideme"
    ]
   },
   "outputs": [],
   "source": [
    "def make_class_pairs(data_info_dict):\n",
    "    return zip(data_info_dict['classifications'], data_info_dict['truth_tables'])\n",
    "\n",
    "def make_file_locs(data_info_dict):\n",
    "    names = data_info_dict['names']\n",
    "    data_info_dict['dirname'] = topdir + data_info_dict['label'] + '/'\n",
    "    data_info_dict['classifications'] = ['%s/predicted_prob_%s.csv'%(name, name) for name in names]\n",
    "    data_info_dict['truth_tables'] = ['%s/truth_table_%s.csv'%(name, name) for name in names]\n",
    "#     print(data_info_dict)\n",
    "    return data_info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hideme"
    ]
   },
   "outputs": [],
   "source": [
    "mystery = {}\n",
    "mystery['label'] = 'Unknown'\n",
    "mystery['names'] = ['RandomForest', 'KNeighbors', 'MLPNeuralNet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hideme"
    ]
   },
   "outputs": [],
   "source": [
    "snphotcc = {}\n",
    "snphotcc['label'] = 'SNPhotCC'\n",
    "prefixes = ['Templates', 'Wavelets']\n",
    "suffixes = ['BoostForest', 'KNN', 'NB', 'NeuralNetwork', 'SVM']\n",
    "snphotcc['names'] = []\n",
    "for prefix in prefixes:\n",
    "    for suffix in suffixes:\n",
    "        snphotcc['names'].append(prefix+suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hideme"
    ]
   },
   "outputs": [],
   "source": [
    "topdir = '../examples/'\n",
    "for dataset in [mystery, snphotcc]:\n",
    "    dataset = make_file_locs(dataset)\n",
    "    dataset['class_pairs'] = make_class_pairs(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hideme"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_cm(probs, truth, text, loc=''):\n",
    "    cm = proclam.metrics.util.prob_to_cm(probs, truth)\n",
    "    plt.matshow(cm.T, vmin=0., vmax=1.)\n",
    "# plt.xticks(range(max(truth)+1), names)\n",
    "# plt.yticks(range(max(truth)+1), names)\n",
    "    plt.xlabel('predicted class')\n",
    "    plt.ylabel('true class')\n",
    "    plt.colorbar()\n",
    "    plt.title(text)\n",
    "    plt.show()\n",
    "#     plt.savefig(loc+name+'_cm.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hideme"
    ]
   },
   "outputs": [],
   "source": [
    "def process_strings(dataset, cc):\n",
    "    loc = dataset['dirname']\n",
    "    text = dataset['label'] + ' ' + dataset['names'][cc]\n",
    "    return loc, text\n",
    "\n",
    "def just_read_class_pairs(pair, dataset, cc):\n",
    "    loc, text = process_strings(dataset, cc)\n",
    "    clfile = pair[0]\n",
    "    truthfile = pair[1]\n",
    "    prob_mat = pd.read_csv(loc + clfile, delim_whitespace=True).values\n",
    "    nobj = np.shape(prob_mat)[0]\n",
    "    nclass = np.shape(prob_mat)[1]\n",
    "    truth_values = pd.read_csv(loc + truthfile, delim_whitespace=True).values\n",
    "    nobj_truth = np.shape(truth_values)[0]\n",
    "    nclass_truth = np.shape(truth_values)[1]\n",
    "    tvec = np.where(truth_values==1)[1]\n",
    "    pmat = prob_mat\n",
    "    return pmat, tvec\n",
    "    \n",
    "def read_class_pairs(pair, dataset, cc):\n",
    "    fileloc, text = process_strings(dataset, cc)\n",
    "    pmat, tvec = just_read_class_pairs(pair, dataset, cc)\n",
    "    plot_cm(pmat, tvec, text, loc=fileloc + dataset['names'][cc] + '/')\n",
    "    return pmat, tvec\n",
    "\n",
    "def just_plot_cm(dataset, cc, pmat, tvec):\n",
    "    fileloc, text = process_strings(dataset, cc)\n",
    "    plot_cm(pmat, tvec, text, loc=fileloc + dataset['names'][cc] + '/')\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods (Metrics)\n",
    "======\n",
    "\n",
    "We considered two metrics of classification probabilities, each of which is interpretable and avoids reducing probabilities to point estimates\n",
    "\n",
    "The Brier score is defined as\n",
    "\\begin{eqnarray*}\n",
    "B &=& \\sum_{m=1}^{M}\\frac{w_{m}}{N_{m}}\\sum_{n=1}^{N_{m}}\\left((1-p_{n}(m | m))^{2}+\\sum_{m'\\neq m}^{M}(p_{n}(m' | m))^{2}\\right)\n",
    "\\end{eqnarray*}\n",
    "\n",
    "The log-loss is defined as\n",
    "\\begin{eqnarray*}\n",
    "L &=& -\\sum_{m=1}^{M}\\frac{w_{m}}{N_{m}}\\sum_{n=1}^{N_{m}}\\ln[p_{n}(m | m)]\n",
    "\\end{eqnarray*}\n",
    "\n",
    "We calculate the metric within each class by taking an average of its value for each true member of the class.  Then we weight the metrics for each class by an arbitrary weight vector and take a weighted average of the per-class metrics to produce a global scalar metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hideme"
    ]
   },
   "outputs": [],
   "source": [
    "metricslist = ['Brier', 'LogLoss']\n",
    "colors = ['b', 'r']\n",
    "markerlist = ['o', 's', '*']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights\n",
    "\n",
    "We may take weighted averages of the per-class metrics, and these weights may be considered in terms of the systematics we discussed, by upweighting or downweighting the \"chosen\" class most affected by the systematics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_weight = np.ones(M_classes)\n",
    "hi_weight = np.ones(M_classes) / np.float(M_classes)\n",
    "hi_weight[chosen] = 1.\n",
    "lo_weight = np.ones(M_classes) \n",
    "lo_weight[chosen] = 1. / np.float(M_classes)\n",
    "all_weights = {}\n",
    "all_weights['flat'] = flat_weight\n",
    "all_weights['up'] = hi_weight\n",
    "all_weights['down'] = lo_weight\n",
    "all_weights['per_class'] = 'per_class'\n",
    "all_weights['per_item'] = 'per_item'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results\n",
    "=======\n",
    "\n",
    "*one plot per set of \"true\" classes: classifiers on x axis, metrics on y axes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hideme"
    ]
   },
   "outputs": [],
   "source": [
    "def make_patch_spines_invisible(ax):\n",
    "    ax.set_frame_on(True)\n",
    "    ax.patch.set_visible(False)\n",
    "    for sp in ax.spines.values():\n",
    "        sp.set_visible(False)\n",
    "        \n",
    "def per_metric_helper(ax, n, data, metric_names, codes, shapes, colors):\n",
    "    plot_n = n+1\n",
    "    in_x = np.arange(len(codes))\n",
    "    ax_n = ax\n",
    "    n_factor = 0.1 * (plot_n - 2)\n",
    "    if plot_n>1:\n",
    "        ax_n = ax.twinx()\n",
    "        rot_ang = 270\n",
    "        label_space = 15.\n",
    "    else:\n",
    "        rot_ang = 90\n",
    "        label_space = 0.\n",
    "    if plot_n>2:\n",
    "        ax_n.spines[\"right\"].set_position((\"axes\", 1. + 0.1 * (plot_n-1)))\n",
    "        make_patch_spines_invisible(ax_n)\n",
    "        ax_n.spines[\"right\"].set_visible(True)\n",
    "    handle = ax_n.scatter(in_x+n_factor*np.ones_like(data[n]), data[n], marker=shapes[n], s=10, color=colors[n], label=metric_names[n])\n",
    "    ax_n.set_ylabel(metric_names[n], rotation=rot_ang, fontsize=14, labelpad=label_space)\n",
    "#     ax_n.set_ylim(0.9 * min(data[n]), 1.1 * max(data[n]))\n",
    "    return(ax, ax_n, handle)\n",
    "\n",
    "def metric_plot(dataset, res, metric_names, shapes, colors, modtext=''):\n",
    "    codes = dataset['names']\n",
    "    data = res\n",
    "    text = dataset['label']\n",
    "#     fileloc = dataset['dirname']+dataset['label']+'_results.png'\n",
    "    xs = np.arange(len(codes))\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.subplots_adjust(right=1.)\n",
    "    handles = []\n",
    "    for n in range(len(metric_names)):\n",
    "#         print(np.shape(data[n]))\n",
    "        (ax, ax_n, handle) = per_metric_helper(ax, n, data, metric_names, codes, shapes, colors)\n",
    "        handles.append(handle)\n",
    "    plt.xticks(xs, codes)\n",
    "    for tick in ax.get_xticklabels():\n",
    "        tick.set_rotation(90)\n",
    "    plt.xlabel('Classifiers', fontsize=14)\n",
    "    plt.legend(handles, metric_names, loc='lower left')\n",
    "    fig.suptitle(text+modtext)\n",
    "#     plt.savefig(fileloc)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mock classifier systematics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hideme"
    ]
   },
   "outputs": [],
   "source": [
    "generator = proclam.simulators.LogUnbalanced()\n",
    "N_objects = 10000\n",
    "truth = generator.simulate(M_classes, N_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hidein"
    ]
   },
   "outputs": [],
   "source": [
    "d = np.diff(np.unique(truth)).min()\n",
    "left_of_first_bin = truth.min() - float(d)/2\n",
    "right_of_last_bin = truth.max() + float(d)/2\n",
    "plt.hist(truth, np.arange(left_of_first_bin, right_of_last_bin + d, d), log=True, alpha=0.5)\n",
    "# plt.xticks(range(max(truth)+1), names)\n",
    "plt.hist(truth, log=True, alpha=0.5)\n",
    "plt.ylabel('counts')\n",
    "plt.xlabel('class')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hideme"
    ]
   },
   "outputs": [],
   "source": [
    "# data = np.empty((len(metricslist), len(plasticc['names'])))\n",
    "plasticc['probs'] = {}\n",
    "for cc, name in enumerate(plasticc['names']):\n",
    "    code = proclam.classifiers.FromCM()\n",
    "    probs = code.classify(plasticc['cm'][name], truth, other=False)\n",
    "    plasticc['probs'][name] = probs\n",
    "#     for count, metric in enumerate(metricslist):\n",
    "#         D = getattr(proclam.metrics, metric)()\n",
    "#         hm = D.evaluate(probs, truth, averaging='per_class')\n",
    "#         data[count][cc] = hm\n",
    "#     plasticc['probs'] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "hidein"
    ]
   },
   "outputs": [],
   "source": [
    "for wt in all_weights.keys():\n",
    "    data = np.empty((len(metricslist), len(plasticc['names'])))\n",
    "    for cc, name in enumerate(plasticc['names']):\n",
    "        probs = plasticc['probs'][name]\n",
    "        for count, metric in enumerate(metricslist):\n",
    "            D = getattr(proclam.metrics, metric)()\n",
    "            hm = D.evaluate(probs, truth, averaging=all_weights[wt])\n",
    "            data[count][cc] = hm\n",
    "#     plasticc['results'] = data\n",
    "    metric_plot(plasticc, data, metricslist, markerlist, colors, modtext=' '+wt+'weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Would like to do this many times to generate error bars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try with different weights relative to randomly chosen class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real classification results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hideme"
    ]
   },
   "outputs": [],
   "source": [
    "for dataset in [mystery, snphotcc]:\n",
    "    data = np.empty((len(metricslist), len(dataset['names'])))\n",
    "    for cc, pair in enumerate(dataset['class_pairs']):\n",
    "        probm, truthv = read_class_pairs(pair, dataset, cc)\n",
    "        for count, metric in enumerate(metricslist):\n",
    "            D = getattr(proclam.metrics, metric)()\n",
    "            hm = D.evaluate(probm, truthv)\n",
    "            data[count][cc] = hm\n",
    "#     dataset['results'] = data\n",
    "    metric_plot(dataset, data, metricslist, markerlist, colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hideme"
    ]
   },
   "outputs": [],
   "source": [
    "# metric_plot(snphotcc, metricslist, markerlist, colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hideme"
    ]
   },
   "outputs": [],
   "source": [
    "# metric_plot(mystery, metricslist, markerlist, colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions\n",
    "===========\n",
    "\n",
    "We conclude that the Brier and log-loss metrics convey different information but are more or less consistent with our intuition for what makes a good classifier.  The Brier metric includes a penalty term not present in the log-loss but somehow is always consistent with the log-loss, meaning the penalty term doesn't really make a difference.  The log-loss has a larger dynamic range, which seems good but probably isn't that big a deal either."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acknowledgments\n",
    "===============\n",
    "\n",
    "The DESC acknowledges ongoing support from the Institut National de Physique Nucleaire et de Physique des Particules in France; the Science & Technology Facilities Council in the United Kingdom; and the Department of Energy, the National Science Foundation, and the LSST Corporation in the United States.\n",
    "\n",
    "DESC uses resources of the IN2P3 Computing Center (CC-IN2P3--Lyon/Villeurbanne - France) funded by the Centre National de la Recherche Scientifique; the National Energy Research Scientific Computing Center, a DOE Office of Science User Facility supported by the Office of Science of the U.S. Department of Energy under Contract No. DE-AC02-05CH11231; STFC DiRAC HPC Facilities, funded by UK BIS National E-infrastructure capital grants; and the UK particle physics grid, supported by the GridPP Collaboration.\n",
    "\n",
    "This work was performed in part under DOE Contract DE-AC02-76SF00515."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contributions\n",
    "=======\n",
    "\n",
    "Alex Malz: conceptualization, data curation, formal analysis, investigation, methodology, project administration, software, supervision, validation, visualization, writing - original draft\n",
    "\n",
    "Renee Hlozek: data curation, formal analysis, funding acquisition, investigation, project administration, software, supervision, validation, visualization, writing - original draft\n",
    "\n",
    "Tarek Alam: investigation, software, validation\n",
    "\n",
    "Anita Bahmanyar: formal analysis, investigation, methodology, software, writing - original draft\n",
    "\n",
    "Rahul Biswas: conceptualization, methodology, software\n",
    "\n",
    "Rafael Martinez-Galarza: data curation, software, visualization\n",
    "\n",
    "Gautham Narayan: data curation, formal analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hideme"
    ]
   },
   "outputs": [],
   "source": [
    "# cells with a tag of \"hideme\" will not appear in html resulting from:\n",
    "# jupyter nbconvert desc_note/main.ipynb --TagRemovePreprocessor.remove_cell_tags='[\"hideme\"]'\n",
    "# jupyter nbconvert desc_note/main.ipynb --TagRemovePreprocessor.remove_input_tags='[\"hidein\"]'\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
