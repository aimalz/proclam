{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Photometric LSST Astronomical Time-series Classification Challenge (PLAsTiCC): code that runs the performance metric\n",
    "\n",
    "*Alex Malz (NYU)*, *Renee Hlozek (U. Toronto)*, *Tarek Alam (UCL)*, *Anita Bahmanyar (U. Toronto)*, *Rahul Biswas (U. Stockholm)*, *Rafael Martinez-Galarza (Harvard)*, *Gautham Narayan (STScI)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# This is the code available on GitHub for calculating metrics,\n",
    "# as well as performing other diagnostics on probability tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_class_pairs(data_info_dict):\n",
    "    \"\"\"\n",
    "    Paris the paths to classifier output and truth tables for each classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_info_dict: dictionary\n",
    "       \n",
    "    Returns\n",
    "    -------\n",
    "    data_info_dict: dictionary\n",
    "        updated keywords: class_pairs, dict - classifier: [path_to_class_output, path_to_truth_tables] \n",
    "    \"\"\"\n",
    "    \n",
    "    for name in data_info_dict['names']:\n",
    "        data_info_dict['class_pairs'][name] = [data_info_dict['classifications'][name], data_info_dict['truth_tables'][name]]\n",
    "    \n",
    "    return(data_info_dict['class_pairs'])\n",
    "        \n",
    "def make_file_locs(data_info_dict):\n",
    "    \"\"\"\n",
    "    Set paths to data directory, classifier output and truth tables.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_info_dict: dictionary                  \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    data_info_dict: dictionary\n",
    "        updated keywords: dirname - data directory, str\n",
    "                          classifications, dict - classifier: path to classifier output, str\n",
    "                          truth_tables, dict - classifier: path to truth tables - str\n",
    "    \"\"\"\n",
    "    \n",
    "    # get the names of classifiers to be considered\n",
    "    names = data_info_dict['names']\n",
    "    \n",
    "    # set data directory\n",
    "    data_info_dict['dirname'] = topdir + data_info_dict['label'] + '/'\n",
    "\n",
    "    for name in names:\n",
    "        # get the path to classifier output\n",
    "        data_info_dict['classifications'][name] = '%s/predicted_prob_%s.csv'%(name, name)\n",
    "        \n",
    "        # get the path to truth table\n",
    "        data_info_dict['truth_tables'][name] = '%s/truth_table_%s.csv'%(name, name)\n",
    "        \n",
    "    return data_info_dict\n",
    "\n",
    "def process_strings(dataset, cc):\n",
    "    \"\"\"\n",
    "    Get info on directory name and classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset: dictionary   \n",
    "    cc: classifier name, str\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    loc: data directory, str\n",
    "    text: version label, str\n",
    "   \"\"\"\n",
    "    \n",
    "    loc = dataset['dirname']\n",
    "    text = dataset['label'] + ' ' + dataset['names'][cc]\n",
    "    \n",
    "    return loc, text\n",
    "\n",
    "def just_read_class_pairs(pair, dataset, cc):\n",
    "    \"\"\"\n",
    "    Reads predicted probabilities and truth table.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pair: list of str - [path_to_classifier_output, path_to_truth_table]\n",
    "    dataset: dictionary\n",
    "    cc: classifier name, str\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    prob_mat: probability matrix (output from classifier)\n",
    "    tvec: truth vector\n",
    "    \"\"\"\n",
    "    \n",
    "    loc, text = process_strings(dataset, cc)\n",
    "    clfile = pair[0]\n",
    "    truthfile = pair[1]\n",
    "    \n",
    "    # read classifier output\n",
    "    prob_mat = pd.read_csv(loc + clfile, delim_whitespace=True).values\n",
    "    \n",
    "    # read truth table\n",
    "    truth_values = pd.read_csv(loc + truthfile, delim_whitespace=True).values\n",
    "    tvec = np.where(truth_values==1)[1]\n",
    "    \n",
    "    return prob_mat, tvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dictionary to store classification results\n",
    "mystery = {}\n",
    "mystery['label'] = 'Unknown'\n",
    "mystery['names'] = ['RandomForest', 'KNeighbors', 'MLPNeuralNet']\n",
    "mystery['classifications'] = {}\n",
    "mystery['truth_tables'] = {}\n",
    "mystery['class_pairs'] = {}\n",
    "mystery['probs'] = {}\n",
    "mystery['truth'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read classifier output and truth tables\n",
    "topdir = '../examples/'\n",
    "mystery = make_file_locs(mystery)\n",
    "mystery['class_pairs'] = make_class_pairs(mystery)\n",
    "for nm, name in enumerate(mystery['names']):\n",
    "    probm, truthv = just_read_class_pairs(mystery['class_pairs'][name], mystery, nm)\n",
    "    mystery['probs'][name] = probm\n",
    "    mystery['truth'][name] = truthv\n",
    "M_classes = np.shape(probm)[-1]\n",
    "\n",
    "# we need the class labels in the dataset in a consistently sorted order \n",
    "# and will assume the weights of the weightvec correspond to this order\n",
    "class_labels = sorted(np.unique(mystery['truth']['RandomForest']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'RandomForest': ['RandomForest/predicted_prob_RandomForest.csv',\n",
       "  'RandomForest/truth_table_RandomForest.csv'],\n",
       " 'KNeighbors': ['KNeighbors/predicted_prob_KNeighbors.csv',\n",
       "  'KNeighbors/truth_table_KNeighbors.csv'],\n",
       " 'MLPNeuralNet': ['MLPNeuralNet/predicted_prob_MLPNeuralNet.csv',\n",
       "  'MLPNeuralNet/truth_table_MLPNeuralNet.csv']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topdir = '../examples/'\n",
    "mystery = make_file_locs(mystery)\n",
    "mystery['class_pairs'] = make_class_pairs(mystery)\n",
    "mystery['class_pairs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method (Metric)\n",
    "======\n",
    "\n",
    "The log-loss is defined as\n",
    "\\begin{eqnarray*}\n",
    "L &=& -\\sum_{m=1}^{M}\\frac{w_{m}}{N_{m}}\\sum_{n=1}^{N_{m}}\\ln[p_{n}(m | m)]\n",
    "\\end{eqnarray*}\n",
    "\n",
    "We calculate the metric within each class $m$ by taking an average of its value $-\\ln[p_{n}(m | m)]$ for each true member  $n$ of the class.  Then we weight the metrics for each class by an arbitrary weight $w_{m}$ and take a weighted average of the per-class metrics to produce a global scalar metric $L$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is all from proclam but copied here so no one has to install it.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import collections\n",
    "\n",
    "\"\"\"\n",
    "Utility functions for PLAsTiCC metrics\n",
    "\"\"\"\n",
    "\n",
    "# from __future__ import absolute_import, division\n",
    "# __all__ = ['sanitize_predictions',\n",
    "#            'weight_sum', 'averager', 'check_weights',\n",
    "#            'det_to_prob',\n",
    "#            'prob_to_det',\n",
    "#            'det_to_cm', 'prob_to_cm',\n",
    "#            'cm_to_rate', 'det_to_rate', 'prob_to_rate']\n",
    "\n",
    "RateMatrix = collections.namedtuple('rates', 'TPR FPR FNR TNR')\n",
    "\n",
    "def sanitize_predictions(predictions, epsilon=sys.float_info.epsilon):\n",
    "    \"\"\"\n",
    "    Replaces 0 and 1 with 0+epsilon, 1-epsilon\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    predictions: numpy.ndarray, float\n",
    "        N*M matrix of probabilities per object, may have 0 or 1 values\n",
    "    epsilon: float\n",
    "        small placeholder number, defaults to floating point precision\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    predictions: numpy.ndarray, float\n",
    "        N*M matrix of probabilities per object, no 0 or 1 values\n",
    "    \"\"\"\n",
    "    assert epsilon > 0. and epsilon < 0.0005\n",
    "    mask1 = (predictions < epsilon)\n",
    "    mask2 = (predictions > 1.0 - epsilon)\n",
    "\n",
    "    predictions[mask1] = epsilon\n",
    "    predictions[mask2] = 1.0 - epsilon\n",
    "    predictions = predictions / np.sum(predictions, axis=1)[:, np.newaxis]\n",
    "    return predictions\n",
    "\n",
    "def det_to_prob(dets, prediction=None):\n",
    "    \"\"\"\n",
    "    Reformats vector of class assignments into matrix with 1 at true/assigned class and zero elsewhere\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dets: numpy.ndarray, int\n",
    "        vector of classes\n",
    "    prediction: numpy.ndarray, float, optional\n",
    "        predicted class probabilities\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    probs: numpy.ndarray, float\n",
    "        matrix with 1 at input classes and 0 elsewhere\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    det_to_prob formerly truth_reformatter\n",
    "    Does not yet handle number of classes in truth not matching number of classes in prediction, i.e. for having \"other\" class or secret classes not in training set.  The prediction keyword is a kludge to enable this but should be replaced.\n",
    "    \"\"\"\n",
    "    N = len(dets)\n",
    "    indices = range(N)\n",
    "\n",
    "    if prediction is None:\n",
    "        prediction_shape = (N, int(np.max(dets) + 1))\n",
    "    else:\n",
    "        prediction, dets = np.asarray(prediction), np.asarray(dets)\n",
    "        prediction_shape = np.shape(prediction)\n",
    "\n",
    "    probs = np.zeros(prediction_shape)\n",
    "    probs[indices, dets] = 1.\n",
    "\n",
    "    return probs\n",
    "\n",
    "def prob_to_det(probs):\n",
    "    \"\"\"\n",
    "    Converts probabilistic classifications to deterministic classifications by assigning the class with highest probability\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    probs: numpy.ndarray, float\n",
    "        N * M matrix of class probabilities\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dets: numpy.ndarray, int\n",
    "        maximum probability classes\n",
    "    \"\"\"\n",
    "    dets = np.argmax(probs, axis=1)\n",
    "\n",
    "    return dets\n",
    "\n",
    "def det_to_cm(dets, truth, per_class_norm=True, vb=False):\n",
    "    \"\"\"\n",
    "    Converts deterministic classifications and truth into confusion matrix\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dets: numpy.ndarray, int\n",
    "        assigned classes\n",
    "    truth: numpy.ndarray, int\n",
    "        true classes\n",
    "    per_class_norm: boolean, optional\n",
    "        equal weight per class if True, equal weight per object if False\n",
    "    vb: boolean, optional\n",
    "        if True, print cm\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cm: numpy.ndarray, int\n",
    "        confusion matrix\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    I need to fix the norm keyword all around to enable more options, like normed output vs. not.\n",
    "    \"\"\"\n",
    "    pred_classes, pred_counts = np.unique(dets, return_counts=True)\n",
    "    true_classes, true_counts = np.unique(truth, return_counts=True)\n",
    "    if vb: print((pred_classes, pred_counts), (true_classes, true_counts))\n",
    "\n",
    "    M = max(max(pred_classes), max(true_classes)) + 1\n",
    "\n",
    "    cm = np.zeros((M, M), dtype=float)\n",
    "    # print((np.shape(dets), np.shape(truth)))\n",
    "    coords = np.array(list(zip(dets, truth)))\n",
    "    indices, index_counts = np.unique(coords, axis=0, return_counts=True)\n",
    "    # if vb: print(indices, index_counts)\n",
    "    indices = indices.T\n",
    "    # if vb: print(np.shape(indices))\n",
    "    cm[indices[0], indices[1]] = index_counts\n",
    "    if vb: print(cm)\n",
    "\n",
    "    if per_class_norm:\n",
    "        # print(type(cm))\n",
    "        # print(type(true_counts))\n",
    "        # cm = cm / true_counts\n",
    "        # cm /= true_counts[:, np.newaxis] #\n",
    "        cm = cm / true_counts[np.newaxis, :]\n",
    "\n",
    "    if vb: print(cm)\n",
    "\n",
    "    return cm\n",
    "\n",
    "def prob_to_cm(probs, truth, per_class_norm=True, vb=False):\n",
    "    \"\"\"\n",
    "    Turns probabilistic classifications into confusion matrix by taking maximum probability as deterministic class\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    probs: numpy.ndarray, float\n",
    "        N * M matrix of class probabilities\n",
    "    truth: numpy.ndarray, int\n",
    "        N-dimensional vector of true classes\n",
    "    per_class_norm: boolean, optional\n",
    "        equal weight per class if True, equal weight per object if False\n",
    "    vb: boolean, optional\n",
    "        if True, print cm\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cm: numpy.ndarray, int\n",
    "        confusion matrix\n",
    "    \"\"\"\n",
    "    dets = prob_to_det(probs)\n",
    "\n",
    "    cm = det_to_cm(dets, truth, per_class_norm=per_class_norm, vb=vb)\n",
    "\n",
    "    return cm\n",
    "\n",
    "def cm_to_rate(cm, vb=False):\n",
    "    \"\"\"\n",
    "    Turns a confusion matrix into true/false positive/negative rates\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cm: numpy.ndarray, int or float\n",
    "        confusion matrix, first axis is predictions, second axis is truth\n",
    "    vb: boolean, optional\n",
    "        print progress to stdout?\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    rates: named tuple, float\n",
    "        RateMatrix named tuple\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    BROKEN!\n",
    "    This can be done with a mask to weight the classes differently here.\n",
    "    \"\"\"\n",
    "    if vb: print(cm)\n",
    "    diag = np.diag(cm)\n",
    "    if vb: print(diag)\n",
    "\n",
    "    TP = np.sum(diag)\n",
    "    FN = np.sum(np.sum(cm, axis=0) - diag)\n",
    "    FP = np.sum(np.sum(cm, axis=1) - diag)\n",
    "    TN = np.sum(cm) - TP\n",
    "    if vb: print((TP, FN, FP, TN))\n",
    "\n",
    "    T = TP + TN\n",
    "    F = FP + FN\n",
    "    P = TP + FP\n",
    "    N = TN + FN\n",
    "    if vb: print((T, F, P, N))\n",
    "\n",
    "    TPR = TP / P\n",
    "    FPR = FP / N\n",
    "    FNR = FN / P\n",
    "    TNR = TN / N\n",
    "\n",
    "    rates = RateMatrix(TPR=TPR, FPR=FPR, FNR=FNR, TNR=TNR)\n",
    "    if vb: print(rates)\n",
    "\n",
    "    return rates\n",
    "\n",
    "def det_to_rate(dets, truth, per_class_norm=True, vb=False):\n",
    "    cm = det_to_cm(dets, truth, per_class_norm=per_class_norm, vb=vb)\n",
    "    rates = cm_to_rate(cm, vb=vb)\n",
    "    return rates\n",
    "\n",
    "def prob_to_rate(probs, truth, per_class_norm=True, vb=False):\n",
    "    cm = prob_to_cm(probs, truth, per_class_norm=per_class_norm, vb=vb)\n",
    "    rates = cm_to_rate(cm, vb=vb)\n",
    "    return rates\n",
    "\n",
    "def weight_sum(per_class_metrics, weight_vector, norm=True):\n",
    "    \"\"\"\n",
    "    Calculates the weighted metric\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    per_class_metrics: numpy.float\n",
    "        the scores separated by class (a list of arrays)\n",
    "    weight_vector: numpy.ndarray floar\n",
    "        The array of weights per class\n",
    "    norm: boolean, optional\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    weight_sum: np.float\n",
    "        The weighted metric\n",
    "    \"\"\"\n",
    "    weight_sum = np.dot(weight_vector, per_class_metrics)\n",
    "\n",
    "    return weight_sum\n",
    "\n",
    "def check_weights(avg_info, M, truth=None):\n",
    "    \"\"\"\n",
    "    Converts standard weighting schemes to weight vectors for weight_sum\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    avg_info: str or numpy.ndarray, float\n",
    "        keyword about how to calculate weighted average metric\n",
    "    M: int\n",
    "        number of classes\n",
    "    truth: numpy.ndarray, int, optional\n",
    "        true class assignments\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    weights: numpy.ndarray, float\n",
    "        relative weights per class\n",
    "    \"\"\"\n",
    "    if type(avg_info) != str:\n",
    "        avg_info = np.asarray(avg_info)\n",
    "        weights = avg_info / np.sum(avg_info)\n",
    "        assert(np.isclose(sum(weights), 1.))\n",
    "    elif avg_info == 'per_class':\n",
    "        weights = np.ones(M) / float(M)\n",
    "    elif avg_info == 'per_item':\n",
    "        classes, counts = np.unique(truth, return_counts=True)\n",
    "        weights = np.zeros(M)\n",
    "        weights[classes] = counts / float(len(truth))\n",
    "        assert len(weights) == M\n",
    "    return weights\n",
    "\n",
    "def averager(per_object_metrics, truth, M):\n",
    "    \"\"\"\n",
    "    Creates a list with the metrics per object, separated by class\n",
    "    \"\"\"\n",
    "    group_metric = per_object_metrics\n",
    "    class_metric = np.empty(M)\n",
    "    for m in range(M):\n",
    "        true_indices = np.where(truth == m)[0]\n",
    "        how_many_in_class = len(true_indices)\n",
    "        try:\n",
    "            assert(how_many_in_class > 0)\n",
    "            per_class_metric = group_metric[true_indices]\n",
    "            # assert(~np.all(np.isnan(per_class_metric)))\n",
    "            class_metric[m] = np.average(per_class_metric)\n",
    "        except AssertionError:\n",
    "            class_metric[m] = 0.\n",
    "        # print((m, how_many_in_class, class_metric[m]))\n",
    "    return class_metric\n",
    "\n",
    "\"\"\"\n",
    "A superclass for metrics\n",
    "\"\"\"\n",
    "class Metric(object):\n",
    "\n",
    "    def __init__(self, scheme=None, **kwargs):\n",
    "        \"\"\"\n",
    "        An object that evaluates a function of the true classes and class probabilities\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        scheme: string\n",
    "            the name of the metric\n",
    "        \"\"\"\n",
    "        self.scheme = scheme\n",
    "\n",
    "    def evaluate(self, prediction, truth, weights=None, **kwds):\n",
    "        \"\"\"\n",
    "        Evaluates a function of the truth and prediction\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        prediction: numpy.ndarray, float\n",
    "            predicted class probabilities\n",
    "        truth: numpy.ndarray, int\n",
    "            true classes\n",
    "        weights: numpy.ndarray, float\n",
    "            per-class weights\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        metric: float\n",
    "            value of the metric\n",
    "        \"\"\"\n",
    "        print('No metric specified: returning true positive rate based on maximum value')\n",
    "\n",
    "        return # metric\n",
    "\n",
    "\"\"\"\n",
    "A metric subclass for the log-loss\n",
    "\"\"\"\n",
    "class LogLoss(Metric):\n",
    "    def __init__(self, scheme=None):\n",
    "        \"\"\"\n",
    "        An object that evaluates the log-loss metric\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        scheme: string\n",
    "            the name of the metric\n",
    "        \"\"\"\n",
    "        super(LogLoss, self).__init__(scheme)\n",
    "        self.scheme = scheme\n",
    "\n",
    "    def evaluate(self, prediction, truth, averaging='per_class'):\n",
    "        \"\"\"\n",
    "        Evaluates the log-loss\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        prediction: numpy.ndarray, float\n",
    "            predicted class probabilities\n",
    "        truth: numpy.ndarray, int\n",
    "            true classes\n",
    "        averaging: string or numpy.ndarray, float\n",
    "            'per_class' weights classes equally, other keywords possible\n",
    "            vector assumed to be class weights\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        logloss: float\n",
    "            value of the metric\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        This uses the natural log.\n",
    "        \"\"\"\n",
    "        prediction, truth = np.asarray(prediction), np.asarray(truth)\n",
    "        prediction_shape = np.shape(prediction)\n",
    "        (N, M) = prediction_shape\n",
    "\n",
    "        weights = check_weights(averaging, M, truth=truth)\n",
    "        truth_mask = det_to_prob(truth, prediction)\n",
    "\n",
    "        prediction = sanitize_predictions(prediction)\n",
    "\n",
    "        log_prob = np.log(prediction)\n",
    "        logloss_each = -1. * np.sum(truth_mask * log_prob, axis=1)[:, np.newaxis]\n",
    "\n",
    "        # use a better structure for checking keyword support\n",
    "        class_logloss = averager(logloss_each, truth, M)\n",
    "\n",
    "        logloss = weight_sum(class_logloss, weight_vector=weights)\n",
    "\n",
    "        assert(~np.isnan(logloss))\n",
    "\n",
    "        return logloss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighbors with weights [0.05882353 0.11764706 0.05882353 0.05882353 0.05882353 0.05882353\n",
      " 0.05882353 0.05882353 0.05882353 0.05882353 0.11764706 0.11764706\n",
      " 0.11764706] has LogLoss = 20.749255306361132\n"
     ]
    }
   ],
   "source": [
    "# This is how you run the metric with a random weight vector.\n",
    "\n",
    "metric = 'LogLoss'\n",
    "weightvec = np.ones(M_classes)\n",
    "\n",
    "# dummy example for SNPhotCC demo data\n",
    "special_classes = (1, 10, 11, 12)\n",
    "\n",
    "# we should be using this for the PLAsTiCC data\n",
    "# special_clases = (51, 99)\n",
    "\n",
    "mask = np.array([True if classname in special_classes else False for classname in class_labels])\n",
    "weightvec[mask] = 2\n",
    "weightvec = weightvec / sum(weightvec)\n",
    "name = np.random.choice(mystery['names'])\n",
    "probm = mystery['probs'][name]\n",
    "truthv = mystery['truth'][name]\n",
    "LL = LogLoss()\n",
    "val = LL.evaluate(prediction=probm, truth=truthv, averaging=weightvec)\n",
    "print(name+' with weights '+str(weightvec)+' has '+metric+' = '+str(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acknowledgments\n",
    "===============\n",
    "\n",
    "The DESC acknowledges ongoing support from the Institut National de Physique Nucleaire et de Physique des Particules in France; the Science & Technology Facilities Council in the United Kingdom; and the Department of Energy, the National Science Foundation, and the LSST Corporation in the United States.\n",
    "\n",
    "DESC uses resources of the IN2P3 Computing Center (CC-IN2P3--Lyon/Villeurbanne - France) funded by the Centre National de la Recherche Scientifique; the National Energy Research Scientific Computing Center, a DOE Office of Science User Facility supported by the Office of Science of the U.S. Department of Energy under Contract No. DE-AC02-05CH11231; STFC DiRAC HPC Facilities, funded by UK BIS National E-infrastructure capital grants; and the UK particle physics grid, supported by the GridPP Collaboration.\n",
    "\n",
    "This work was performed in part under DOE Contract DE-AC02-76SF00515."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contributions\n",
    "=======\n",
    "\n",
    "Alex Malz: conceptualization, data curation, formal analysis, investigation, methodology, project administration, software, supervision, validation, visualization, writing - original draft\n",
    "\n",
    "Renee Hlozek: data curation, formal analysis, funding acquisition, investigation, project administration, software, supervision, validation, visualization, writing - original draft\n",
    "\n",
    "Tarek Alam: investigation, software, validation\n",
    "\n",
    "Anita Bahmanyar: formal analysis, investigation, methodology, software, writing - original draft\n",
    "\n",
    "Rahul Biswas: conceptualization, methodology, software\n",
    "\n",
    "Rafael Martinez-Galarza: data curation, software, visualization\n",
    "\n",
    "Gautham Narayan: data curation, formal analysis"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
