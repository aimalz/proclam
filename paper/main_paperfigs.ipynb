{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hideme"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as cpkl\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hideme"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "# print(mpl.rcParams.items)\n",
    "mpl.use('Agg')\n",
    "mpl.rcParams['text.usetex'] = False\n",
    "mpl.rcParams['mathtext.rm'] = 'serif'\n",
    "mpl.rcParams['font.family'] = 'serif'\n",
    "mpl.rcParams['font.serif'] = ['Times New Roman']\n",
    "# mpl.rcParams['font.family'] = ['Times New Roman']\n",
    "mpl.rcParams['axes.titlesize'] = 25\n",
    "mpl.rcParams['axes.labelsize'] = 20\n",
    "mpl.rcParams['xtick.labelsize'] = 15\n",
    "mpl.rcParams['ytick.labelsize'] = 15\n",
    "mpl.rcParams['savefig.dpi'] = 250\n",
    "mpl.rcParams['figure.dpi'] = 250\n",
    "mpl.rcParams['savefig.format'] = 'pdf'\n",
    "mpl.rcParams['savefig.bbox'] = 'tight'\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# print(mpl.rcParams.items)\n",
    "\n",
    "import pylab\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_colormap(cmap, minval=0.0, maxval=1.0, n=100):\n",
    "    new_cmap = mpl.colors.LinearSegmentedColormap.from_list(\n",
    "        'trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval),\n",
    "        cmap(np.linspace(minval, maxval, n)))\n",
    "    return new_cmap\n",
    "\n",
    "cmap = plt.get_cmap('hot_r')\n",
    "fave_cmap = truncate_colormap(cmap, 0.35, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hideme"
    ]
   },
   "source": [
    "<!-- ![](./header.png) -->\n",
    "<img src=\"./header.png\",width=100%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Photometric LSST Astronomical Time-series Classification Challenge (PLAsTiCC): Selection of a performance metric for classification probabilities\n",
    "\n",
    "*Alex Malz (NYU)*, \n",
    "*Renee Hlozek (U. Toronto)*, \n",
    "*Tarek Alam (UCL)*, \n",
    "*Anita Bahmanyar (U. Toronto)*, \n",
    "*Rahul Biswas (Stockholm U.)*, \n",
    "*Emille Ishida (Clermont)*, \n",
    "*David Jones (Berkeley)*, \n",
    "*Ashish Mahabal (CfA)*, \n",
    "*Rafael Martinez-Galarza (Harvard)*, \n",
    "*Gautham Narayan (STScI)*,\n",
    "*Christian Setzer (Stockholm U.)*\n",
    "\n",
    "We describe and illustrate the process by which a global performance metric was chosen for Photometric LSST Astronomical Time-series Classification Challenge (PLAsTiCC), a Kaggle competition aiming to identify promising transient and variable classifiers for LSST by involving the broader community outside astronomy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction\n",
    "============\n",
    "\n",
    "The metric of this note is for the first version of the Kaggle competition, though there are future plans for an early classification challenge and identification of class-specific metrics for different science goals.  \n",
    "\n",
    "* The metric must return a single scalar value.\n",
    "* The metric must be well-defined for non-binary classes.\n",
    "* The metric must balance diverse science use cases in the presence of heavily nonuniform class prevalence.\n",
    "* The metric must respect the information content of probabilistic classifications.\n",
    "* The metric must be able to evaluate deterministic classifications.\n",
    "* The metric must be interpretable, meaning it gives a more optimal value for \"good\" mock classifiers and a less optimal value for mock classifiers plagued by anticipated systematic errors; in other words, it must pass basic tests of intuition.\n",
    "* The metric must be reliable, giving consistent results for different instantiations of the same test case.\n",
    "\n",
    "The Probabilistic Classification Metric (ProClaM) code used in this exploration of performance metrics is publicly available on [GitHub](https://github.com/aimalz/proclam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hideme"
    ]
   },
   "outputs": [],
   "source": [
    "import proclam\n",
    "from proclam import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data\n",
    "====\n",
    "\n",
    "We confirm the behavior of the metrics on mock data with well-understood systematics as well as real data from past classification challenges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hideme"
    ]
   },
   "outputs": [],
   "source": [
    "M_classes = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = proclam.simulators.LogUnbalanced()\n",
    "N_objects = int(1e6)\n",
    "truth = generator.simulate(M_classes, N_objects, base=6)\n",
    "print(np.histogram(truth, bins=range(M_classes+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = np.diff(np.unique(truth)).min()\n",
    "# left_of_first_bin = truth.min() - float(d)/2\n",
    "# right_of_last_bin = truth.max() + float(d)/2\n",
    "# plt.hist(truth, np.arange(left_of_first_bin, right_of_last_bin + d, d), log=True, alpha=0.5, color=fave_cmap(1.))\n",
    "# plt.xticks(range(max(truth)+1))\n",
    "# # plt.hist(truth, log=True, alpha=0.5)\n",
    "# plt.ylabel('counts', fontsize=20)\n",
    "# plt.xlabel('class', fontsize=20)\n",
    "# plt.savefig('fig/mock_counts.png')\n",
    "# plt.show()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mock classifier systematics\n",
    "\n",
    "Controls\n",
    "* agnostic: uniform probabilities across all classes\n",
    "* perfect: perfectly accurate on all classes\n",
    "* almost: a slight perturbation of the perfect classifier\n",
    "* noisy: a large perturbation of the perfect classifier\n",
    "\n",
    "Anticipated systematics\n",
    "* tunnel vision: classifies one class well and others randomly\n",
    "* cruise control: classifies all objects as a single class\n",
    "* subsumed: consistently misclassifies one class as one other class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hideme"
    ]
   },
   "outputs": [],
   "source": [
    "plasticc = {}\n",
    "plasticc['label'] = 'ProClaM'\n",
    "plasticc['names'] = []\n",
    "plasticc['cm'] = {}\n",
    "plasticc['delta'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hideme"
    ]
   },
   "outputs": [],
   "source": [
    "# M_classes = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hideme"
    ]
   },
   "outputs": [],
   "source": [
    "# chosen = np.random.randint(0, M_classes)\n",
    "chosen = 6\n",
    "print(chosen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hideme"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_cm_from_cm(cm, text):\n",
    "    \n",
    "    import matplotlib.colors as mpcolors\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.imshow(cm.T, vmin=0., vmax=1., norm=mpcolors.SymLogNorm(linthresh=0.03, linscale=0.03, vmin=0.0, vmax=1.0),\n",
    "               cmap=fave_cmap)\n",
    "    plt.title(text, fontsize=25)\n",
    "    ax = plt.gca()\n",
    "    plt.xlabel('predicted class', fontsize=25)\n",
    "    plt.ylabel('true class', fontsize=25)\n",
    "    cbar = plt.colorbar()\n",
    "    cbar.set_ticks([0.01,0.1,1])\n",
    "    cbar.set_ticklabels(['0.01','0.1','1'])\n",
    "    cbar.set_label('Probability', rotation=270, fontsize=20)\n",
    "    cbar.ax.tick_params(labelsize=20)\n",
    "    plt.savefig('fig/'+text+'.png', bbox='tight', pad_inches=0.1)\n",
    "    \n",
    "def wrap_up_classifier(cm, testname, info_dict, delta=0.1):\n",
    "    cm = cm / np.sum(cm, axis=1)[:, np.newaxis]\n",
    "    cm = cm.T\n",
    "#     plot_cm_from_cm(cm, testname)\n",
    "    info_dict = add_one_cm(info_dict, cm.T, testname, delta)\n",
    "    return info_dict\n",
    "        \n",
    "def add_one_cm(info_dict, cm, testname, delta):\n",
    "    info_dict['names'].append(testname)\n",
    "    info_dict['cm'][testname] = cm\n",
    "    info_dict['delta'][testname] = delta\n",
    "    return info_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agnostic\n",
    "\n",
    "Totally uniform CM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hidein"
    ]
   },
   "outputs": [],
   "source": [
    "cm = np.ones((M_classes, M_classes))\n",
    "plasticc = wrap_up_classifier(cm, 'Uncertain', plasticc, delta=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perfect classifier\n",
    "\n",
    "Identity matrix CM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hidein"
    ]
   },
   "outputs": [],
   "source": [
    "cm = np.eye(M_classes) + 1.e-8\n",
    "plasticc = wrap_up_classifier(cm, 'Perfect', plasticc, delta=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Almost perfect classifier\n",
    "\n",
    "Identity matrix CM plus low-amplitude uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hidein"
    ]
   },
   "outputs": [],
   "source": [
    "cm = np.eye(M_classes) + 0.1 * np.ones((M_classes, M_classes))\n",
    "plasticc = wrap_up_classifier(cm, 'Almost', plasticc, delta=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noisy classifier\n",
    "\n",
    "Identity matrix CM plus high-amplitude uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "hidein"
    ]
   },
   "outputs": [],
   "source": [
    "cm = np.eye(M_classes) + 0.5* np.ones((M_classes, M_classes))\n",
    "plasticc = wrap_up_classifier(cm, 'Noisy', plasticc, delta=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tunnel vision classifier\n",
    "\n",
    "accurate predictions on one class and uniform on others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hidein"
    ]
   },
   "outputs": [],
   "source": [
    "cm = np.ones((M_classes, M_classes))\n",
    "cm = cm * np.asarray(0.1)[np.newaxis, np.newaxis]\n",
    "print(chosen)\n",
    "# cm[chosen] = plasticc['cm']['Perfect'][chosen]\n",
    "# cm[:, chosen] = plasticc['cm']['Perfect'][chosen]#np.zeros((M_classes))#[np.newaxis, :]\n",
    "cm[:, chosen] = cm[:, chosen] / M_classes\n",
    "cm[chosen][chosen] += M_classes\n",
    "plasticc = wrap_up_classifier(cm, 'Tunnel', plasticc, delta=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cruise control classifier\n",
    "\n",
    "always predict one class regardless of true class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hidein"
    ]
   },
   "outputs": [],
   "source": [
    "cm = np.ones((M_classes, M_classes))\n",
    "cm = cm * 0.1\n",
    "cm[:, chosen] = 1.\n",
    "plasticc = wrap_up_classifier(cm, 'Cruise', plasticc, delta=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsumed classifiers\n",
    "\n",
    "Subsumed to: the chosen class is consistently misclassified as a different class\n",
    "\n",
    "Subsumed from: another class is consistently misclassified as the chosen class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hidein"
    ]
   },
   "outputs": [],
   "source": [
    "cm = plasticc['cm']['Almost'].copy()\n",
    "cm[chosen] = cm[chosen-1]\n",
    "plasticc = wrap_up_classifier(cm, 'SubsumedTo', plasticc, delta=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hidein"
    ]
   },
   "outputs": [],
   "source": [
    "cm = plasticc['cm']['Almost'].copy()\n",
    "cm[chosen-1] = cm[chosen]\n",
    "plasticc = wrap_up_classifier(cm, 'SubsumedFrom', plasticc, delta=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hideme"
    ]
   },
   "outputs": [],
   "source": [
    "def cm_mega_plot(info_dict, fn='', numrows=2, numcols=4):\n",
    "    import matplotlib.colors as mpcolors\n",
    "    fig = pylab.figure(figsize=(10.1, 5.1))\n",
    "    bigAxes = pylab.axes(frameon=False)     # hide frame\n",
    "    bigAxes.set_xticks([])                        # don't want to see any ticks on this axis\n",
    "    bigAxes.set_yticks([])\n",
    "    \n",
    "#    cbar = plt.colorbar()\n",
    "#    cbar.set_ticks([0.01,0.1,1])\n",
    "#    cbar.set_ticklabels(['0.01','0.1','1'])\n",
    "#    cbar.set_label('Probability', rotation=270, fontsize=20)\n",
    "#    cbar.ax.tick_params(labelsize=20)\n",
    "    \n",
    "    grid = ImageGrid(fig, 111,          # as in plt.subplot(111)\n",
    "                 nrows_ncols=(2,4),\n",
    "                 axes_pad=0.05,\n",
    "                 share_all=True,\n",
    "                 cbar_location=\"right\",\n",
    "                 cbar_mode=\"single\",\n",
    "                 cbar_size=\"5%\",\n",
    "                 cbar_pad=0.05,\n",
    "                 )\n",
    "    numrows=2\n",
    "    numcols=4\n",
    "    gridnos = np.arange(numrows * numcols).reshape((numcols, numrows)).T.flatten()\n",
    "    for j in range(numcols * numrows):\n",
    "       \n",
    "        i = gridnos[j]\n",
    "#         print(i,j)\n",
    "        ax = grid[j]#fig.add_subplot(numrows,numcols,i+1)#, frameon=False)\n",
    "        if (i==0) or (i==1):\n",
    "            labs = ax.get_yticklabels()\n",
    "#             print(labs)\n",
    "            ax.set_yticklabels(['1','2','3','4','5','6','7','8','9','10','11','12','13'])\n",
    "        \n",
    "        else:\n",
    "            ax.set_yticks([])\n",
    "            ax.set_yticklabels([])\n",
    "        \n",
    "        ax.set_xticks([]) \n",
    "        ax.set_xticklabels([])\n",
    "        \n",
    "        position = ax.get_position()\n",
    "#         print(position)\n",
    "        position.x0 += 0.01\n",
    "        position.y0 += 0.01\n",
    "        position.x1 += 0.01\n",
    "        position.y1 += 0.01\n",
    "        ax.set_position(position)\n",
    "#         print(position)\n",
    "        testname = info_dict['names'][i]\n",
    "#         print(testname)\n",
    "        \n",
    "        im = ax.imshow(info_dict['cm'][testname], norm=mpcolors.SymLogNorm(linthresh=0.03, linscale=0.03, vmin=0.0, vmax=1.0), cmap=fave_cmap)\n",
    "        ax.text(.67, .9, testname, horizontalalignment='center', transform=ax.transAxes, fontsize=16)\n",
    "        if testname[0] == 'S':\n",
    "            ax.scatter(chosen, chosen, marker='+', s=10, color='darkorchid')\n",
    "#     pylab.colorbar()\n",
    "#     fig.subplots_adjust(right=0.5)\n",
    "#     cbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\n",
    "#     fig.colorbar(im, cax=cbar_ax)\n",
    "    cbar = ax.cax.colorbar(im)\n",
    "    ax.cax.toggle_label(True)\n",
    "    #cbar.set_ticks([0.01,0.1,1])\n",
    "#    cbar.set_ticklabels(['0.01','0.1','1'])\n",
    "#    cbar.set_label('Probability', rotation=270, fontsize=20)\n",
    "    bigAxes.set_ylabel(r'true class', fontsize=20)\n",
    "    #bigAxes.set_yticks(['1','2','3','4',])\n",
    "    bigAxes.set_xlabel(r'predicted class', fontsize=20)\n",
    "    pylab.tight_layout()\n",
    "    pylab.savefig('fig/all_'+fn+'_cm.png', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hidein"
    ]
   },
   "outputs": [],
   "source": [
    "cm_mega_plot(plasticc, fn='sim')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined mock systematics confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # afflicted = np.random.choice(range(0, M_classes), size=10, replace=False)\n",
    "# cruise = [0, 1]#afflicted[2:4]\n",
    "# subsumed = [2, 3, 5, 6]#afflicted[4:8]\n",
    "# tunnel = [4, 7]#afflicted[8:]\n",
    "# noisy_cls = [-2, -1]#afflicted[:2]\n",
    "# afflicted = cruise + subsumed + tunnel + noisy_cls\n",
    "\n",
    "# cm = plasticc['cm']['Almost'].copy()\n",
    "# cm[noisy_cls] = plasticc['cm']['Noisy'][noisy_cls]\n",
    "# cm[subsumed[:2]] = cm[cruise[0]]#[np.newaxis] + np.asarray(0.01)[np.newaxis] * np.random.uniform(M_classes)[np.newaxis, :]\n",
    "# cm[subsumed[2:]] = cm[cruise[1]]\n",
    "# cm[tunnel] = plasticc['cm']['Perfect'][tunnel]\n",
    "# # cm = cm / np.sum(cm, axis=1)[:, np.newaxis]\n",
    "# plasticc = wrap_up_classifier(cm, 'Combined', plasticc, delta=0.01)\n",
    "# print(np.sum(plasticc['cm']['Combined'], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real classification results\n",
    "\n",
    "* SNPhotCC \\[from Michelle?\\]\n",
    "* \\[Ashish's data?\\]\n",
    "* Mystery \\[Renee's data?\\]\n",
    "\n",
    "*show confusion matrices*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hideme"
    ]
   },
   "outputs": [],
   "source": [
    "# temporarily omitting real data\n",
    "def plot_cm(probs, truth, text, loc=None):\n",
    "    cm = proclam.metrics.util.prob_to_cm(probs, truth)\n",
    "    plt.matshow(cm.T, vmin=0., vmax=1.)\n",
    "# plt.xticks(range(max(truth)+1), names)\n",
    "# plt.yticks(range(max(truth)+1), names)\n",
    "    plt.xlabel('predicted class')\n",
    "    plt.ylabel('true class')\n",
    "    plt.colorbar()\n",
    "    plt.title(text)\n",
    "#     plt.show()\n",
    "    plt.savefig('fig/'+loc+'_cm.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hideme"
    ]
   },
   "outputs": [],
   "source": [
    "# temporarily omitting real data\n",
    "# These are all functions for preprocessing Renee's data files.\n",
    "\n",
    "def make_class_pairs(data_info_dict):\n",
    "    for name in data_info_dict['names']:\n",
    "        data_info_dict['class_pairs'][name] = [data_info_dict['classifications'][name], data_info_dict['truth_tables'][name]]\n",
    "    return(data_info_dict['class_pairs'])\n",
    "        \n",
    "def make_file_locs(data_info_dict):\n",
    "    data_info_dict['classifications'] = {}\n",
    "    data_info_dict['truth_tables'] = {}\n",
    "    data_info_dict['class_pairs'] = {}\n",
    "    data_info_dict['probs'] = {}\n",
    "    data_info_dict['truth'] = {}\n",
    "    names = data_info_dict['names']\n",
    "    data_info_dict['dirname'] = topdir + data_info_dict['label'] + '/'\n",
    "    for name in names:\n",
    "        data_info_dict['classifications'][name] = '%s/predicted_prob_%s.csv'%(name, name)\n",
    "        data_info_dict['truth_tables'][name] = '%s/truth_table_%s.csv'%(name, name)\n",
    "    return data_info_dict\n",
    "\n",
    "def process_strings(dataset, cc):\n",
    "    loc = dataset['dirname']\n",
    "    text = dataset['label'] + '_' + dataset['names'][cc]\n",
    "    return loc, text\n",
    "\n",
    "# def plot_cm(probs, truth, text, loc=''):\n",
    "#     cm = proclam.metrics.util.prob_to_cm(probs, truth)\n",
    "#     plt.matshow(cm.T, vmin=0., vmax=1.)\n",
    "#     plt.xlabel('predicted class')\n",
    "#     plt.ylabel('true class')\n",
    "#     plt.colorbar()\n",
    "#     plt.title(text)\n",
    "# #     plt.show()\n",
    "#     plt.savefig(loc+'plot.png', dpi=250)\n",
    "#     plt.close()\n",
    "\n",
    "def plot_cm(probs, truth, text, loc=None):\n",
    "    cm = proclam.metrics.util.prob_to_cm(probs, truth)\n",
    "    plt.matshow(cm.T, vmin=0., vmax=1., cmap=fave_cmap)\n",
    "# plt.xticks(range(max(truth)+1), names)\n",
    "# plt.yticks(range(max(truth)+1), names)\n",
    "#     plt.xlabel('predicted class')\n",
    "#     plt.ylabel('true class')\n",
    "#     plt.colorbar()\n",
    "#     plt.title(text)\n",
    "#     plt.show()\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    saveloc = 'fig/'+text+'_cm.png'\n",
    "#     print(saveloc)\n",
    "    plt.savefig(saveloc, bbox_inches='tight', pad_inches=0)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "def just_plot_cm(dataset, cc, pmat, tvec):\n",
    "    fileloc, text = process_strings(dataset, cc)\n",
    "    plot_cm(pmat, tvec, text, loc=dataset['label']+'_'+dataset['names'][cc])\n",
    "    return\n",
    "\n",
    "def just_read_class_pairs(pair, dataset, cc):\n",
    "    loc, text = process_strings(dataset, cc)\n",
    "    clfile = pair[0]\n",
    "    truthfile = pair[1]\n",
    "    prob_mat = pd.read_csv(loc + clfile, delim_whitespace=True).values\n",
    "    nobj = np.shape(prob_mat)[0]\n",
    "    nclass = np.shape(prob_mat)[1]\n",
    "    truth_values = pd.read_csv(loc + truthfile, delim_whitespace=True).values\n",
    "    nobj_truth = np.shape(truth_values)[0]\n",
    "    nclass_truth = np.shape(truth_values)[1]\n",
    "    tvec = np.where(truth_values==1)[1]\n",
    "    pmat = prob_mat\n",
    "    return pmat, tvec\n",
    "\n",
    "def read_class_pairs(pair, dataset, cc):\n",
    "    fileloc, text = process_strings(dataset, cc)\n",
    "    pmat, tvec = just_read_class_pairs(pair, dataset, cc)\n",
    "    filename = fileloc + dataset['names'][cc] + '/'\n",
    "#     plot_cm(pmat, tvec, text, loc=filename)\n",
    "    return pmat, tvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hideme"
    ]
   },
   "outputs": [],
   "source": [
    "# temporarily omitting real data\n",
    "topdir = '../examples/'\n",
    "mystery = {}\n",
    "mystery['cm'] = {}\n",
    "mystery['label'] = 'Unknown'\n",
    "mystery['names'] = ['RandomForest', 'KNeighbors', 'MLPNeuralNet']\n",
    "# mystery['classifications'] = {}\n",
    "# mystery['truth_tables'] = {}\n",
    "# mystery['class_pairs'] = {}\n",
    "# mystery['probs'] = {}\n",
    "# mystery['truth'] = {}\n",
    "mystery = make_file_locs(mystery)\n",
    "mystery['class_pairs'] = make_class_pairs(mystery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hideme"
    ]
   },
   "outputs": [],
   "source": [
    "# temporarily omitting real data\n",
    "snphotcc = {}\n",
    "snphotcc['cm'] = {}\n",
    "snphotcc['label'] = 'SNPhotCC'\n",
    "prefixes = ['Templates', 'Wavelets']\n",
    "suffixes = ['BoostForest', 'KNN', 'NB', 'NeuralNetwork', 'SVM']\n",
    "snphotcc['names'] = []\n",
    "snphotcc['shortnames'] = []\n",
    "for prefix in prefixes:\n",
    "    for suffix in suffixes:\n",
    "        if suffix == 'BoostForest':\n",
    "            short_suffix = 'BF'\n",
    "        elif suffix == 'NeuralNetwork':\n",
    "            short_suffix = 'NN'\n",
    "        else:\n",
    "            short_suffix = suffix\n",
    "        snphotcc['names'].append(prefix+suffix)\n",
    "        snphotcc['shortnames'].append(prefix[0]+short_suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hideme"
    ]
   },
   "outputs": [],
   "source": [
    "# temporarily omitting real data\n",
    "topdir = '../examples/'\n",
    "for dataset in [mystery, snphotcc]:\n",
    "#     print(dataset['label'])\n",
    "    dataset = make_file_locs(dataset)\n",
    "    dataset['class_pairs'] = make_class_pairs(dataset)\n",
    "    for nm, name in enumerate(dataset['names']):\n",
    "        probm, truthv = read_class_pairs(dataset['class_pairs'][name], dataset, nm)\n",
    "#         print(dataset['class_pairs'][name])\n",
    "        dataset['probs'][name] = probm\n",
    "        dataset['truth'][name] = truthv\n",
    "        det = proclam.metrics.util.prob_to_det(probm)\n",
    "        dataset['cm'][name] = proclam.metrics.util.det_to_cm(det, truthv, per_class_norm=True)\n",
    "#         plot_cm(probm, truthv, dataset['label']+'_'+name, loc=dataset['label']+'_'+name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporarily omitting real data\n",
    "def cm_mega_plot(info_dict, fn=''):\n",
    "    numrows = 2\n",
    "    numcols = 5\n",
    "    fig = pylab.figure(figsize=(9.1, 4.1))\n",
    "    bigAxes = pylab.axes(frameon=False)     # hide frame\n",
    "    bigAxes.set_xticks([])                        # don't want to see any ticks on this axis\n",
    "    bigAxes.set_yticks([])\n",
    "    \n",
    "    grid = ImageGrid(fig, 111,          # as in plt.subplot(111)\n",
    "                 nrows_ncols=(2,5),\n",
    "                 axes_pad=0.05,\n",
    "                 share_all=True,\n",
    "                 cbar_location=\"right\",\n",
    "                 cbar_mode=\"single\",\n",
    "                 cbar_size=\"5%\",\n",
    "                 cbar_pad=0.05,\n",
    "                 )\n",
    "    gridnos = np.arange(numrows * numcols).reshape((numrows, numcols)).flatten()\n",
    "    for j in range(numcols * numrows):\n",
    "        i = gridnos[j]\n",
    "        ax = grid[j]#fig.add_subplot(numrows,numcols,i+1)#, frameon=False)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        position = ax.get_position()\n",
    "#         print(position)\n",
    "        position.x0 += 0.01\n",
    "        position.y0 += 0.01\n",
    "        position.x1 += 0.01\n",
    "        position.y1 += 0.01\n",
    "        ax.set_position(position)\n",
    "#         print(position)\n",
    "        testname = info_dict['names'][i]\n",
    "        shortname = info_dict['shortnames'][i]\n",
    "        im = ax.imshow(info_dict['cm'][testname].T, vmin=0., vmax=1., cmap=fave_cmap)\n",
    "        ax.text(.8, .9, shortname, horizontalalignment='center', transform=ax.transAxes, fontsize=16)\n",
    "#     pylab.colorbar()\n",
    "#     fig.subplots_adjust(right=0.5)\n",
    "#     cbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\n",
    "#     fig.colorbar(im, cax=cbar_ax)\n",
    "    ax.cax.colorbar(im)\n",
    "    ax.cax.toggle_label(True)\n",
    "    bigAxes.set_ylabel(r'true class', fontsize=20)\n",
    "    bigAxes.set_xlabel(r'predicted class', fontsize=20)\n",
    "    pylab.tight_layout()\n",
    "    pylab.savefig('fig/all_'+fn+'_cm.png', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporarily omitting real data\n",
    "cm_mega_plot(snphotcc, fn='snphotcc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods (Metrics)\n",
    "======\n",
    "\n",
    "We considered two metrics of classification probabilities, each of which is interpretable and avoids reducing probabilities to point estimates\n",
    "\n",
    "The Brier score is defined as\n",
    "\\begin{eqnarray*}\n",
    "B &=& \\sum_{m=1}^{M}\\frac{w_{m}}{N_{m}}\\sum_{n=1}^{N_{m}}\\left((1-p_{n}(m | m))^{2}+\\sum_{m'\\neq m}^{M}(p_{n}(m' | m))^{2}\\right)\n",
    "\\end{eqnarray*}\n",
    "\n",
    "The log-loss is defined as\n",
    "\\begin{eqnarray*}\n",
    "L &=& -\\sum_{m=1}^{M}\\frac{w_{m}}{N_{m}}\\sum_{n=1}^{N_{m}}\\ln[p_{n}(m | m)]\n",
    "\\end{eqnarray*}\n",
    "\n",
    "We calculate the metric within each class $m$ by taking an average of its value $-\\ln[p_{n}(m | m)]$ for each true member $n$ of the class.  Then we weight the metrics for each class by an arbitrary weight $w_{m}$ and take a weighted average of the per-class metrics to produce a global scalar metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hideme"
    ]
   },
   "outputs": [],
   "source": [
    "metricslist = ['Brier', 'LogLoss']\n",
    "colors = [fave_cmap(0.), fave_cmap(1.)]\n",
    "markerlist = ['s', 'o']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights\n",
    "\n",
    "We may take weighted averages of the per-class metrics, and these weights may be considered in terms of the systematics we discussed, by upweighting or downweighting the \"chosen\" class most affected by the systematics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hideme"
    ]
   },
   "outputs": [],
   "source": [
    "chosen = {}\n",
    "chosen['weights'] = {}\n",
    "chosen['names'] = {'common': 0, 'rare': 4, 'subsumer': 5, 'subsumee': 6, 'invisible': 8}\n",
    "ideas = chosen['names'].keys()\n",
    "M_classes = 13\n",
    "\n",
    "for idea in chosen['names'].keys():\n",
    "    chosen['weights'][idea] = {}\n",
    "    cc = chosen['names'][idea]\n",
    "    flat_weight = np.ones(M_classes)\n",
    "    hi_weight = np.ones(M_classes) / np.float(M_classes)\n",
    "    hi_weight[cc] = 1.\n",
    "    lo_weight = np.ones(M_classes) \n",
    "    lo_weight[cc] = 1. / np.float(M_classes)\n",
    "    chosen['weights'][idea]['flat'] = flat_weight\n",
    "    chosen['weights'][idea]['up'] = hi_weight\n",
    "    chosen['weights'][idea]['down'] = lo_weight\n",
    "    chosen['weights'][idea]['per_class'] = 'per_class'\n",
    "    chosen['weights'][idea]['per_item'] = 'per_item'\n",
    "    \n",
    "schemes = ['flat', 'up', 'down', 'per_class', 'per_item']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results\n",
    "=======\n",
    "\n",
    "*one plot per set of \"true\" classes: classifiers on x axis, metrics on y axes*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mock classifier systematics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hideme"
    ]
   },
   "outputs": [],
   "source": [
    "def make_mock_results_fromcm(datadict):\n",
    "    new_datadict = datadict.copy()\n",
    "    for cc, name in enumerate(datadict['names']):\n",
    "        code = proclam.classifiers.FromCM()\n",
    "        probs = code.classify(datadict['cm'][name], truth, delta=datadict['delta'][name], other=False)\n",
    "        new_datadict['probs'][name] = probs\n",
    "    return(new_datadict)\n",
    "\n",
    "def make_mock_results_fromcmdm(datadict):\n",
    "    new_datadict = datadict.copy()\n",
    "    for cc, name in enumerate(datadict['names']):\n",
    "        code = proclam.classifiers.FromCMDM()\n",
    "        probs = code.classify(datadict['cm'][name], truth, delta=datadict['delta'][name], other=False)\n",
    "        new_datadict['probs'][name] = probs\n",
    "    return(new_datadict)\n",
    "\n",
    "# plasticc_fromcm = \n",
    "\n",
    "# # data = np.empty((len(metricslist), len(plasticc['names'])))\n",
    "# # plasticc['probs'] = {}\n",
    "# plasticc_fromcm = plasticc.copy()\n",
    "# plasticc_fromcmdm = plasticc.copy()\n",
    "# for cc, name in enumerate(plasticc['names']):\n",
    "#     code = proclam.classifiers.FromCM()\n",
    "#     probs = code.classify(plasticc['cm'][name], truth, delta=plasticc['delta'][name], other=False)\n",
    "#     plasticc_fromcm['probs'][name] = probs\n",
    "#     code = proclam.classifiers.FromCMDM()\n",
    "#     probs = code.classify(plasticc['cm'][name], truth, delta=plasticc['delta'][name], other=False)\n",
    "#     plasticc_fromcmdm['probs'][name] = probs\n",
    "# #     for count, metric in enumerate(metricslist):\n",
    "# #         D = getattr(proclam.metrics, metric)()\n",
    "# #         hm = D.evaluate(probs, truth, averaging='per_class')\n",
    "# #         data[count][cc] = hm\n",
    "# #     plasticc['probs'] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plasticc['probs'] = {}\n",
    "plasticc_fromcm = make_mock_results_fromcm(plasticc)\n",
    "plasticc_fromcmdm = make_mock_results_fromcmdm(plasticc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hideme"
    ]
   },
   "outputs": [],
   "source": [
    "# def make_patch_spines_invisible(ax):\n",
    "#     ax.set_frame_on(True)\n",
    "#     ax.patch.set_visible(False)\n",
    "#     for sp in ax.spines.values():\n",
    "#         sp.set_visible(False)\n",
    "        \n",
    "# def per_metric_helper(ax, n, data, metric_names, codes, shapes, colors):\n",
    "#     plot_n = n+1\n",
    "#     in_x = np.arange(len(codes))\n",
    "#     ax_n = ax\n",
    "#     n_factor = 0.1 * (plot_n - 2)\n",
    "#     if plot_n>1:\n",
    "#         ax_n = ax.twinx()\n",
    "#         rot_ang = 270\n",
    "#         label_space = 15.\n",
    "#     else:\n",
    "#         rot_ang = 90\n",
    "#         label_space = 0.\n",
    "#     if plot_n>2:\n",
    "#         ax_n.spines[\"right\"].set_position((\"axes\", 1. + 0.1 * (plot_n-1)))\n",
    "#         make_patch_spines_invisible(ax_n)\n",
    "#         ax_n.spines[\"right\"].set_visible(True)\n",
    "#     handle = ax_n.scatter(in_x+n_factor*np.ones_like(data[n]), data[n], marker=shapes[n], s=10, color=colors[n], label=metric_names[n])\n",
    "#     ax_n.set_ylabel(metric_names[n], rotation=rot_ang, fontsize=14, labelpad=label_space)\n",
    "# #     ax_n.set_ylim(0.9 * min(data[n]), 1.1 * max(data[n]))\n",
    "#     return(ax, ax_n, handle)\n",
    "\n",
    "# def metric_plot(dataset, res, metric_names, shapes, colors, modtext=''):\n",
    "#     codes = dataset['names']\n",
    "#     data = res\n",
    "#     text = dataset['label']\n",
    "# #     fileloc = dataset['dirname']+dataset['label']+'_results.png'\n",
    "#     xs = np.arange(len(codes))\n",
    "#     fig, ax = plt.subplots()\n",
    "#     fig.subplots_adjust(right=1.)\n",
    "#     handles = []\n",
    "#     for n in range(len(metric_names)):\n",
    "# #         print(np.shape(data[n]))\n",
    "#         (ax, ax_n, handle) = per_metric_helper(ax, n, data, metric_names, codes, shapes, colors)\n",
    "#         handles.append(handle)\n",
    "#     plt.xticks(xs, codes)\n",
    "#     for tick in ax.get_xticklabels():\n",
    "#         tick.set_rotation(90)\n",
    "#     plt.xlabel('Classifiers', fontsize=14)\n",
    "#     plt.legend(handles, metric_names, loc='lower left')\n",
    "#     fig.suptitle(text+modtext)\n",
    "#     plt.savefig(text+modtext+'.png')\n",
    "#     return\n",
    "\n",
    "# def make_patch_spines_invisible(ax):\n",
    "#     ax.set_frame_on(True)\n",
    "#     ax.patch.set_visible(False)\n",
    "#     for sp in ax.spines.values():\n",
    "#         sp.set_visible(False)\n",
    "        \n",
    "def per_metric_helper(ax, n, data, metric_names, codes, shapes, colors):\n",
    "    plot_n = n+1\n",
    "    in_x = np.arange(len(codes))\n",
    "    ax_n = ax\n",
    "    n_factor = 0.1 * (plot_n - 2)\n",
    "    if plot_n>1:\n",
    "        ax_n = ax.twinx()\n",
    "        rot_ang = 270\n",
    "        label_space = 15.\n",
    "    else:\n",
    "        rot_ang = 90\n",
    "        label_space = 0.\n",
    "#     if plot_n>2:\n",
    "#         ax_n.spines[\"right\"].set_position((\"axes\", 1. + 0.1 * (plot_n-1)))\n",
    "#         make_patch_spines_invisible(ax_n)\n",
    "#         ax_n.spines[\"right\"].set_visible(True)\n",
    "    handle = ax_n.scatter(in_x+n_factor*np.ones_like(data[n]), data[n], marker=shapes[n], s=10, color=colors[n], label=metric_names[n])\n",
    "    ax_n.set_ylabel(metric_names[n], fontsize=14)#s, labelpad=label_space, rotation=rot_ang)\n",
    "#     ax_n.set_ylim(0.9 * min(data[n]), 1.1 * max(data[n]))\n",
    "    return(ax, ax_n, handle)\n",
    "\n",
    "def multi_metric_plot(dataset, res, metric_names, shapes, colors, modtext='', fn='plot.png'):\n",
    "    codes = dataset['names']\n",
    "    data = res\n",
    "    text = dataset['label']\n",
    "#     fileloc = dataset['dirname']+dataset['label']+'_results.png'\n",
    "    xs = np.arange(len(codes))\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.subplots_adjust(right=1.)\n",
    "    handles = []\n",
    "    for n in range(len(metric_names)):\n",
    "#         print(np.shape(data[n]))\n",
    "        (ax, ax_n, handle) = per_metric_helper(ax, n, data, metric_names, codes, shapes, colors)\n",
    "        handles.append(handle)\n",
    "    plt.xticks(xs, codes)\n",
    "    for tick in ax.get_xticklabels():\n",
    "        tick.set_rotation(90)\n",
    "    plt.xlabel('Classifiers', fontsize=14)\n",
    "#     plt.ylabel('LogLoss')\n",
    "    plt.legend(handles, metric_names, loc='lower left')\n",
    "    fig.suptitle(text+modtext)\n",
    "#     plt.show()\n",
    "    plt.savefig('fig/multi_metric_'+fn, dpi=250)\n",
    "    plt.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(afflicted)\n",
    "# upweight most common\n",
    "# under no systematics, cruise control, tunnel vision\n",
    "per_item_tests = {}\n",
    "\n",
    "cm = plasticc['cm']['Almost'].copy()\n",
    "per_item_tests['Systematic-free'] = cm\n",
    "\n",
    "cm = np.ones((M_classes, M_classes))\n",
    "cm = cm * 0.1\n",
    "cm[:, -1] = 1. \n",
    "per_item_tests['Cruise'] = cm\n",
    "\n",
    "\n",
    "cruise = [0, 1]#afflicted[2:4]\n",
    "subsumed = [2, 3, 5, 6]#afflicted[4:8]\n",
    "tunnel = [4, 7]#afflicted[8:]\n",
    "noisy_cls = [-2, -1]#afflicted[:2]\n",
    "afflicted = cruise + subsumed + tunnel + noisy_cls\n",
    "\n",
    "cm = plasticc['cm']['Almost'].copy()\n",
    "cm[:, noisy_cls] = plasticc['cm']['Noisy'][:, noisy_cls]\n",
    "cm[subsumed[:2]] = cm[cruise[0]]#[np.newaxis] + np.asarray(0.01)[np.newaxis] * np.random.uniform(M_classes)[np.newaxis, :]\n",
    "cm[subsumed[2:]] = cm[cruise[1]]\n",
    "cm[tunnel] = plasticc['cm']['Perfect'][tunnel]\n",
    "# cm = cm / np.sum(cm, axis=1)[:, np.newaxis]\n",
    "# plasticc = wrap_up_classifier(cm, 'Combined', plasticc, delta=0.01)\n",
    "# print(np.sum(plasticc['cm']['Combined'], axis=1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "hidein"
    ]
   },
   "outputs": [],
   "source": [
    "for wt in schemes:\n",
    "    data = np.empty((len(metricslist), len(plasticc['names'])))\n",
    "    for cc, name in enumerate(plasticc['names']):\n",
    "        probs = plasticc['probs'][name]\n",
    "        for count, metric in enumerate(metricslist):\n",
    "            D = getattr(proclam.metrics, metric)()\n",
    "            hm = D.evaluate(probs, truth, averaging=wt)\n",
    "            data[count][cc] = hm\n",
    "#     plasticc['results'] = data\n",
    "    multi_metric_plot(plasticc, data, metricslist, markerlist, colors, \n",
    "                modtext=' '+wt+'weight', fn=wt+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hideme"
    ]
   },
   "outputs": [],
   "source": [
    "# generator = proclam.simulators.LogUnbalanced()\n",
    "# minitruth = generator.simulate(M_classes, N_objects)\n",
    "# minipops = np.histogram(minitruth, bins=range(M_classes+1))[0]/float(N_objects)\n",
    "# classifier = proclam.classifiers.FromCM()\n",
    "\n",
    "# minitest = {}\n",
    "# which_affected = range(M_classes)\n",
    "# systematic_info = {'agnostic':'s', 'tunnel':'o', 'almost':'+', 'noisy':'x', 'cruise':'d', 'subsumed':(3,0,0)}\n",
    "# which_systematics = systematic_info.keys()#['agnostic', 'tunnel', 'almost', 'noisy', 'tunnel', 'cruise', 'subsumed']\n",
    "# markerlist = systematic_info.values()#['', '', 'd', 'o', 'd', 's', (3,0,0)]\n",
    "\n",
    "# minidelta = 0.1\n",
    "# starter = np.eye(M_classes) + minidelta * np.ones((M_classes, M_classes))\n",
    "# starter = starter / np.sum(starter, axis=1)[:, np.newaxis]\n",
    "\n",
    "# which_weighted = ['by pop', 'flat', 'up', 'down']\n",
    "# wt_const = 1. / float(M_classes)\n",
    "    \n",
    "# which_metrics = ['Brier', 'LogLoss']\n",
    "# # markerlist = ['o', 'd', 's', (3,0,0)]#[['o', 'd', 's', (3,0,0)], ['.', 'x', '+', (3,1,90)]]\n",
    "\n",
    "# def make_cm(start_cm, m, systematic):\n",
    "#     # can wrap this in loop if systematic is a list\n",
    "#     cm = start_cm\n",
    "#     big_M = len(start_cm)\n",
    "#     if systematic == 'agnostic':\n",
    "#         cm[m] = np.ones(big_M)\n",
    "#     if systematic == 'almost':\n",
    "#         cm[m] = 0.5 * np.ones(big_M)\n",
    "#         cm[m][m] += 1.5\n",
    "#     if systematic == 'noisy':\n",
    "#         cm[m] = 0.5 * np.ones(big_M)\n",
    "#         cm[m][m] += 0.5\n",
    "#     if systematic == 'tunnel' or systematic == 'perfect':\n",
    "#         cm[m] = np.zeros(big_M)\n",
    "#         cm[:, m] = np.zeros(big_M).T\n",
    "#         cm[m][m] += 1.\n",
    "#     if systematic == 'cruise' or systematic == 'subsumer':\n",
    "#         cm[:, m] += 1.\n",
    "#     if systematic == 'subsumed':\n",
    "#         cm[m] = cm[m-1]\n",
    "#     cm = cm / np.sum(cm, axis=1)[:, np.newaxis]\n",
    "#     return cm\n",
    "\n",
    "# def make_wv(m, wt_kw):\n",
    "# #     wv = np.ones(M_classes)\n",
    "#     if wt_kw == 'by pop':\n",
    "#         wv = minipops\n",
    "#     if wt_kw == 'flat': \n",
    "#         wv = np.ones(M_classes)\n",
    "#     if wt_kw == 'up':\n",
    "#         wv = np.ones(M_classes)\n",
    "#         wv[m] += 1.\n",
    "#     if wt_kw == 'down':\n",
    "#         wv = np.ones(M_classes)\n",
    "#         wv[m] /= 2.\n",
    "#     wv = wv / np.sum(wv)\n",
    "#     assert(np.isclose(np.sum(wv), 1.))\n",
    "#     return wv\n",
    "\n",
    "# miniweights = np.empty((len(which_weighted), M_classes, M_classes))\n",
    "# for m in which_affected:\n",
    "#     for wi in range(len(which_weighted)):\n",
    "#         w = which_weighted[wi]\n",
    "#         miniweights[wi][m] = make_wv(m, w)\n",
    "\n",
    "# # for s in which_systematics:\n",
    "# #     minitest[s] = {}\n",
    "# #     for m in which_affected:\n",
    "# #         minitest[s][str(m)] = {}\n",
    "# #         minitest[s][str(m)]['cm'] = make_cm(starter, m, s)\n",
    "# #         minitest[s][str(m)]['probs'] = classifier.classify(minitest[s][str(m)]['cm'], minitruth, \n",
    "# #                                                            delta=minidelta, other=False)\n",
    "# #         minitest[s][str(m)]['results'] = {}\n",
    "# #         for met in which_metrics:\n",
    "# #             minitest[s][str(m)][met] = {}\n",
    "# #             for wi in range(len(which_weighted)):\n",
    "# #                 w = which_weighted[wi]\n",
    "# #                 D = getattr(proclam.metrics, met)()\n",
    "# #                 minitest[s][str(m)][met][w] = D.evaluate(minitest[s][str(m)]['probs'], minitruth, \n",
    "# #                                                          averaging=miniweights[wi][m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hideme"
    ]
   },
   "outputs": [],
   "source": [
    "# for s in which_systematics:\n",
    "#     minitest[s] = {}\n",
    "#     for m in which_affected:\n",
    "#         minitest[s][str(m)] = {}\n",
    "#         minitest[s][str(m)]['cm'] = make_cm(starter, m, s)\n",
    "#         minitest[s][str(m)]['probs'] = classifier.classify(minitest[s][str(m)]['cm'], minitruth, \n",
    "#                                                            delta=minidelta, other=False)\n",
    "#         minitest[s][str(m)]['results'] = {}\n",
    "#         for met in which_metrics:\n",
    "#             minitest[s][str(m)][met] = {}\n",
    "#             for wi in range(len(which_weighted)):\n",
    "#                 w = which_weighted[wi]\n",
    "#                 D = getattr(proclam.metrics, met)()\n",
    "#                 minitest[s][str(m)][met][w] = D.evaluate(minitest[s][str(m)]['probs'], minitruth, \n",
    "#                                                          averaging=miniweights[wi][m])\n",
    "#     print('finished '+s)\n",
    "#     cpkl.dump(minitest[s], open(s+'_data.pkl', 'wb'))\n",
    "#     print('saved '+s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hideme"
    ]
   },
   "outputs": [],
   "source": [
    "# cpkl.dump(minitest, open('intermediatedata.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hideme"
    ]
   },
   "outputs": [],
   "source": [
    "# minitest = {}\n",
    "# for s in which_systematics:\n",
    "#     minitest[s] = cpkl.load(open(s+'_data.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hideme"
    ]
   },
   "outputs": [],
   "source": [
    "# def complete_metric_plot(dataset, metric_names, shapes):\n",
    "    \n",
    "#     systematics = dataset.keys()\n",
    "#     xs = np.arange(len(systematics))\n",
    "#     fig, ax = plt.subplots(figsize=(5,5))\n",
    "    \n",
    "#     ax.text(.3, .95, r'color$\\sim \\log[N_{m}]$', \n",
    "#             verticalalignment='center', transform=ax.transAxes, \n",
    "#             fontsize=10)\n",
    "#     ax.text(.3, .9, r'size$\\sim w_{m}$', \n",
    "#             verticalalignment='center', transform=ax.transAxes, \n",
    "#             fontsize=10)\n",
    "#     fig.subplots_adjust(right=1.)\n",
    "# #     handles = []\n",
    "#     seeds = []\n",
    "#     for m in which_affected:\n",
    "#         one_seed = np.log10(max(1., N_objects * float(minipops[m]))) / 10.\n",
    "#         seeds.append(one_seed)\n",
    "#         for si in range(len(systematics)):\n",
    "#             s = systematics[si]\n",
    "#             for wi in range(len(which_weighted)):\n",
    "#                 w = which_weighted[wi]\n",
    "# #                 rel_wt = (np.average(miniweights[wi][m]) - miniweights[wi][m][m]) / np.average(miniweights[wi][m]) / 2. + 0.5\n",
    "# #                 print(rel_wt)\n",
    "#                 ax.scatter(dataset[s][str(m)][metric_names[0]][w], dataset[s][str(m)][metric_names[1]][w],\n",
    "#                   c=mpl.cm.winter_r(one_seed),\n",
    "#                   s=100.*miniweights[wi][m],\n",
    "#                   marker=markerlist[si],\n",
    "#                   alpha=0.25)\n",
    "#     for si in range(len(systematics)):\n",
    "#         ax.scatter(0., 0., c='k',\n",
    "#                   marker=markerlist[si],\n",
    "#                   alpha=0.25, label=systematics[si])\n",
    "    \n",
    "#     ax.set_xlabel(metric_names[0])\n",
    "#     ax.set_ylabel(metric_names[1])\n",
    "#     ax.set_ylim(0.95, 3.25)\n",
    "#     ax.set_xlim(0.045, 0.085)\n",
    "#     ax.legend(loc='lower right')\n",
    "    \n",
    "#     seeds = np.array(seeds)\n",
    "#     print(seeds)\n",
    "#     axins = inset_axes(ax,\n",
    "#                     width=\"25%\",  # width = 25% of parent_bbox width\n",
    "#                     height=\"5%\",  # height : 5%\n",
    "#                     loc=2)\n",
    "#     mpl.colorbar.ColorbarBase(axins, cmap=mpl.cm.winter_r,\n",
    "#                                 norm=mpl.colors.Normalize(vmin=0., vmax=1.), \n",
    "#                               orientation='horizontal')\n",
    "# #     axins.xaxis.set_ticks_position(\"top\")\n",
    "#     axins.xaxis.set_ticks(np.flip(seeds, axis=0))\n",
    "#     axins.xaxis.set_ticklabels([]) #rotation=270, fontsize=10)\n",
    "    \n",
    "# #     plt.show()\n",
    "#     plt.savefig('fig/all_effects_isolated.png', dpi=250)\n",
    "#     plt.close()\n",
    "#     return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(markerlist)\n",
    "# complete_metric_plot(minitest, which_metrics, markerlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hideme"
    ]
   },
   "outputs": [],
   "source": [
    "# omitting real data for now\n",
    "# def make_patch_spines_invisible(ax):\n",
    "#     ax.set_frame_on(True)\n",
    "#     ax.patch.set_visible(False)\n",
    "#     for sp in ax.spines.values():\n",
    "#         sp.set_visible(False)\n",
    "        \n",
    "# def per_metric_helper(ax, n, data, metric_names, codes, shapes, colors):\n",
    "#     plot_n = n+1\n",
    "#     in_x = np.arange(len(codes))\n",
    "#     ax_n = ax\n",
    "#     n_factor = 0.1 * (plot_n - 2)\n",
    "#     if plot_n>1:\n",
    "#         ax_n = ax.twinx()\n",
    "#         rot_ang = 270\n",
    "#         label_space = 15.\n",
    "#     else:\n",
    "#         rot_ang = 90\n",
    "#         label_space = 0.\n",
    "#     if plot_n>2:\n",
    "#         ax_n.spines[\"right\"].set_position((\"axes\", 1. + 0.1 * (plot_n-1)))\n",
    "#         make_patch_spines_invisible(ax_n)\n",
    "#         ax_n.spines[\"right\"].set_visible(True)\n",
    "#     handle = ax_n.scatter(in_x+n_factor*np.ones_like(data[n]), data[n], marker=shapes[n], s=10, color=colors[n], label=metric_names[n])\n",
    "#     ax_n.set_ylabel(metric_names[n], rotation=rot_ang, fontsize=14, labelpad=label_space)\n",
    "# #     ax_n.set_ylim(0.9 * min(data[n]), 1.1 * max(data[n]))\n",
    "#     return(ax, ax_n, handle)\n",
    "\n",
    "# def metric_plot(dataset, res, metric_names, shapes, colors, modtext=''):\n",
    "#     codes = dataset['names']\n",
    "#     data = res\n",
    "#     text = dataset['label']\n",
    "# #     fileloc = dataset['dirname']+dataset['label']+'_results.png'\n",
    "#     xs = np.arange(len(codes))\n",
    "#     fig, ax = plt.subplots()\n",
    "#     fig.subplots_adjust(right=1.)\n",
    "#     handles = []\n",
    "#     for n in range(len(metric_names)):\n",
    "# #         print(np.shape(data[n]))\n",
    "#         (ax, ax_n, handle) = per_metric_helper(ax, n, data, metric_names, codes, shapes, colors)\n",
    "#         handles.append(handle)\n",
    "#     plt.xticks(xs, codes)\n",
    "#     for tick in ax.get_xticklabels():\n",
    "#         tick.set_rotation(90)\n",
    "#     plt.xlabel('Classifiers', fontsize=14)\n",
    "#     plt.legend(handles, metric_names, loc='lower left')\n",
    "#     fig.suptitle(text+modtext)\n",
    "#     plt.savefig(text+modtext+'.png')\n",
    "#     return\n",
    "\n",
    "# def make_patch_spines_invisible(ax):\n",
    "#     ax.set_frame_on(True)\n",
    "#     ax.patch.set_visible(False)\n",
    "#     for sp in ax.spines.values():\n",
    "#         sp.set_visible(False)\n",
    "        \n",
    "def per_metric_helper(ax, n, data, metric_names, codes, shapes, colors):\n",
    "    plot_n = n+1\n",
    "    in_x = np.arange(len(codes))\n",
    "    ax_n = ax\n",
    "    n_factor = 0.1 * (plot_n - 2)\n",
    "#     if plot_n>1:\n",
    "#         ax_n = ax.twinx()\n",
    "#         rot_ang = 270\n",
    "#         label_space = 15.\n",
    "#     else:\n",
    "#         rot_ang = 90\n",
    "#         label_space = 0.\n",
    "#     if plot_n>2:\n",
    "#         ax_n.spines[\"right\"].set_position((\"axes\", 1. + 0.1 * (plot_n-1)))\n",
    "#         make_patch_spines_invisible(ax_n)\n",
    "#         ax_n.spines[\"right\"].set_visible(True)\n",
    "    handle = ax_n.scatter(in_x+n_factor*np.ones_like(data[n]), data[n], marker=shapes[n], s=10, color=colors[n], label=metric_names[n])\n",
    "#     ax_n.set_ylabel(metric_names[n], fontsize=14)#s, labelpad=label_space, rotation=rot_ang)\n",
    "#     ax_n.set_ylim(0.9 * min(data[n]), 1.1 * max(data[n]))\n",
    "    return(ax, ax_n, handle)\n",
    "\n",
    "def systematic_metric_plot(dataset, res, metric_names, shapes, colors, modtext='', fn='plot.png'):\n",
    "    codes = dataset['names']\n",
    "    data = res\n",
    "    text = dataset['label']\n",
    "#     fileloc = dataset['dirname']+dataset['label']+'_results.png'\n",
    "    xs = np.arange(len(codes))\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.subplots_adjust(right=1.)\n",
    "    handles = []\n",
    "    for n in range(len(metric_names)):\n",
    "#         print(np.shape(data[n]))\n",
    "        (ax, ax_n, handle) = per_metric_helper(ax, n, data, metric_names, codes, shapes, colors)\n",
    "        handles.append(handle)\n",
    "    plt.xticks(xs, codes)\n",
    "    for tick in ax.get_xticklabels():\n",
    "        tick.set_rotation(90)\n",
    "    plt.xlabel('Classifiers', fontsize=14)\n",
    "    plt.ylabel('LogLoss')\n",
    "    plt.legend(handles, metric_names, loc='upper left')\n",
    "    fig.suptitle(text+modtext)\n",
    "#     plt.show()\n",
    "    plt.savefig('fig/systematic_'+fn, dpi=250)\n",
    "    plt.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hidein"
    ]
   },
   "outputs": [],
   "source": [
    "# omitting real data for now\n",
    "# loop over datasets, loop over \n",
    "\n",
    "data = np.empty((len(chosen['names'].keys()), len(schemes), len(mystery['names'])))\n",
    "# data = np.empty((len(schemes), len(chosen['names'].keys()), len(mystery['names'])))\n",
    "\n",
    "for cc, idea in enumerate(chosen['names'].keys()):\n",
    "    for sc, scheme in enumerate(schemes):\n",
    "# for sc, scheme in enumerate(schemes): \n",
    "#     for cc, idea in enumerate(chosen['names'].keys()):\n",
    "        for nm, name in enumerate(mystery['names']):\n",
    "            probm = mystery['probs'][name]\n",
    "            truthv = mystery['truth'][name]\n",
    "            D = getattr(proclam.metrics, 'LogLoss')()\n",
    "            weighting = idea+' '+scheme\n",
    "            hm = D.evaluate(prediction=probm, truth=truthv, averaging=chosen['weights'][idea][scheme])\n",
    "            data[cc][sc][nm] = hm\n",
    "#             data[sc][cc][nm] = hm\n",
    "        print(weighting+': '+str(data[cc][sc]))\n",
    "#         print(weighting+': '+str(data[sc][cc]))\n",
    "    systematic_metric_plot(mystery, data[cc], schemes, markerlist, colors, \n",
    "                modtext=' '+idea+' weight', fn=mystery['label']+'_'+idea+'.png')\n",
    "#     metric_plot(mystery, data[sc], list(ideas), markerlist, colors, modtext=' '+scheme+' weight', fn=scheme+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hideme"
    ]
   },
   "outputs": [],
   "source": [
    "# omitting real data for now\n",
    "# def make_patch_spines_invisible(ax):\n",
    "#     ax.set_frame_on(True)\n",
    "#     ax.patch.set_visible(False)\n",
    "#     for sp in ax.spines.values():\n",
    "#         sp.set_visible(False)\n",
    "        \n",
    "# def per_metric_helper(ax, n, data, metric_names, codes, shapes, colors):\n",
    "#     plot_n = n+1\n",
    "#     in_x = np.arange(len(codes))\n",
    "#     ax_n = ax\n",
    "#     n_factor = 0.1 * (plot_n - 2)\n",
    "#     if plot_n>1:\n",
    "#         ax_n = ax.twinx()\n",
    "#         rot_ang = 270\n",
    "#         label_space = 15.\n",
    "#     else:\n",
    "#         rot_ang = 90\n",
    "#         label_space = 0.\n",
    "#     if plot_n>2:\n",
    "#         ax_n.spines[\"right\"].set_position((\"axes\", 1. + 0.1 * (plot_n-1)))\n",
    "#         make_patch_spines_invisible(ax_n)\n",
    "#         ax_n.spines[\"right\"].set_visible(True)\n",
    "#     handle = ax_n.scatter(in_x+n_factor*np.ones_like(data[n]), data[n], marker=shapes[n], s=10, color=colors[n], label=metric_names[n])\n",
    "#     ax_n.set_ylabel(metric_names[n], rotation=rot_ang, fontsize=14, labelpad=label_space)\n",
    "# #     ax_n.set_ylim(0.9 * min(data[n]), 1.1 * max(data[n]))\n",
    "#     return(ax, ax_n, handle)\n",
    "\n",
    "# def metric_plot(dataset, res, metric_names, shapes, colors, modtext=''):\n",
    "#     codes = dataset['names']\n",
    "#     data = res\n",
    "#     text = dataset['label']\n",
    "# #     fileloc = dataset['dirname']+dataset['label']+'_results.png'\n",
    "#     xs = np.arange(len(codes))\n",
    "#     fig, ax = plt.subplots()\n",
    "#     fig.subplots_adjust(right=1.)\n",
    "#     handles = []\n",
    "#     for n in range(len(metric_names)):\n",
    "# #         print(np.shape(data[n]))\n",
    "#         (ax, ax_n, handle) = per_metric_helper(ax, n, data, metric_names, codes, shapes, colors)\n",
    "#         handles.append(handle)\n",
    "#     plt.xticks(xs, codes)\n",
    "#     for tick in ax.get_xticklabels():\n",
    "#         tick.set_rotation(90)\n",
    "#     plt.xlabel('Classifiers', fontsize=14)\n",
    "#     plt.legend(handles, metric_names, loc='lower left')\n",
    "#     fig.suptitle(text+modtext)\n",
    "#     plt.savefig(text+modtext+'.png')\n",
    "#     return\n",
    "\n",
    "# def make_patch_spines_invisible(ax):\n",
    "#     ax.set_frame_on(True)\n",
    "#     ax.patch.set_visible(False)\n",
    "#     for sp in ax.spines.values():\n",
    "#         sp.set_visible(False)\n",
    "        \n",
    "def per_metric_helper(ax, n, data, metric_names, codes, shapes, colors):\n",
    "    plot_n = n+1\n",
    "    in_x = np.arange(len(codes))\n",
    "    ax_n = ax\n",
    "    n_factor = 0.1 * (plot_n - 2)\n",
    "#     if plot_n>1:\n",
    "#         ax_n = ax.twinx()\n",
    "#         rot_ang = 270\n",
    "#         label_space = 15.\n",
    "#     else:\n",
    "#         rot_ang = 90\n",
    "#         label_space = 0.\n",
    "#     if plot_n>2:\n",
    "#         ax_n.spines[\"right\"].set_position((\"axes\", 1. + 0.1 * (plot_n-1)))\n",
    "#         make_patch_spines_invisible(ax_n)\n",
    "#         ax_n.spines[\"right\"].set_visible(True)\n",
    "    handle = ax_n.scatter(in_x+n_factor*np.ones_like(data[n]), data[n], marker=shapes[n], s=10, color=colors[n], label=metric_names[n])\n",
    "#     ax_n.set_ylabel(metric_names[n], fontsize=14)#s, labelpad=label_space, rotation=rot_ang)\n",
    "#     ax_n.set_ylim(0.9 * min(data[n]), 1.1 * max(data[n]))\n",
    "    return(ax, ax_n, handle)\n",
    "\n",
    "def weightby_metric_plot(dataset, res, metric_names, shapes, colors, modtext='', fn='plot.png'):\n",
    "    codes = dataset['names']\n",
    "    data = res\n",
    "    text = dataset['label']\n",
    "#     fileloc = dataset['dirname']+dataset['label']+'_results.png'\n",
    "    xs = np.arange(len(codes))\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.subplots_adjust(right=1.)\n",
    "    handles = []\n",
    "    for n in range(len(metric_names)):\n",
    "#         print(np.shape(data[n]))\n",
    "        (ax, ax_n, handle) = per_metric_helper(ax, n, data, metric_names, codes, shapes, colors)\n",
    "        handles.append(handle)\n",
    "    plt.xticks(xs, codes)\n",
    "    for tick in ax.get_xticklabels():\n",
    "        tick.set_rotation(90)\n",
    "    plt.xlabel('Classifiers', fontsize=14)\n",
    "    plt.ylabel('LogLoss')\n",
    "    plt.legend(handles, metric_names, loc='upper left')\n",
    "    fig.suptitle(text+modtext)\n",
    "#     plt.show()\n",
    "    plt.savefig('fig/weightby_'+fn, dpi=250)\n",
    "    plt.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hidein"
    ]
   },
   "outputs": [],
   "source": [
    "# omitting real data for now\n",
    "# data = np.empty((len(chosen['names'].keys()), len(schemes), len(mystery['names'])))\n",
    "data = np.empty((len(schemes), len(chosen['names'].keys()), len(mystery['names'])))\n",
    "\n",
    "# for cc, idea in enumerate(chosen['names'].keys()):\n",
    "#     for sc, scheme in enumerate(schemes):\n",
    "for sc, scheme in enumerate(schemes): \n",
    "    for cc, idea in enumerate(chosen['names'].keys()):\n",
    "        for nm, name in enumerate(mystery['names']):\n",
    "            probm = mystery['probs'][name]\n",
    "            truthv = mystery['truth'][name]\n",
    "            D = getattr(proclam.metrics, 'LogLoss')()\n",
    "            weighting = idea+' '+scheme\n",
    "            hm = D.evaluate(prediction=probm, truth=truthv, averaging=chosen['weights'][idea][scheme])\n",
    "#             data[cc][sc][nm] = hm\n",
    "            data[sc][cc][nm] = hm\n",
    "#         print(weighting+': '+str(data[cc][sc]))\n",
    "        print(weighting+': '+str(data[sc][cc]))\n",
    "#     metric_plot(mystery, data[cc], schemes, markerlist, colors, modtext=' '+idea+' weight', fn=idea+'.png')\n",
    "    weightby_metric_plot(mystery, data[sc], list(ideas), markerlist, colors, \n",
    "                modtext=' '+scheme+' weight', fn=mystery['label']+'_'+scheme+'.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Would like to do this many times to generate error bars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try with different weights relative to randomly chosen class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real classification results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hideme"
    ]
   },
   "outputs": [],
   "source": [
    "# temporarily omitting real data\n",
    "\n",
    "def make_patch_spines_invisible(ax):\n",
    "    ax.set_frame_on(True)\n",
    "    ax.patch.set_visible(False)\n",
    "    for sp in ax.spines.values():\n",
    "        sp.set_visible(False)\n",
    "        \n",
    "def per_metric_helper(ax, n, data, metric_names, codes, shapes, colors):\n",
    "    plot_n = n+1\n",
    "    in_x = np.arange(len(codes))\n",
    "    ax_n = ax\n",
    "    n_factor = 0.1 * (plot_n - 2)\n",
    "    if plot_n>1:\n",
    "        ax_n = ax.twinx()\n",
    "        rot_ang = 270\n",
    "        label_space = 15.\n",
    "    else:\n",
    "        rot_ang = 90\n",
    "        label_space = 0.\n",
    "    if plot_n>2:\n",
    "        ax_n.spines[\"right\"].set_position((\"axes\", 1. + 0.1 * (plot_n-1)))\n",
    "        make_patch_spines_invisible(ax_n)\n",
    "        ax_n.spines[\"right\"].set_visible(True)\n",
    "    handle = ax_n.scatter(in_x+n_factor*np.ones_like(data[n]), data[n], marker=shapes[n], s=10, color=colors[n], label=metric_names[n])\n",
    "    ax_n.set_ylabel(metric_names[n], fontsize=14, rotation=rot_ang)#s, labelpad=label_space, rotation=rot_ang)\n",
    "#     ax_n.set_ylim(0.9 * min(data[n]), 1.1 * max(data[n]))\n",
    "    return(ax, ax_n, handle)\n",
    "\n",
    "def metric_plot(dataset, res, metric_names, shapes, colors, modtext='', fn='plot.png'):\n",
    "    codes = dataset['names']\n",
    "    data = res\n",
    "    text = dataset['label']\n",
    "#     fileloc = dataset['dirname']+dataset['label']+'_results.png'\n",
    "    xs = np.arange(len(codes))\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.subplots_adjust(right=1.)\n",
    "    handles = []\n",
    "    for n in range(len(metric_names)):\n",
    "#         print(np.shape(data[n]))\n",
    "        (ax, ax_n, handle) = per_metric_helper(ax, n, data, metric_names, codes, shapes, colors)\n",
    "        handles.append(handle)\n",
    "    plt.xticks(xs, codes)\n",
    "    for tick in ax.get_xticklabels():\n",
    "        tick.set_rotation(90)\n",
    "    plt.xlabel('Classifiers', fontsize=14)\n",
    "#     plt.ylabel('LogLoss')\n",
    "    plt.legend(handles, metric_names, loc='upper left')\n",
    "    fig.suptitle(text+modtext)\n",
    "#     plt.show()\n",
    "    plt.savefig('fig/'+fn, dpi=250)\n",
    "    plt.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hidein"
    ]
   },
   "outputs": [],
   "source": [
    "# temporarily excluding real data\n",
    "for dataset in [mystery, snphotcc]:\n",
    "    data = np.empty((len(metricslist), len(dataset['names'])))\n",
    "    for cc, name in enumerate(dataset['class_pairs'].keys()):\n",
    "        probm, truthv = read_class_pairs(dataset['class_pairs'][name], dataset, cc)\n",
    "        for count, metric in enumerate(metricslist):\n",
    "            D = getattr(proclam.metrics, metric)()\n",
    "            hm = D.evaluate(probm, truthv)\n",
    "            data[count][cc] = hm\n",
    "#     dataset['results'] = data\n",
    "    metric_plot(dataset, data, metricslist, markerlist, colors, fn=dataset['label']+'_res.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions\n",
    "===========\n",
    "\n",
    "We conclude that the Brier and log-loss metrics convey different information but are more or less consistent with our intuition for what makes a good classifier.  The Brier metric includes a penalty term not present in the log-loss but somehow is always consistent with the log-loss, meaning the penalty term doesn't really make a difference.  The log-loss has a larger dynamic range, which seems good but probably isn't that big a deal either."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acknowledgments\n",
    "===============\n",
    "\n",
    "The DESC acknowledges ongoing support from the Institut National de Physique Nucleaire et de Physique des Particules in France; the Science & Technology Facilities Council in the United Kingdom; and the Department of Energy, the National Science Foundation, and the LSST Corporation in the United States.\n",
    "\n",
    "DESC uses resources of the IN2P3 Computing Center (CC-IN2P3--Lyon/Villeurbanne - France) funded by the Centre National de la Recherche Scientifique; the National Energy Research Scientific Computing Center, a DOE Office of Science User Facility supported by the Office of Science of the U.S. Department of Energy under Contract No. DE-AC02-05CH11231; STFC DiRAC HPC Facilities, funded by UK BIS National E-infrastructure capital grants; and the UK particle physics grid, supported by the GridPP Collaboration.\n",
    "\n",
    "This work was performed in part under DOE Contract DE-AC02-76SF00515."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contributions\n",
    "=======\n",
    "\n",
    "Alex Malz: conceptualization, data curation, formal analysis, investigation, methodology, project administration, software, supervision, validation, visualization, writing - original draft\n",
    "\n",
    "Renee Hlozek: data curation, formal analysis, funding acquisition, investigation, project administration, software, supervision, validation, visualization, writing - original draft\n",
    "\n",
    "Tarek Alam: investigation, software, validation\n",
    "\n",
    "Anita Bahmanyar: formal analysis, investigation, methodology, software, writing - original draft\n",
    "\n",
    "Rahul Biswas: conceptualization, methodology, software\n",
    "\n",
    "Emille Ishida: conceptualization, project administration, supervision\n",
    "\n",
    "David Jones: software\n",
    "\n",
    "Ashish Mahabal: data curation, software\n",
    "\n",
    "Rafael Martinez-Galarza: data curation, software, visualization\n",
    "\n",
    "Gautham Narayan: data curation, formal analysis\n",
    "\n",
    "Christian Setzer: initial metric discussions/suggestions, software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hideme"
    ]
   },
   "outputs": [],
   "source": [
    "# cells with a tag of \"hideme\" will not appear in html resulting from:\n",
    "# jupyter nbconvert desc_note/main.ipynb --TagRemovePreprocessor.remove_cell_tags='[\"hideme\"]'\n",
    "# jupyter nbconvert desc_note/main.ipynb --TagRemovePreprocessor.remove_input_tags='[\"hidein\"]'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "proclam (Python 3)",
   "language": "python",
   "name": "proclam_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
