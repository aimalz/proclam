\section{Data}
\label{sec:data}

We explore the behavior of metrics on mock classification probabilities with isolated strengths and weaknesses as well as realistic mock classification probabilities from a publicly available light curve catalog.
Throughout this paper, \textit{data} always refers to mock classification submissions to \plasticc, not the \plasticc\ light curves; no light curves were simulated, viewed, or classified in the preparation of this paper.

Our data is in the form of catalogs of $N$ posterior probability vectors $p(m \mid d_{n}, D, \mathcal{C})$ over $M$ classes with labels $m$ conditioned on each observed light curve $d_{n}$, the training set $D$, and some parameters $\mathcal{C}$ concerning the behavior of the classifier.
We motivate $\mathcal{C}$ here before deferring its detailed explanation to later in Section~\ref{sec:mockdata}.

If a mock classifier produced $p(m \mid d_{n})$, it would take solely the light curve and produce a posterior over classes.
Since such a situation involves no information besides the light curve $d_{n}$, every classifier would produce identical classification submissions $\bar{p}(m \mid d_{n})$.
Including the training set $D$ would not remedy the problem, as every classifier for \plasticc\ has access to the same training set and so would still have no way to produce different classification submissions $p(m \mid d_{n}, D)$.
Thus there must be some other parameters $\mathcal{C}$ that are specific to each classifier and contribute to the mock classification posteriors it produces.\footnote{It should be noted that classification submissions may not be derived in this way, i.e. the parameters $\mathcal{C}$ may not be explicitly known or may indicate a procedure that does not produce posteriors but, rather, scores of some kind.  However, we assume for these purposes that classifiers produce the classification posteriors \plasticc\ seeks.}
We describe below the way in which mock data is synthesized and return to the classifier parameters $\mathcal{C}$ later.

\begin{figure}
	\begin{center}
    \includegraphics[width=0.49\textwidth]{./fig/complete_counts.png}
		\caption{\sout{The number of objects in a given class as a function of class population size.
		The true class populations are logarithmically distributed.} \changes{Class populations used in this work shown as the number of objects in each of the 13 classes: This was simulated by drawing the number of objects in a given class from a logarithmic distribution to emulate the class imbalance frequently observed in typical astronomical samples}}
		\label{fig:classdist}
	\end{center}
\end{figure}

As is anticipated of the real \lsst\ dataset, we use class populations that are logarithmically distributed such that they span many orders of magnitude.
We then take $M$ draws $u_{m} \sim \mathcal{U}(0, 1)$ from the standard continuous uniform distribution.
These draws $\{u_{m}\}$ are used to establish a discrete probability distribution $p(m) = b^{u_{m}}\ / \sum_{m} b^{u_{m}}$ such that $\sum_{m=1}^{M} p(m) = 1$.
From $p(m)$ we draw $N = 10^{b}$ instances $\{m'_{n}\}$ of a true class $m'$ for each light curve $n$ in the catalog.

The true class membership distribution of our tests with $M = 13$ and $b = 6$ is shown in Figure~\ref{fig:classdist}.
Though the class labels for \plasticc\ are expected to be randomized, we artificially order our mock class labels by their prevalence for ease of visual interpretation.
Once the true classes have been set, mock classification probabilities for each class are derived using the procedure described in Section~\ref{sec:mockdata}.

\subsection{Mock classification schemes}
\label{sec:mockdata}

In order to observe metric performance on different classification schemes, we simulate some archetypical mock classifiers, devised to produce generic responses to a classification challenge, without any interaction with actual challenge data, nor any other light curves.
We use these mock classifiers to investigate how the performance under each metric changes in the presence of certain types of failure modes, or \textit{systematics}.
A robust metric should not reward classification schemes that display these systematic effects.
% In other words, we want a metric that deters ``gaming the system'' as much as possible.
%The cases of this section are devised to test if a metric is congruent with our intuitive understanding of what constitutes a good classifier, that it should not reward classifications suffering from the most concerning failure modes, which we will refer to as \textit{systematics} throughout the paper.
%% TODO change systematics to symptoms?
%Our impressions of systematics can be seen as reflections of the conditional probability matrix, a familiar metric of deterministic classification \citep{2012PASP..124.1175B}.

The archetypical systematics can be seen as modifications to the confusion matrix, a measure of deterministic classification \citep{bloom_automating_2012}.
The confusion matrix is an $M \times M$ table of observed counts (or, if normalized, rates) of pairs of estimated class labels $\hat{m}$ (columns) and true classes $m'$ (rows) computed after a deterministic classification has been performed on some data set with $N$ objects.

Under a binary deterministic classification between positive and negative possibilities, the confusion matrix contains the numbers of true positives $\mathrm{TP}$, false positives $\mathrm{FP}$ (Type 1 error), true negatives $\mathrm{TN}$, and false negatives $\mathrm{FN}$ (Type 2 error), which can be turned into rates relative to the true numbers of positive and negative instances.
These rates may serve as building blocks for more sophisticated metrics of multi-class deterministic classifiers addressed in Section~\ref{sec:methods}.
Though probabilistic classifications are not compatible with the confusion matrix, regardless of normalization, we design tests around proposed normalized confusion matrices exhibiting various systematics that we anticipate being problematic for \lsst.

Under a deterministic classification scheme with a normalized confusion matrix with elements $p(\hat{m}, m')$, an object with true class $m'$ would have an assigned class $\hat{m}$ drawn from $p(\hat{m} \mid m') = p(\hat{m}, m') / p(m')$, via Bayes' Rule.
We note that the elements of the confusion matrix have values of $N p(\hat{m}, m')$ and that $p(m') = N_{m'} / N$, where $N_{m'}$ is the number of true members of class $m'$, must be known in order to produce a confusion matrix.
% We can thus easily obtain classification posteriors $p(\hat{m} \mid m')$ to use as the basis for classification probability submissions resulting from a confusion matrix.
We refer to the matrix $\mathbb{C}$ composed of $p(\hat{m} \mid m')$ as the \textit{conditional probability matrix} (CPM), and we use it to derive mock classification posteriors.

% Without loss of generality, we can decompose $\mathbb{C}$ under some basis functions of parameters $\mathcal{C}$, the same as those introduced above in the definition of a classification posterior $p(m \mid d_{n}, D, \mathcal{C})$.
% The CPM $\mathbb{C}$ thus defines the behavior of a classifier.
Assuming the light curves contain information about the true class (an assumption that underlies classification as a whole), we can use the appropriate row $\mathbb{C}_{m'_{n}} = p(\hat{m} \mid m', \mathcal{C})$ of the CPM $\mathbb{C}$ as a proxy for $p(m \mid d_{n}, D, \mathcal{C})$, without directly classifying light curves themselves.\footnote{This assumption is key to the generality of this work, which was condicted without any knowledge of the \plasticc\ dataset simulation procedure.}
To emulate the effect of natural variation of information content in different light curves (e.g. a noisy lightcurve has less information to recover than one with a higher signal-to-noise ratio) using the above, we generate a posterior probability vector $\vec{p}(m \mid m', \mathbb{C})$ by taking a Dirichlet-distributed draw
\begin{eqnarray}
  \label{eq:cmtoprob}
  \vec{p}(m \mid d_{n}, D, \mathcal{C}) &\sim& \mathrm{Dir}[\mathbb{C}_{m'_{n}} \delta]
	 % \frac{\mathrm{Gamma}(\mathbb{C}_{m_{n}'} \delta)}{\sum_{m'} \mathrm{Gamma}(\mathbb{C}_{m'} \delta)}
\end{eqnarray}
about $\mathbb{C}_{m'_{n}}$, with a small nonnegative perturbation factor $\delta = 0.01$.
In this way, the posterior probability vector has an expected value equal to the appropriate row in the CPM, with a variance set by $\delta$.
% \begin{eqnarray}
%  \label{eq:gamma}
%  \Gamma(\alpha) &=&
% \end{eqnarray}
We impose one restriction in addition to the normalization factor of Equation~\ref{eq:cmtoprob}, namely that all elements of $p(m \mid d_{n}, D, \mathcal{C})$ exceed $10^{-8}$, to ensure numerical stability in light of the limitations of floating point precision.

% TODO: letters for panels, refer to them in text
\begin{figure*}
	\begin{center}
    \includegraphics[width=0.8\textwidth]{./fig/all_sim_cm.png}
		\caption{Conditional probability matrices \changes{(CPMs)}for eight mock classifiers.
		Top row:
		the uncertain classifier's uniform CPM;
		the perfect classifier's identity CPM;
		the almost perfect classifier's CPM, a linear combination of one part uniform and four parts identity;
		the noisy classifier's CPM, a linear combination of one part uniform and two parts identity.
		Bottom row:
		the tunnel vision classifier's CPM is uniform except at the row and column corresponding to one class, where it takes the values of the identity matrix;
		the cruise control classifier's CPM, which has the every row equal to a particular row of the identity;
		the subsuming classifier's CPM, which has two or more rows equal to one another;
		the mutually subsuming classifier's CPM, a symmetric case of the subsuming classifier.
		The top row shows CPMs that serve as unbiased control cases.
		The CPMs of the bottom row represent concerning systematics that we would like to ensure are not rewarded by the \plasticc\ metric.
		% The left panels show uncertain (top) and perfect (bottom) classifications, while the second panel from the left shows the almost perfect (top) and unbiased but noise (bottom) classifications. The third panel from the left illustrate the perfect classifications that generate Type 1 (False Positive) and Type 2 (False Negative) errors for one class and uniform for all others (the `tunnel classifier', top row), while the bottom row illustrates the classification where all objects are assigned to the same class (the `cruise' classifier). Finally, the panels on the far right show classifications where one class is consistently assigned to another class (subsumed to, top row), and where one consistently assigns another class to one particular class (subsumed from, bottom row). Such classifications arise when one is optimising for a particular science case (e.g., one might prioritize a tunnel classification scheme), or where one object is considered much more `valuable' to classify than others (in which case one would employ a cruise classification scheme.)
		}
		\label{fig:mock_cm}
	\end{center}
\end{figure*}

We consider eight mock classifiers, each characterized by a single systematic affecting their CPM.
Figure~\ref{fig:mock_cm} shows the CPMs corresponding to each systematic considered, discussed in detail below.

For each of our archetypical mock classifiers, we address:
\begin{enumerate}
  \item What characteristic behavior defines this classifier?
  \item Under what conditions does this behavior arise in real classifications?
  \item What are our expectations of and desires for response of the metric to this archetypical classifier?
\end{enumerate}

\begin{figure}
	\begin{center}
		\includegraphics[width=0.49\textwidth]{./fig/combined.png}\\
    \includegraphics[width=0.49\textwidth]{./fig/examples.png}
		\caption{A realistically complex conditional probability matrix \changes{(CPM)} and classification posteriors drawn from it.
		Top: An example of a realistically complex conditional probability matrix, constructed by selecting a systematic for each individual class.
		This illustrates (for example), how a classifier may exhibit multiple systematics from Figure~\ref{fig:mock_cm} for each true class.
		Bottom: Example classification probabilities, drawn from the above CPM, with their true class indicated by a red star and the systematic, characterized by its row in the CPM, affecting that true class described on the right.
		The Dirichlet process emulates the variation in classification posteriors due to differences between light curves within a given class, leading to different classification posteriors even among rows sharing a true class.
		}
		\label{fig:mock_probs}
	\end{center}
\end{figure}

An actual classifier is expected to be more complex than the simplified cases of Figure~\ref{fig:mock_cm}, with different systematic behavior for each class.
An example of a combined CPM across different classes and systematics is given in the top panel of Figure~\ref{fig:mock_probs}.
The rows of this CPM correspond to rows of the archetypical classifiers of Figure~\ref{fig:mock_cm}.
To demonstrate the procedure by which mock classification posteriors are generated from rows of the CPM, we provide 22 examples of draws of the posterior CPM in the bottom panel of Figure~\ref{fig:mock_probs}.
Given a set of true class identities, the mock classification posteriors of the bottom panel are Dirichlet draws from the corresponding row of the CPM of the top panel.

\subsubsection{Uncertain classification}
\label{sec:uncertaindata}

A CPM $\mathbb{U}$ with uniform probabilities for all classes, as shown in the leftmost top panel of Figure~\ref{fig:mock_cm}, would correspond to uniform random guesses for deterministic classification, but in accordance with Equation~\ref{eq:cmtoprob}, the classification posteriors are perturbations away from a uniform distribution across all classes.
The peak values of one such classification posterior would correspond to random classification drawn from a uniform distribution, with $p(m' \mid d_{n}, D, \mathcal{C}_{\mathbb{U}}) \approx M^{-1}$.
We can consider the \textit{uncertain} classifier as an experimental control for the least effective possible classification scheme, bearing in mind that if classifications were anticorrelated with true classes, the experimenter could simply reassign the classification labels to improve performance under any metric.

\subsubsection{Accurate classification}
\label{sec:accuratedata}

The \textit{perfect} classifier has a diagonal CPM $\mathbb{I}$ (left-center top panel of Figure~\ref{fig:mock_cm}), which would correspond to deterministic classifications that are always correct.
In terms of probabilistic classifications, a perfect result would be a classification posterior with 1 for the true class and 0 for all other classes.
In accordance with the classification posterior synthesis scheme of Equation~\ref{eq:cmtoprob}, the class with maximum probability is almost always still the true class, and indeed with $N \sim 10^{6}$ and $\delta = 0.01$, this is always true.
This case is also a control, in that \plasticc\ would not be necessary if we believed the perfect classifier were potentially achievable.

In addition to a perfect classifier, we test linear combinations $\mathbb{C} = (s + 1)^{-1} \left(s\mathbb{I} + \mathbb{U}\right)$ of the perfect and uncertain CPMs where the contribution of the perfect classifier is greater than that of the uncertain classifier by a factor of $s > 0$.
Deterministic classifications drawn from such a CPM would be correct $s$ times as often as they take any one wrong label, and the incorrect labels would be uncorrelated across classes.
The classification posteriors drawn from such CPMs would have some probability at classes other than the true class, but almost all would still have their peak value at their true class.
% While $s$ would realistically be expected to vary for each class, we do not conduct a systematic investigation of this possibility at this time, and instead fix $s$ as constant for any given mock classifier.
We consider the case of the \textit{almost perfect} classifier with $s=4$ (right-center top panel of Figure~\ref{fig:mock_cm}) and the \textit{noisy} classifier with $s=2$ (rightmost top panel of Figure~\ref{fig:mock_cm}).

A classifier with different accuracy for each class may be considered a systematic in its own right.
An extreme example of such a classifier is one with perfect classification performance on one class and uncertain classification on all others.
This classifier's CPM would be uniform except for one row, which would take a value of unity on the diagonal and zero elsewhere; if the classifier were also resilient against Type 1 errors, the CPM would also take zeros along the column in question, aside from the value of unity on the diagonal.
For a single science application, this type of classifier is desirable, but the goal of \plasticc\ is to serve the needs of those who study a wide variety of classes for different purposes.
Hence, from the perspective of \plasticc, we seek a metric that disfavors the \textit{tunnel vision} classifier (leftmost bottom panel of Figure~\ref{fig:mock_cm}).
%However favoritism is inappropriate for the overall \plasticc\ metric, which must serve the needs of those who study all the classes and for different purposes.

\subsubsection{Inaccurate classification}
\label{sec:inaccuratedata}

If a deterministic classifier is systematically inaccurate, its CPM has significant off-diagonal contributions.
We model inaccurate probabilistic classifications of class $m'$ by using the row of the CPM corresponding to class $\tilde{m}$ as the basis for the perturbed probability vector $p(m \mid m') = p(m \mid \tilde{m})$.
Class $m'$ is said to be \textit{subsumed} by class $\tilde{m}$ by a classifier that absorbs class $m'$ into class $\tilde{m}$ (right-central bottom panel of Figure~\ref{fig:mock_cm}).
The subsuming classifier may be asymmetric, or the classes may be mutually subsumed (rightmost bottom panel of Figure~\ref{fig:mock_cm}) if one already has significant off-diagonal probability, as is true for the uncertain classifier.

Subsuming is not always the mark of a poor classifier and may be insurmountable by more sophisticated classification techniques.
Real classification posteriors $p(m \mid d_{n}, D, \mathcal{C})$ are conditioned on light curves, training data, and assumptions necessary for the classification algorithm, and there may simply not be enough information in a light curve and/or training set to distinguish between classes.
% Aside from hierarchical classes, limited data can cause even wholly different physical phenomena to be indistinguishable.

For example, based on only the first few light curve points, it is sometimes impossible to separate cataclysmic variables (stars that are not destroyed and can brighten and fade many times) from supernovae, which are stars that are completely destroyed in their explosions.
Even with observations over extended periods, it can still be impossible to distinguish cataclysmic variables from active galactic nuclei that result from activity near a galaxy's central black hole.
Similarly, tidal disruption events that occur when stars are destroyed by proximity to the central black hole of a galaxy can look much like supernovae that simply happen to be near a galaxy's center.
When the prior information of the location of the source is more informative than its sparse, noisy, irregularly sampled, or short light curve, it may present a challenge no classifier can overcome, a fundamental limit on available information about the object.

% Subsuming is expected when distinguishing between subtypes of some class, though this failure mode is not necessarily the mark of a poor classifier.
Distinguishing between subclasses of a single phenomenon is subject to limits not only on the light curves of the unknown targets but also by the availability of adequate training sets.
It is nonetheless essential to identify subclasses when they have wholly different science applications.
As an example, supernovae (SN) Ia and Ibc are notorious for being difficult to distinguish.
In fact, it is more common for SN Ibc to be misclassified as SN Ia than the other way around.
This asymmetry is due to systematic underrepresentation of SN Ibc in available training sets.
However, SN Ibc contaminants in the traditional cosmology analysis done with SN Ia can bias estimates of the cosmological parameters, so the distinction is critical.

Class imbalance is a ubiquitous problem in astronomy that can severely exacerbate this form of inaccuracy, as the relative rates of various astrophysical events and objects differ by orders of magnitude from one another.
For example, RRc and RRd Lyrae stars are challenging to separate despite having different pulsation modes, and RRd stars, due to their rarity, are typically subsumed by RRc labels.
% (RRc stars pulsate in the first overtone, while RRd stars pulsate simultaneously in the fundamental mode and first overtone, and are much more rare than RRc stars), and yet are challenging to separate observationally.

An extreme case of inaccurate classification is to classify all objects as the most common class (in the training or test set), which is of particular concern to \plasticc\ given non-representative class balance of the training set.
Such a \textit{cruise control} classifier (left-center bottom panel of Figure~\ref{fig:mock_cm}) counters \plasticc's goal of identifying objects belonging to extremely rare classes.
We would like the \plasticc\ metric to reward a classifier that successfully avoids this kind of error.

\subsection{Realistic classifications}
\label{sec:realdata}

\begin{figure*}
	\begin{center}
    \includegraphics[width=\textwidth]{./fig/all_snphotcc_cm.png}
		\caption{Conditional probability matrices \changes{(CPMs)} of the \citet{lochner_photometric_2016} methods applied to the second post-challenge release of the \snphotcc\ dataset.
		Columns: the five machine learning methods of Boosted Decision Tree (BDT), K-Nearest Neighbors (KNN), Naive Bayes (NB), Neural Network (NN), and Support Vector Machine (SVM).
    Top row: five machine learning methods applied to template decompositions as features.
    Bottom row: the same five machine learning methods applied to wavelet decompositions as features.
		These CPMs derived from the dataset of a precursor light curve classification challenge by modern methods exhibit some of the systematics identified in Section~\ref{sec:mockdata} and Figure~\ref{fig:mock_cm}, particularly cruise control (WKNN, WNB), noisy (class Ibc in all but TBDT and WKNN), and perfect (class II in all).
		It is worth noting that \citet{lochner_photometric_2016} applies their classification to a representative sub-sample of the \snphotcc\ data selected once the challenge was complete, circumventing some of the issues of non-representativity present in the original submissions to the \snphotcc.}
		\label{fig:snphotcc_cm}
	\end{center}
\end{figure*}

%\aim{This would be more meaningful if we were given the conditional probability matrices of actual submissions to \snphotcc\ and then checked whether the \plasticc\ metric would have designated a different winner.
%Also, it may be more interpretable if we instead use Ashish's conditional probability matrices that have more classes.}

In order to understand the performance of classifiers on simulated datasets approximating reality, we calculate the values of our metric candidates on representative classifiers of a precursor light curve classification challenge.
The Supernova Photometric Classification Challenge (\snphotcc) \citep{kessler_supernova_2010} focused on deterministically classifying a heterogenous population of supernovae into subclasses of SN Ia, SN II, and SN Ibc.

The \snphotcc\ attracted diverse classification approaches, encompassing $\chi^{2}$ fits of the supernova light curves to publicly available templates \citep{nugent_kcorrections_2002}, empirical models \citep{conley_sifto:_2008}, as well as alternatives to curve-fitting such as outlier identification on the training set Hubble diagram, dimensionality reduction, and clustering.
% \footnote{we give an introduction to both the \plasticc\ data and also the background introduction to the Hubble diagram of distance vs. redshift and its usefulness to cosmology in the \plasticc\ release note: \href{arXiv:XXXX.XXXXX}{arXiv:XXXX.XXXXX}}
% TODO cite InCA?
% A general light curve shape (rather than one motivated by the physical differences between SNeIa and core collapse SNe) was assumed by some competitors and then a kernel density estimation was performed over the fit parameters, with various approaches employed including boosting over the feature space.
Machine learning was also employed, using features such as the light-curve slopes to produce a predictive model for the training data.
% TODO need citations of ML submissions to SNPhotCC
%, which are prone to bias given non-representativity of the test data and agnostic
% \snphotcc\ attracted physically motivated template-based methods as well as those based on decomposition of the light curves into generic features at risk of neglecting available physical information.

Since the conclusion of the \snphotcc, the light curves became a testbed for a suite of machine learning classifiers.
We consider a collection of probabilistic classification methods, as presented in \citet{lochner_photometric_2016}, whose CPMs\footnote{The classifiers of \citet{lochner_photometric_2016} are indeed probabilistic but are reduced to confusion matrices via deterministic labels (by assigning a label of the class achieving the highest probability) for this visualization and the science-motivated metric of Section~\ref{sec:deterministic}.
In all other instances, the classification posteriors are used directly.} are shown in Figure~\ref{fig:snphotcc_cm}.

The set of classification algorithms includes template-based classification procedures, denoted as T, (\citet{sako_photometric_2011}, top row) and a wavelet decomposition, denoted as W, of the light curves to construct the features over which to classify (\citet{newling_statistical_2011}, bottom row), each paired with Boosted Decision Tree (BDT), K-Nearest Neighbors (KNN), Naive Bayes (NB), Neural Network (NN), and Support Vector Machine (SVM) machine learning algorithms (columns).
While the complexity of entries to the \snphotcc\ was greater than this subset, we use these examples to establish the behavior of our metrics on realistic classification submissions.

We draw attention to the marked presence of the systematics introduced in Section~\ref{sec:mockdata} in the CPMs of Figure~\ref{fig:snphotcc_cm}.
Note that the WNN and WNB methods both suffer from the cruise control systematic on SN II, which were the most prevalent in the \snphotcc\ dataset.
Nearly all the other CPMs exhibit classifications that are almost perfect for SN Ia, perfect for SN II, and noisy for SN Ibc.
A likely cause for this effect is that SN Ibc are poorly represented in training and template sets.
% \subsubsection{Unknown dataset}
% \label{sec:mystery}
%
% \begin{figure*}
% 	\begin{center}
% 		\includegraphics[width=0.3\textwidth]{./fig/Unknown_MLPNeuralNet_cm.png}
% 		\includegraphics[width=0.3\textwidth]{./fig/Unknown_KNeighbors_cm.png}
% 		\includegraphics[width=0.3\textwidth]{./fig/Unknown_RandomForest_cm.png}
% 		\caption{}
% 		\label{fig:unknown_cm}
% 	\end{center}
% \end{figure*}
