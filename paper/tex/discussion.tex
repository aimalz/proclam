\section{Discussion}
\label{sec:discussion}

The goal of this work is to identify the metric most suited to \plasticc, which seeks classification posteriors of complete light curves similar to those anticipated from \lsst, with an emphasis on classification over all types, rewarding a ``best in show'' classifier rather than focusing on any one class or scientific application.\footnote{At the conclusion of \plasticc, other metrics specific to scientific uses of one or more particular classes will be used to identify ``best in class'' classification procedures that will be useful for more targeted science cases.}
The weighted log-loss is thus the metric most suited to the current \plasticc\ release.

\sout{Future releases of \plasticc\ will focus on different challenges in transient and variable object classification, with metrics appropriate to identifying methodologies that best enable those goals.
We discuss approaches to identifying optimal metrics for these variations, which may be developed further in future work.}
\changes{Transient and variable object classification is crucial for a variety of scientific objectives.
The impact of a shared performance metric on this diversity of goals leads to complex and covariant trade-offs, which thus must be evaluated using multiple metrics.
While a detailed accounting of these possibilities for future releases of \plasticc\ and the selection of appropriate metrics for individual science cases are outside the scope of this first investigation, we discuss below some issues concerning the identification of metrics for a few example science cases.}

\subsection{\sout{Early classification} \changes{Ongoing transient follow-up}}
\label{sec:early}

Spectroscopic follow-up is only expected of a small fraction of \lsst's detected transients and variable objects due to limited resources for such observations.
In addition to optical spectroscopic follow-up, photometric observations in other wavelength bands (near infrared and x-ray from space; microwave and radio from the ground) \changes{or at different times} will be key to building a physical understanding of the object, particularly as we enter the era of multi-messenger astronomy with the added possibility of optical gravitational wave signatures.
Prompt follow-up observations are highly informative for fitting models to the light curves of familiar source classes and to characterizing anomalous light curves that could indicate never-before-seen classes that have eluded identification due to rarity or faintness.
As such, decisions about follow-up resource allocation must be made quickly and under the constraint that resources wasted on a misclassification consume the budget remaining for future follow-up attempts.
A future version of \plasticc\ focused on early light curve classification should have a metric that accounts for these limitations and rewards classifiers that perform better even when fewer observations of the lightcurve are available.

We consider the decision of whether to initiate follow-up observations to be binary and deterministic.
However, it is possible to conceive of non-binary decisions about follow-up resources; for example, one could choose between dedicating several hours on a spectroscopic instrument following up on one likely candidate or dedicating an hour each on several less likely candidates.
Here, we will discuss a metric for an early classification challenge to be focused on deterministic classification because the conversion between classification posteriors and decisions is uncharted territory that we do not explore at this time.

Even within the scope of spectroscopic follow-up as a primary motivation for early light curve classification, the goals of model-fitting to known classes and discovery of new classes would likely not share an optimal metric.
The critical question for choosing the most appropriate metric for any specific science goal motivating follow-up observations is to maximize information.
We provide two examples of the kind of information one must maximize via early light curve classification and the qualities of a deterministic metric that might enable it.

\changes{\subsection{Spectroscopic supernova cosmology}}
\label{sec:spec_sncosmo}

Supernova cosmology with spectroscopically confirmed light curves benefits from true positives, which contribute to the constraining power of the analysis by including one more data point;
when the class in which one is interested is as plentiful as SN Ia and our resources limited a priori, we may not be concerned by a high rate of false negatives.
% requires making a decision balancing the improved constraining power of including another SN Ia in the analysis, thereby constraining the cosmological parameters, so only true positives contribute information, and if we had a perfect classifier and standard follow-up spectroscopy resources, there would be a maximum amount of information about the cosmological parameters that could be gained in this way.
% Each false positive uses the same resources but adds no information about the cosmological parameters, and each false negative consumes no follow-up resources and deprives the Hubble diagram of one more data point.
False positives, on the other hand, may not enter the cosmology analysis, but they consume follow-up resources, thereby depriving the endeavor of the constraining power due to a single SN Ia.

A perfect classifier would lead to a maximum amount of information about the cosmological parameters conditioned on the follow-up resource budget.
\sout{For this scientific application, the metric must be chosen to balance the value of the information forgone by a false positive and the value of information forgone by a false negative, and the value placed on these is effectively weighted by the value we as researchers place on follow-up resources.}
\changes{Consider deterministic labels derived from cutoffs in probabilistic classifications for this scientific application; raising the probability cutoff reduces the number of false positives, boosting the cosmological constraining power, but at a cost of increasing the number of false negatives, which represent constraining power forgone.
As this tradeoff is asymmetric, it is insufficient to consider only the true and false positive and negative rates, as the \snphotcc\ FoM does, without propagating their impact on the information gained about the cosmological parameters.}
% \aim{Cite some deterministic metrics relating to TP/FP?}

\changes{\subsection{Anomalous transient and variable detection}}
\label{sec:anom}

\changes{A particularly exciting science case is anomaly detection, the discovery of entirely unknown classes of transient or variable astrophysical sources, or distinguishing some of the rarest types of sources from more abundant types.
Like the case of spectroscopic supernova cosmology discussed above,} anomaly detection also gains information only from true positives, but the cost function is different in that the potential information gain is unbounded when there is no prior information about undiscovered classes.
% \aim{COMMENT RB: not to stay in doc, but I don't understand the prev sentence. I would also object to the recent detection of kilonova as a good example of anomaly detection, I can buy it if I squint very hard
% COMMENT AIM: Agreed, but I couldn't think of a better one at the time of writing.}
\sout{An example would be the recent detection of a kilonova, flagged initially by the detection of gravitational waves from an object.}
\changes{The discovery of pulsars serves as an example of novelty detection enabled by a human classifier \citep{hewish_observation_1968, bell_burnell_measurement_1969}.}

Resource availability for identifying new classes is more flexible, increasing when new predictions or promising preliminary observations attract attention, and decreasing when a discovery is confirmed and the new class is established.
In this way, a false positive does not necessarily consume a resource that could otherwise be dedicated to a true positive, and the potential information gain is sufficiently great that additional resources would likely be allocated to observe the potential object.
% A false negative, on the other hand, represents forgoing an unbounded quantity of information, so minimizing the false negative rate is as important as maximizing the true positive rate.
% For a rare event like a kilonova, a false negative represents an unbounfalse positive does not appreciably reduce the amount of remaining information available to collect, but a false negative represents a large quantity of information forgone.
% Furthermore, r
% In this case, the information forgone by a false negative is significant compared to the information forgone by a false positive.
Thus, a metric \sout{tuned to} \changes{for evaluating} anomaly detection would aim to \changes{\emph{minimize the false negative rate and maximize the true positive rate.}}
% \aim{Cite some deterministic metrics relating to TP/FN?}

% \subsection{Hierarchical classes}
% \label{sec:hierarchical}
%
% \aim{TODO: We would like to at some point add some content on possible ideas for extending metrics to hierarchical classes, namely conditional extensions of log-loss and possible drawbacks of penalization that can be compensated for by weighting, as well as the challenge that could pose for interpretation.}

\subsection{Difficult light curve classification}
\label{sec:difficult}

Photometric light curve classification may be challenging for a number of reasons, including the sparsity and irregularity of observations, the possible classes and how often they occur, and the distances and brightnesses of the sources of the light curves.
These factors may represent limitations on the information content of the light curves, but appropriate classifiers may be able to overcome them to a certain degree.

Though quality cuts can eliminate the most difficult light curves from entering samples used for science applications, such a practice discards information that may be of value under an analysis methodology leveraging the larger number of light curves included in a sample without cuts.
Thus, classification methods that perform well on light curves characterized by lower signal-to-noise ratios are specially important for exploiting the full potential of upcoming surveys like \lsst.

This version of \plasticc\ implements quality cuts to homogenize difficulty to some degree, and notions of classification difficulty may depend on information that will not be available until after the challenge concludes.
While the groundwork for a metric incorporating data quality has been laid by \citet{wu_radio_2018}, we defer to future work an investigation of this possibility.
