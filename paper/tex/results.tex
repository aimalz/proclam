\section{Results}
\label{sec:results}

In the following sections, we explore the response of the log-loss and Brier score metrics to the classifiers of Section~\ref{sec:data} and as a function of the weights on affected classes.

\subsection{Mock classifier systematics}
\label{sec:mockresults}

\begin{figure*}
	\begin{center}
		\includegraphics[width=0.99\textwidth]{./fig/multipanel_res.png}
		\caption{Weighted log-loss and Brier scores for baseline classifiers with combinations of systematics.
		Each point represents a classifier with baseline behavior (classifiers with perfect behavior are shown as triangles, noisy classifiers are shown as squares and almost perfect classifiers as diamonds). The asterisk markers show the case where one class is given a systematic effect. From left to right, we zoom in on a particular range of Brier/log-loss scores, to highlight the effect of systematics on the scores for well-behaved methods with low Brier/log-loss scores. 
		The color of the marker for the systematic effect indicates the weight on the one class affected by that systematic, while the color of the baseline behavior marker indicates the integrated weight evenly distributed over other classes with baseline behavior. The `subsumed as' systematic on a perfect baseline (triangle with `y-shaped' marker) is the classification example that is most sensitive to changes in the class weight, as can be seen by the changes in log-loss score in the left hand panel.
%		Both metrics are shown in three regimes of sensitivity: to the subsumed systematic on a perfect baseline is highest, all other baselines with the subsumed systematic, and all other systematics.
		The ranges of Brier score and log-loss values between the panels are in ratios of approximately $10:7:3$ and $100:10:5$, respectively, indicating the log-loss's higher sensitivity to the presence of systematics.
		}
	\end{center}
	\label{fig:all_combined}
\end{figure*}

We demonstrate the behavior of the metrics with classification posteriors derived from conditional probability matrices composed of pairs of the characteristic classifiers of Section~\ref{sec:mockdata} is shown in Figure~\ref{fig:all_combined}.
The systematics introduced to each baselines are those that we intuitively expect to worsen performance of any particular classifier:
\begin{itemize}
\item the uncertain, \textit{almost perfect}, noisy, and subsuming classifiers are anticipated to worsen an otherwise \textit{perfect} classifier;
\item the uncertain, noisy, and subsuming classifiers are anticipated to worsen an otherwise \textit{almost perfect} classifier;
\item the uncertain and subsuming classifiers are anticipated to worsen an otherwise \textit{noisy} classifier.
\end{itemize}
In every case, we apply the systematic to one true class, i.e. transforming one row of the baseline conditional probability matrix.
The metrics are evaluated at a number of weights $w$ on the affected class, with the weights on the remaining baseline classes equal to one another at $(1 - w) / (M - 1)$.
The weight values range from $0$ (all weight on the baseline) to $1$ (all weight on the systematic) at intervals of $0.1$.
This variation in weights establishes linear relationships between the log-loss and Brier score metrics for each pair of baseline and systematic.

Figure~\ref{fig:all_combined} confirms that for all weight on the perfect classifier, the values of both metrics vanish to zero.
It is immediately evident that the log-loss has more dynamic range than the Brier score overall, and that the log-loss is acutely sensitive to the subsuming systematic on a baseline of a perfect classifier.
In fact, the log-loss value for a classifier that subsumes a class into one that is classified perfectly should actually be infinite if the classes unaffected by the systematic have no weight.
The finite maximum in our tests is set by the artificial zero point established for numerical stability.
Because the combination of a subsuming classifier with a perfect baseline is so different from all other combinations, we separate the subsuming systematic on all other baselines (lower left panel) from all other systematics (lower right panel) in Figure~\ref{fig:all_combined}.
The lower panels of Figure~\ref{fig:all_combined} show that the dynamic range of the log-loss remains higher even outside the regime where it tends toward infinity.
The extent of the values of both metrics is higher in the lower right panel with the subsuming systematic on various baselines than in the lower left panel with all other systematics.
This shows that both metrics are still more sensitive to the subsuming systematic than any other systematic, confirming that both can appropriately penalize the cruise control effect.

\begin{table}[]
\begin{tabular}{lll}
Classifier characteristic & Brier score & Log-loss\\
\hline
Perfect & 0.0 & 0.0\\
Almost perfect & 0.042 & 0.225\\
Noisy & 0.113 & 0.408\\
Uncertain & 0.253 & 0.699\\
Subsumed from Noisy & 0.447 & 1.109\\
Subsumed from Almost & 0.641 & 1.629\\
Subsumed from Perfect & 1.0 & 18.421\footnote{The entry for the log-loss of a classifier that subsumes a class into one that is otherwise perfectly classified should be infinite but is bounded by the numerical precision of our calculations.}
\end{tabular}
\caption{The value of each metric when the weight is entirely on the class(es) with the indicated characteristic.}
\label{tab:extents}
\end{table}

When all weight is on the class affected by a systematic, there is a characteristic limit for each metric's values, shown in Table~\ref{tab:extents}.
Because a subsumed class takes the conditional probability vector of the subsuming class, the metric values depend on what systematics may be affecting the subsuming class as well.
Based on Table~\ref{tab:extents}, both metrics would agree on the ranking of these classifiers, though this agreement is not in general guaranteed.
Furthermore, this invariant ranking is consistent with our intuition about the severity of anticipated faults of classifiers.

\begin{table}[]
\begin{tabular}{l|llll}
	& Systematics & & &\\
Baselines & Subsumed & Uncertain & Noisy & Almost\\
\hline
Perfect & 18.421 & 2.763 & 3.601 & 5.387\\
Almost perfect & 2.343 & 2.246 & 2.556 & \\
Noisy & 2.102 & 2.085 & &
\end{tabular}
\caption{The slopes for each baseline-plus-systematic pair in the space of log-loss  versus Brier score.
A higher slope corresponds to increased sensitivity of the log-loss.}
\label{tab:slopes}
\end{table}

The relative sensitivity ratios of the log-loss to the Brier score are the slopes in the trends of Figure~\ref{fig:all_combined} and are given in Table~\ref{tab:slopes}.
The log-loss always has higher sensitivity than the Brier score, particularly to the difference between the perfect classifier and any lesser classifier.
A possible implication of this behavior is that the log-loss may have an enhanced ability to distinguish between multiple high-performing classifiers that might not have meaningfully different metric values under the Brier score.
In a sense, it means that the log-loss is more susceptible to the tunnel vision classifier, with a significant response to any move toward perfection.
% Heavily unequal weighting can incentivize tunnel vision classification.
% However, the slope is higher for the almost perfect systematic than the noisy systematic, and higher for the noisy systematic than the uncertain systematic; this means that a tunnel vision classifier is deprived of the metric's favor more if it neglects other classes than if it already does fairly well for them, a case in which we would not call it much of a systematic.

% \aim{Still reworking past here.}
% Consider a weighting of $\sim0.8$ for a class affected by tunnel vision, leaving $\sim0.2$ to be shared evenly among all other classes uniformly affected by the other systematics.
% Qualitatively, we would say that a classifier that is almost perfect for other classes is superior to one that is noisy, and a classifier that is noisy for other classes is still superior to one that is uniform; furthermore, the subsuming classifier is even more harshly penalized in this situation than the uncertain classifier, meaning both metrics are  consistent with our basic tests of intuition in this case.
% However, this observation also indicates that the tunnel vision systematic is difficult to penalize, and that if the affected class is given a large weight, it can easily dominate the metric.
% If all classes are of scientific importance, heavily unequal weighting can incentivize tunnel vision classification.

% We introduced weighting of per-class metrics to discourage `tunnel vision' and `cruise control' classifiers that can ignore classes other than the most common and nonetheless perform well by a metric.
% Figure~\ref{fig:popweight} shows the impact of weighting the per-class metrics by the number of objects in the class as each is affected by one of the systematics and the other classes are held at the more realistic almost perfect performance.
% The points show different classification schemes, and all points are coloured by the change in the weighting, dependent on the size of the population class being classified.
% Conversely, the cruise control classifier and, to a lesser degree the noisy classifier, always has high log-loss and Brier score values regardless of the weight on the affected class.

% \begin{figure}
% 	\begin{center}
% 		\includegraphics[width=0.5\textwidth]{./fig/all_effects_isolated.png}
% 		\caption{Population-weighted log-loss and Brier scores for classifiers with one class affected by a systematic, as a function of the population of the affected class.
% 		Each point corresponds to an almost perfect classifier that with one class instead affected by a systematic (shape), with log-loss on the $y$ axis and Brier score on the $x$ axis.
% 		The metrics are calculated with a weighting (color and size) proportional to the log of its weight in a weighted average following Equation~\ref{eq:weightavg}.
% 		}
% 	\end{center}
% 	\label{fig:popweight}
% \end{figure}

% The tunnel vision classifier has a consistently low value under the Brier score and log-loss metric (bottom left corner of the plot), only increasing its Brier score once the weighting drops (less blue).
% In this view, the Brier score appears to be more susceptible to tunnel vision than the log-loss, demonstrating a more significant decrease as the size of the affected class increases, but both metrics have concerning behavior in this regard.
% This finding suggests that weighting alone may not be sufficient to counter the influence of this effect, and indicates a need for another balancing mechanism, such as requiring a threshold metric value on all classes.
% When considering a method of converting from one of these metrics to a finall `winner' of the classification challenge, care must be taken to ensure that all approaches do reasonably well at classifying more than one object.
% This thresholding procedure is discussed in the text

% \textbf{Ashish to reaplce/add more here?}
%\aim{Preliminary results indicate weighting will be very important for preventing the tunnel vision classifier from winning. It may be necessary to a priori anticipate which classes will have to be most strongly protected from this systematic via upweighting them.}

%\begin{figure}
%	\begin{center}
%		% \includegraphics[width=0.5\textwidth]{./fig/systematics_onlyperfect.png}
%		\caption{\aim{After much iteration on how best to present these tests, a figure similar to Figure~\ref{fig:cruise} but for the tunnel vision classifier (heading) on different baseline classifications (panels) as a function of weight on the affected class (rather than number of classes) is under construction.}}
%		\label{fig:tunnel}
%	\end{center}
%\end{figure}

\subsection{Representative classifications}
\label{sec:realresults}

We also present in Table~\ref{tab:snmachinevals} the values of all metrics we considered, under equal weight per object, for classification probabilities derived from running the algorithms of \citet{lochner_photometric_2016} on the \snphotcc\ data of Section~\ref{sec:realdata}.
Table~\ref{tab:snmachinerank} shows the ranking of classifier performance under each metric.

% \aim{TODO: combine these tables!}
\begin{table}[]
\begin{tabular}{llll}
%                            & AUC    & Brier   & FoM   & Logloss            \\
% TBRF  						 & 0.982 & 0.0486  & 0.635  & 0.0907  \\
% TKNN             & 0.942 & 0.0787  & 0.384 & 0.146  \\
% TNB              & 0.879 & 0.105  & 0.340  & 0.251 \\
% TNN							 & 0.954 & 0.0650  & 0.496   & 0.125 \\
% TSVM             & 0.969 & 0.0583 & 0.514  & 0.113 \\
% WBRF   						 & 0.978 & 0.0689  & 0.591  & 0.1321 \\
% WKNN              & 0.894 & 0.132  & 0.114 & 0.228 \\
% WNB               & 0.850 & 0.178  & 0.0365 & 0.443  \\
% WNN 							 & 0.946 & 0.0750  & 0.480  & 0.152 \\
% WSVM              & 0.968  & 0.0730  & 0.499  & 0.1316
& Brier   & FoM   & Logloss            \\
\hline
TBRF  		& 0.0486  & 0.635   & 0.0907  \\
TKNN      & 0.0787  & 0.384   & 0.146  \\
TNB       & 0.105   & 0.340   & 0.251 \\
TNN				& 0.0650  & 0.496   & 0.125 \\
TSVM      & 0.0583  & 0.514   & 0.113 \\
WBRF   		& 0.0689  & 0.591   & 0.1321 \\
WKNN      & 0.132   & 0.114   & 0.228 \\
WNB       & 0.178   & 0.0365  & 0.443  \\
WNN 			& 0.0750  & 0.480   & 0.152 \\
WSVM      & 0.0730  & 0.499   & 0.1316 \\
\end{tabular}
\caption{The values of three metrics for each of ten \snmachine\ classifiers with equal weight per object.}
	\label{tab:snmachinevals}
\end{table}

\begin{table}[]
\begin{tabular}{llll}
Rank & Brier           & FoM             & Logloss       \\
% would like FoM/AUC/LogLoss/Brier in order of introduction
\hline
1 & TBRF	& TBRF 	& TBRF \\
2 & TSVM 	& WBRF	& TSVM \\
3 & TNN 	& TSVM	& TNN \\
4 & WBRF 	& WSVM	& WSVM \\
5 & WSVM 	& TNN		& WBRF \\
6 & WNN 	& WNN		& TKNN \\
7 & TKNN 	& TKNN	& WNN \\
8 & TNB 	& TNB		& WKNN \\
9 & WKNN 	& WKNN	& TNB \\
10 & WNB 	& WNB		& WNB \\
%   & AUC             & Brier           & FoM             & Logloss       \\
% 1 & TBRF 	& TBRF	 	& TBRF 	& TBRF \\
% 2 & WBRF	 	& TSVM 	& WBRF		& TSVM \\
% 3 & TSVM 	& TNN 	& TSVM	& TNN \\
% 4 & WSVM 	& WBRF 		& WSVM		& WSVM \\
% 5 & TNN 	& WSVM 	& TNN		& WBRF \\
% 6 & WNN 		& WNN 		& WNN		& TKNN \\
% 7 & TKNN 	& TKNN 	& TKNN	& WNN \\
% 8 & WKNN 	& TNB 	& TNB		& WKNN \\
% 9 & TNB 	& WKNN 	& WKNN		& TNB \\
% 10 & WNB 	& WNB 		& WNB		& WNB \\
\end{tabular}
\caption{The ranking of the classifiers of Table~\ref{tab:snmachinevals} under each metric.}
\label{tab:snmachinerank}
\end{table}
% TODO tell Michelle there's definitely a bug in her logloss!

The FOM of Section~\ref{sec:science} differs more from the Brier and log-loss metrics than they do from one another, but all three metrics are in agreement over the winners and losers, indicating that both of the potential \plasticc\ metrics are roughly consistent with our intuition about what makes a good classifier.

%\textbf{Current plan is that we compute the SNPhotCC metric for Michelle's contributions and also turn the binary classifications into probabilities - Renee will flesh this out}
%\aim{We are still iterating on the most informative tests to conduct on the \snphotcc\ data.
%I would like to have the confusion matrices from a real classification challenge (\snphotcc\ or Ashish Mahabal's) and test different weightings of our metrics on mock classification probabilities derived from those confusion matrices to check whether we choose the same ``winner,'' but the arrangements have not yet been finalized.
%The ``pipeline,'' however, is complete and ready to run as soon as the test conditions are agreed upon.}
