{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing classification probabilities\n",
    "\n",
    "_Alex Malz (GCCL@RUB)_\n",
    "\n",
    "Note: A lot of the cells here are very slow to run but only need to be run once, ever.\n",
    "Follow the `##` commented out instructions to run the slow cells the first time, then comment it out again when fiddling with plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bisect\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "# from sklearn.neighbors.kde import KernelDensity\n",
    "import scipy.stats as sps\n",
    "# import seaborn as sns\n",
    "# import sys\n",
    "\n",
    "# epsilon = sys.float_info.epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "# print(mpl.rcParams.items)\n",
    "mpl.use('PS')\n",
    "mpl.rcParams['text.usetex'] = False\n",
    "mpl.rcParams['mathtext.rm'] = 'serif'\n",
    "mpl.rcParams['font.family'] = 'serif'\n",
    "mpl.rcParams[\"font.family\"] = \"serif\"\n",
    "mpl.rcParams[\"mathtext.fontset\"] = \"dejavuserif\"\n",
    "mpl.rcParams['font.serif'] = 'DejaVu Serif'\n",
    "# mpl.rcParams['text.usetex'] = False\n",
    "# mpl.rcParams['mathtext.rm'] = 'serif'\n",
    "# mpl.rcParams['font.weight'] = 'light'\n",
    "# mpl.rcParams['font.family'] = 'serif'\n",
    "# mpl.rcParams['font.serif'] = ['Times New Roman']\n",
    "# # mpl.rcParams['font.family'] = ['Times New Roman']\n",
    "mpl.rcParams['axes.titlesize'] = 20\n",
    "mpl.rcParams['axes.labelsize'] = 16\n",
    "mpl.rcParams['xtick.labelsize'] = 12\n",
    "mpl.rcParams['ytick.labelsize'] = 12\n",
    "mpl.rcParams['savefig.dpi'] = 250\n",
    "mpl.rcParams['figure.dpi'] = 250\n",
    "mpl.rcParams['savefig.format'] = 'svg'\n",
    "mpl.rcParams['savefig.bbox'] = 'tight'\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# print(mpl.rcParams.items)\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "import pylab\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "# def truncate_colormap(cmap, minval=0.0, maxval=1.0, n=100):\n",
    "#     new_cmap = mpl.colors.LinearSegmentedColormap.from_list(\n",
    "#         'trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval),\n",
    "#         cmap(np.linspace(minval, maxval, n)))\n",
    "#     return new_cmap\n",
    "\n",
    "# cmap = plt.get_cmap('hot_r')\n",
    "# fave_cmap = truncate_colormap(cmap, 0.35, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import proclam\n",
    "# from proclam import *\n",
    "\n",
    "# # epsilon = 1.e-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "First, establish informative names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {90: 'SNIa',\n",
    "              67: 'SNIa-91bg',\n",
    "              52: 'SNIax',\n",
    "              42: 'SNII',\n",
    "              62: 'SNIbc',\n",
    "              95: 'SLSN-I',\n",
    "              15: 'TDE',\n",
    "              64: 'KN',\n",
    "              88: 'AGN',\n",
    "              92: 'RRL',\n",
    "              65: 'M-dwarf',\n",
    "              16: 'EB',\n",
    "              53: 'Mira',\n",
    "              6: r'$\\mu$Lens-Single'}\n",
    "\n",
    "true_labels = label_dict.copy()\n",
    "true_labels[991] = r'$\\mu$Lens-Binary'\n",
    "true_labels[992] = 'ILOT'\n",
    "true_labels[993] = 'CaRT'\n",
    "true_labels[994] = 'PISN'\n",
    "true_labels[995] = r'$mu$Lens-String'\n",
    "\n",
    "sub_labels = label_dict.copy()\n",
    "sub_labels[99] = 'Other'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truth table\n",
    "\n",
    "The true labels subdivide class 99 but the predicted labels don't (which breaks some `proclam.metrcs.util` functions, among other things).\n",
    "Warning, this fix is slow, so only run the cell below once and then read it in from a file next time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use me if you've never run the notebook before.\n",
    "# truth = pd.read_csv('plasticc_test_metadata.csv')\n",
    "# truth['ideal_label'] = truth['true_target']\n",
    "# for i in [991, 992, 993, 994]:\n",
    "#     truth.loc[truth['true_target'] == i, 'ideal_label'] = 99\n",
    "# header = ['object_id', 'true_target', 'ideal_label']\n",
    "# truth.to_csv('truth.csv', columns=header, index=False)\n",
    "\n",
    "## Use me if you've run the notebook before.\n",
    "truth = pd.read_csv('truth.csv', index_col='object_id')\n",
    "\n",
    "true_class_ids, true_class_counts = np.unique(truth['true_target'], return_counts=True)\n",
    "ideal_class_ids, ideal_class_counts = np.unique(truth['ideal_label'], return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "same for the validation classifier, data found [here](https://www.dropbox.com/s/m4l79ire87behmm/validation_tables.tar.gz?dl=0), but first massage the data format (pretty slow!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Run me only if you've never run the notebook before.\n",
    "# val_label_dict = {'SNIa-Normal': 90,#'SNIa',\n",
    "#               'Ia-91bg': 67,#'SNIa-91bg',\n",
    "#               'SNIa-x': 52,#'SNIax',\n",
    "#               'SNCC-II': 42,#'SNII',\n",
    "#               'SNCC-Ibc': 62,#'SNIbc',\n",
    "#               'SLSN-I': 95,#'SLSN-I',\n",
    "#               'TDE': 15,#'TDE',\n",
    "#               'Kilonova': 64,#'KN',\n",
    "#               'AGN': 88,#'AGN',\n",
    "#               'RRLyrae': 92,#'RRL',\n",
    "#               'Mdwarf': 65,#'M-dwarf',\n",
    "#               'EBE': 16,#'EB',\n",
    "#               'Mira': 53,#'Mira',\n",
    "#               'uLens-point': 6}#r'$\\mu$Lens-Single'}\n",
    "\n",
    "# comparisons = []\n",
    "# for field in ['DDF', 'WFD']:\n",
    "#     contestant = 'validation_'+field\n",
    "#     comparisons.append(contestant)\n",
    "#     if not os.path.exists('submissions'):\n",
    "#         os.makedirs('submissions')\n",
    "#     if not os.path.exists(os.path.join('submissions', contestant)):\n",
    "#         os.makedirs(os.path.join('submissions', contestant))\n",
    "#     for status in ['truth_table', 'predicted_prob']:\n",
    "#         fn = status+'_'+field+'.csv'\n",
    "#         validation = pd.read_csv(fn)#, index_col='objids')\n",
    "#         validation.rename(columns=val_label_dict, inplace=True)\n",
    "#         validation['objids'] = validation['objids'].apply(lambda x: x.rpartition('_')[-1])\n",
    "#         validation.rename(columns={'objids': 'object_id'}, inplace=True)\n",
    "#         validation.set_index('object_id', inplace=True)\n",
    "#         if status == 'truth_table':\n",
    "#             truecat = validation.copy()\n",
    "#             truecat['true_target'] = truecat.idxmax(axis=1)\n",
    "#             truecat['true_target'].to_csv(field+'truth.csv', header=True)\n",
    "#         elif status == 'predicted_prob':\n",
    "#             probcat = validation.copy()\n",
    "#             probcat.to_csv('submissions/validation/'+field+'probs.csv', header=True)\n",
    "# #     print('saved per-true-class data for '+contestant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # wfd = pd.read_csv('WFDtruth.csv')\n",
    "# # wfd.set_index('object_id', inplace=True)\n",
    "# # ddf = pd.read_csv('DDFtruth.csv')\n",
    "# # ddf.set_index('object_id', inplace=True)\n",
    "# # in_val = wfd.append(ddf)\n",
    "# # in_val.to_csv('submissions/validation/truth.csv', header=True)\n",
    "\n",
    "# # wfd_probs = pd.read_csv('submissions/validation/WFDprobs.csv').set_index('object_id')\n",
    "# # ddf_probs = pd.read_csv('submissions/validation/DDFprobs.csv').set_index('object_id')\n",
    "# # val_probs = wfd_probs.append(ddf_probs)\n",
    "# val_probs = pd.read_csv('0_validation.csv').set_index('object_id')\n",
    "# # print(val_probs.isnull().values.sum())\n",
    "# # print(np.any(np.isnan(val_probs)))\n",
    "# val_probs.to_csv('submissions/validation/validation_probs.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Run me only if you've never run the notebook before. (slow)\n",
    "# for i in label_dict.keys():\n",
    "#     one_class = val_probs.loc[in_val[in_val['true_target'] == i].index, :]\n",
    "#     if ((np.all(one_class == epsilon) or np.any(np.isnan(one_class)))):\n",
    "#         print('arrr, thar be NaNs or all 0s!')\n",
    "#         one_class.fillna(epsilon)\n",
    "#         one_class.replace(to_replace=0, value=epsilon)\n",
    "#         one_class.replace(to_replace=0., value=epsilon)\n",
    "# #     assert ~((np.all(one_class == epsilon) or np.any(np.isnan(one_class))))\n",
    "#     if ~((np.all(one_class == epsilon) or np.any(np.isnan(one_class)))):\n",
    "#         normalized = one_class.sum(axis=1)\n",
    "#     else:\n",
    "#         print('normalization failed for '+str(i)+' with all '+str(len(one_class))+' zeros='+str(np.all(one_class == epsilon))+' and NaNs='+str(np.any(np.isnan(one_class))))\n",
    "#         normalized = 1.\n",
    "#     to_save = one_class.divide(normalized, axis=0)\n",
    "#     pred_class_inds = {i: int(i[6:]) for i in to_save.columns.values}\n",
    "#     to_save.rename(columns=pred_class_inds, inplace=True)\n",
    "#     to_save.to_csv(os.path.join('submissions/validation', 'probvecs'+str(i)+'true.csv'), header=True)\n",
    "# #     print('saved validation class '+str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification probabilities\n",
    "\n",
    "This is just for the winning submission for now.\n",
    "You have to get the files with the submission results first and put them in the same directory as this notebook.\n",
    "_(I don't know if it's okay to post the link to where those live or if it's private and will be published with the paper through Zenodo or something.)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contestants = ['1_Kyle', '2_MikeSilogram', '3_MajorTom', \n",
    "               '4_AhmetErdem', '5_SKZLostInTranslation', '6_StefanStefanov', \n",
    "               '7_hklee', \n",
    "               '8_rapidsai', '9_ThreeMusketeers',\n",
    "               '10_JJ', \n",
    "               '11_SimonChen', '12_Go_Spartans'\n",
    "                ]\n",
    "all_contestants = ['validation'] + contestants\n",
    "print([x for x in enumerate(contestants)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell chops up the data into bite-sized pieces.\n",
    "It's slow but only needs to be done once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run me only if you've never run the notebook before. (SLOW!)\n",
    "# for contestant in contestants:\n",
    "#     submission = pd.read_csv(contestant+'.csv', index_col='object_id')\n",
    "#     pred_class_inds = {i: int(i[6:]) for i in submission.columns.values}\n",
    "#     submission.rename(columns=pred_class_inds, inplace=True)\n",
    "#     if not os.path.exists('submissions'):\n",
    "#         os.makedirs('submissions')\n",
    "#     if not os.path.exists(os.path.join('submissions', contestant)):\n",
    "#         os.makedirs(os.path.join('submissions', contestant))\n",
    "#     for i in truth.true_target.unique():\n",
    "#         to_save = submission[truth['true_target'] == i]\n",
    "#         to_save.to_csv(os.path.join('submissions/'+contestant, 'probvecs'+str(i)+'true.csv'))\n",
    "#     print('saved per-true-class data for '+contestant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def break_down_submission(ind):\n",
    "    contestant = contestants[ind]\n",
    "    submission = pd.read_csv(contestant+'.csv', index_col='object_id')\n",
    "    pred_class_inds = {i: int(i[6:]) for i in submission.columns.values}\n",
    "    submission.rename(columns=pred_class_inds, inplace=True)\n",
    "    if not os.path.exists('submissions'):\n",
    "        os.makedirs('submissions')\n",
    "    if not os.path.exists(os.path.join('submissions', contestant)):\n",
    "        os.makedirs(os.path.join('submissions', contestant))\n",
    "    for i in truth.true_target.unique():\n",
    "        to_save = submission[truth['true_target'] == i]\n",
    "        to_save.to_csv(os.path.join('submissions/'+contestant, 'probvecs'+str(i)+'true.csv'))\n",
    "    print('saved per-true-class data for '+contestant)\n",
    "    return contestant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nps = mp.cpu_count() - 4\n",
    "# pool = mp.Pool(nps)\n",
    "# printout = pool.map(break_down_submission, range(len(contestants)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "need to account for mismatch in true vs. predicted labels for class 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run me only if you've never run the notebook before.\n",
    "# for contestant in contestants:\n",
    "#     umbrella = []\n",
    "#     for i in truth.true_target.unique():\n",
    "#         if str(i)[:2] == '99':\n",
    "#             umbrella.append(pd.read_csv(os.path.join('submissions/'+contestant, 'probvecs'+str(i)+'true.csv'), index_col='object_id'))\n",
    "#     submission = pd.concat(umbrella)\n",
    "#     submission.to_csv(os.path.join('submissions/'+contestant, 'probvecs99true.csv'))\n",
    "#     print('saved 99 aggregate data for '+contestant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_99(ind):\n",
    "    contestant = contestants[ind]\n",
    "    umbrella = []\n",
    "    for i in truth.true_target.unique():\n",
    "        if str(i)[:2] == '99':\n",
    "            umbrella.append(pd.read_csv(os.path.join('submissions/'+contestant, 'probvecs'+str(i)+'true.csv'), index_col='object_id'))\n",
    "    submission = pd.concat(umbrella)\n",
    "    submission.to_csv(os.path.join('submissions/'+contestant, 'probvecs99true.csv'))\n",
    "    print('saved 99 aggregate data for '+contestant)\n",
    "    return contestant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nps = mp.cpu_count() - 4\n",
    "# pool = mp.Pool(nps)\n",
    "# printout = pool.map(aggregate_99, range(len(contestants)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to further split up the data into small pieces and calculate the KDE of each, which is also slow, though only has to be done once.\n",
    "UPDATE: This isn't actually that slow anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run me only if you've never run the notebook before.\n",
    "# for contestant in contestants:\n",
    "#     for one_target in truth.true_target.unique():\n",
    "#         all_prob = pd.read_csv(os.path.join('submissions/'+contestant, 'probvecs'+str(one_target)+'true.csv'), index_col='object_id')\n",
    "#         n_true = len(all_prob)\n",
    "#         print('calculating '+contestant+'\\'s KDE for true class '+str(one_target)+' with '+str(n_true)+' objects')\n",
    "# #         pred_class_inds = {i: int(i[6:]) for i in all_prob.columns.values}\n",
    "# #         all_prob.rename(columns=pred_class_inds, inplace=True)\n",
    "#         for cl_est in all_prob.columns:\n",
    "#             to_plot = all_prob[cl_est].values\n",
    "#             if not np.all(to_plot == 0):\n",
    "#                 kernel = sps.gaussian_kde(to_plot)\n",
    "#             else:\n",
    "#                 kernel = None\n",
    "#             with open(os.path.join('submissions/'+contestant, 'kernel'+str(one_target)+'true'+str(cl_est)+'pred.pkl'), 'wb') as fn:\n",
    "#                 pickle.dump(kernel, fn)\n",
    "#             print('completed '+contestant+'\\'s KDE for predicted class '+str(cl_est))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_kde(ind):\n",
    "    contestant = contestants[ind]\n",
    "    for one_target in truth.true_target.unique():\n",
    "        all_prob = pd.read_csv(os.path.join('submissions/'+contestant, 'probvecs'+str(one_target)+'true.csv'), index_col='object_id')\n",
    "        n_true = len(all_prob)\n",
    "        print('calculating '+contestant+'\\'s KDE for true class '+str(one_target)+' with '+str(n_true)+' objects')\n",
    "#         pred_class_inds = {i: int(i[6:]) for i in all_prob.columns.values}\n",
    "#         all_prob.rename(columns=pred_class_inds, inplace=True)\n",
    "        for cl_est in all_prob.columns:\n",
    "            to_plot = all_prob[cl_est].values\n",
    "            if not np.all(np.isclose(to_plot, 0.)):\n",
    "                kernel = sps.gaussian_kde(to_plot)\n",
    "            else:\n",
    "                kernel = None\n",
    "            with open(os.path.join('submissions/'+contestant, 'kernel'+str(one_target)+'true'+str(cl_est)+'pred.pkl'), 'wb') as fn:\n",
    "                pickle.dump(kernel, fn)\n",
    "            print('completed '+contestant+'\\'s KDE for predicted class '+str(cl_est))\n",
    "    return contestant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nps = mp.cpu_count() - 4\n",
    "# pool = mp.Pool(nps)\n",
    "# printout = pool.map(split_and_kde, range(len(contestants)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now remove all 99 and only run on objects in the validation set, for comparison.\n",
    "THIS IS WHAT WAS BROKEN -- THE OBJECT IDS DO NOT CORRESPOND BETWEEN VALIDATION AND PLASTICC!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run me only if you've never run the notebook before.\n",
    "# for contestant in contestants:\n",
    "#     if not os.path.exists('submissions/validation/'+contestant):\n",
    "#         os.makedirs('submissions/validation/'+contestant)\n",
    "# #     'validation_truth.csv'\n",
    "#     for one_target in label_dict.keys():\n",
    "#         all_prob = pd.read_csv(os.path.join('submissions/'+contestant, 'probvecs'+str(one_target)+'true.csv'), index_col='object_id')\n",
    "#         val_prob = pd.read_csv(os.path.join('submissions/validation', 'probvecs'+str(one_target)+'true.csv'), index_col='object_id')\n",
    "#         in_val = all_prob.loc[val_prob.index, :]\n",
    "#         in_val.fillna(epsilon)\n",
    "#         in_val.replace(to_replace=0, value=epsilon)\n",
    "#         in_val.replace(to_replace=0., value=epsilon)\n",
    "#         normalize = in_val.sum(axis=1) - in_val['99']\n",
    "#         in_val = in_val.divide(normalize, axis=0)\n",
    "#         in_val.to_csv(os.path.join('submissions/validation/'+contestant, 'probvecs'+str(one_target)+'true.csv'), header=True)\n",
    "#         n_true = len(in_val)\n",
    "#         print('calculating '+contestant+'\\'s KDE for true class '+str(one_target)+' with '+str(n_true)+' objects')\n",
    "#         for cl_est in label_dict.keys():\n",
    "#             to_plot = in_val[str(cl_est)].values\n",
    "#             if not np.any(np.isnan(to_plot)):\n",
    "#                 kernel = sps.gaussian_kde(to_plot)\n",
    "#             else:\n",
    "#                 print(str(sum(np.isnan(to_plot)))+' NaNs preventing KDE for '+cl_est)\n",
    "#                 kernel = None\n",
    "#             with open(os.path.join('submissions/validation/'+contestant, 'kernel'+str(one_target)+'true'+str(cl_est)+'pred.pkl'), 'wb') as fn:\n",
    "#                 pickle.dump(kernel, fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "same for class 99 to account for mismatch in true vs. predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run me only if you've never run the notebook before, or if you changed the probabilities at which to evaluate the KDEs.\n",
    "# for contestant in contestants:\n",
    "#     for one_target in [99]:\n",
    "#         all_prob = pd.read_csv(os.path.join('submissions/'+contestant, 'probvecs'+str(one_target)+'true.csv'), index_col='object_id')\n",
    "#         n_true = len(all_prob)\n",
    "# #         print('calculating '+contestant+'\\'s KDE for true class '+str(one_target)+' with '+str(n_true)+' objects')\n",
    "# #         pred_class_inds = {i: int(i[6:]) for i in all_prob.columns.values}\n",
    "# #         all_prob.rename(columns=pred_class_inds, inplace=True)\n",
    "#         for cl_est in all_prob.columns:\n",
    "#             to_plot = all_prob[cl_est].values\n",
    "#             if not np.all(to_plot == 0):\n",
    "#                 kernel = sps.gaussian_kde(to_plot)\n",
    "#             else:\n",
    "#                 kernel = None\n",
    "#             with open(os.path.join('submissions/'+contestant, 'kernel'+str(one_target)+'true'+str(cl_est)+'pred.pkl'), 'wb') as fn:\n",
    "#                 pickle.dump(kernel, fn)\n",
    "# #             print('completed '+contestant+'\\'s KDE for predicted class '+str(cl_est))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_kde_99(ind):\n",
    "    contestant = contestants[ind]\n",
    "    for one_target in [99]:\n",
    "        all_prob = pd.read_csv(os.path.join('submissions/'+contestant, 'probvecs'+str(one_target)+'true.csv'), index_col='object_id')\n",
    "        n_true = len(all_prob)\n",
    "#         print('calculating '+contestant+'\\'s KDE for true class '+str(one_target)+' with '+str(n_true)+' objects')\n",
    "#         pred_class_inds = {i: int(i[6:]) for i in all_prob.columns.values}\n",
    "#         all_prob.rename(columns=pred_class_inds, inplace=True)\n",
    "        for cl_est in all_prob.columns:\n",
    "            to_plot = all_prob[cl_est].values\n",
    "            if not np.all(to_plot == 0):\n",
    "                kernel = sps.gaussian_kde(to_plot)\n",
    "            else:\n",
    "                kernel = None\n",
    "            with open(os.path.join('submissions/'+contestant, 'kernel'+str(one_target)+'true'+str(cl_est)+'pred.pkl'), 'wb') as fn:\n",
    "                pickle.dump(kernel, fn)\n",
    "#             print('completed '+contestant+'\\'s KDE for predicted class '+str(cl_est))\n",
    "    return contestant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nps = mp.cpu_count() - 4\n",
    "# pool = mp.Pool(nps)\n",
    "# printout = pool.map(fit_kde_99, range(len(contestants)))\n",
    "# printout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the KDEs at grid points, unfortunately also slow (though I'm not sure why).\n",
    "It has to be re-reun for each updated set of grid points but saves the output so the plot can be tweaked without running it again.\n",
    "UPDATE: This is now only kinda slow, major improvement!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbins = 100\n",
    "positions = np.linspace(0., 1., nbins+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run me only if you've never run the notebook before, or if you changed the probabilities at which to evaluate the KDEs.\n",
    "# for contestant in contestants:\n",
    "#     for one_target in truth.true_target.unique():\n",
    "#         print('evaluating '+contestant+'\\'s KDEs for true class '+str(one_target)+' with '+str(n_true)+' objects')\n",
    "#         for cl_est in truth.ideal_label.unique():\n",
    "#             with open(os.path.join('submissions/'+contestant, 'kernel'+str(one_target)+'true'+str(cl_est)+'pred.pkl'), 'rb') as fn:\n",
    "#                 kernel = pickle.load(fn)\n",
    "#             if kernel is not None:\n",
    "#                 data = kernel(positions)#[:, np.newaxis])\n",
    "#             else:\n",
    "#                 data = np.zeros_like(positions)\n",
    "#             print('evaluated '+contestant+'\\'s KDE for predicted class '+str(cl_est))\n",
    "#             np.savetxt(os.path.join('submissions/'+contestant, 'violin'+str(one_target)+'true'+str(cl_est)+'pred.txt'), data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_kde(ind):\n",
    "    contestant = contestants[ind]\n",
    "    for one_target in truth.true_target.unique():\n",
    "        print('evaluating '+contestant+'\\'s KDEs for true class '+str(one_target))\n",
    "        for cl_est in truth.ideal_label.unique():\n",
    "            with open(os.path.join('submissions/'+contestant, 'kernel'+str(one_target)+'true'+str(cl_est)+'pred.pkl'), 'rb') as fn:\n",
    "                kernel = pickle.load(fn)\n",
    "            if kernel is not None:\n",
    "                data = kernel(positions)#[:, np.newaxis])\n",
    "            else:\n",
    "                data = np.zeros_like(positions)\n",
    "            assert(~np.any(data[data < 0.]))\n",
    "            print('evaluated '+contestant+'\\'s KDE for predicted class '+str(cl_est))\n",
    "            np.savetxt(os.path.join('submissions/'+contestant, 'violin'+str(one_target)+'true'+str(cl_est)+'pred.txt'), data)\n",
    "    return contestant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# nps = mp.cpu_count() - 4\n",
    "# pool = mp.Pool(nps)\n",
    "# printout = pool.map(evaluate_kde, range(len(contestants)))\n",
    "# printout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare for violin plots specific to class 99 true vs. predicted asymmetry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run me only if you've never run the notebook before, or if you changed the probabilities at which to evaluate the KDEs.\n",
    "# for contestant in contestants:\n",
    "#     for one_target in [99]:\n",
    "# #         print('evaluating '+contestant+'\\'s KDEs for true class '+str(one_target)+' with '+str(n_true)+' objects')\n",
    "#         for cl_est in truth.ideal_label.unique():\n",
    "#             with open(os.path.join('submissions/'+contestant, 'kernel'+str(one_target)+'true'+str(cl_est)+'pred.pkl'), 'rb') as fn:\n",
    "#                 kernel = pickle.load(fn)\n",
    "#             if kernel is not None:\n",
    "#                 data = kernel(positions)#[:, np.newaxis])\n",
    "#             else:\n",
    "#                 data = np.zeros_like(positions)\n",
    "# #             print('evaluated '+contestant+'\\'s KDE for predicted class '+str(cl_est))\n",
    "#             np.savetxt(os.path.join('submissions/'+contestant, 'violin'+str(one_target)+'true'+str(cl_est)+'pred.txt'), data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_kde_99(ind):\n",
    "    contestant = contestants[ind]\n",
    "    for one_target in [99]:\n",
    "#         print('evaluating '+contestant+'\\'s KDEs for true class '+str(one_target)+' with '+str(n_true)+' objects')\n",
    "        for cl_est in truth.ideal_label.unique():\n",
    "            with open(os.path.join('submissions/'+contestant, 'kernel'+str(one_target)+'true'+str(cl_est)+'pred.pkl'), 'rb') as fn:\n",
    "                kernel = pickle.load(fn)\n",
    "            if kernel is not None:\n",
    "                data = kernel(positions)#[:, np.newaxis])\n",
    "            else:\n",
    "                data = np.zeros_like(positions)\n",
    "#             print('evaluated '+contestant+'\\'s KDE for predicted class '+str(cl_est))\n",
    "            np.savetxt(os.path.join('submissions/'+contestant, 'violin'+str(one_target)+'true'+str(cl_est)+'pred.txt'), data)\n",
    "    return contestant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nps = mp.cpu_count() - 4\n",
    "# pool = mp.Pool(nps)\n",
    "# printout = pool.map(evaluate_kde_99, range(len(contestants)))\n",
    "# printout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now do the same for the validation classifier, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run me only if you've never run the notebook before.\n",
    "# for contestant in ['validation']:\n",
    "#     for one_target in label_dict.keys():\n",
    "#         all_prob = pd.read_csv(os.path.join('submissions/'+contestant, 'probvecs'+str(one_target)+'true.csv'), index_col='object_id')\n",
    "#         n_true = len(all_prob)\n",
    "#         print('calculating '+contestant+'\\'s KDE for true class '+str(one_target)+' with '+str(n_true)+' objects')\n",
    "# #         pred_class_inds = {i: int(i[6:]) for i in all_prob.columns.values}\n",
    "# #         all_prob.rename(columns=pred_class_inds, inplace=True)\n",
    "#         for cl_col in all_prob.columns:\n",
    "#             to_plot = all_prob[cl_col].values\n",
    "#             if not (np.all(to_plot == epsilon) or np.any(np.isnan(to_plot))):\n",
    "#                 try:\n",
    "#                     kernel = sps.gaussian_kde(to_plot)\n",
    "#                     print('completed '+contestant+'\\'s KDE for predicted class '+str(cl_col))\n",
    "#                 except:\n",
    "#                     print('KDE failed with '+str(to_plot))\n",
    "#                     kernel = None\n",
    "#             else:\n",
    "#                 print('KDE failed with all_zeros='+str(np.all(to_plot == epsilon))+' and NaNs='+str(np.any(np.isnan(to_plot))))\n",
    "#                 kernel = None\n",
    "#             with open(os.path.join('submissions/'+contestant, 'kernel'+str(one_target)+'true'+str(cl_col)+'pred.pkl'), 'wb') as fn:\n",
    "#                 pickle.dump(kernel, fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run me only if you've never run the notebook before, or if you changed the probabilities at which to evaluate the KDEs.\n",
    "# for contestant in ['validation']:\n",
    "#     for one_target in label_dict.keys():\n",
    "#         print('evaluating '+contestant+'\\'s KDEs for true class '+str(one_target))\n",
    "#         for cl_est in label_dict:\n",
    "#             with open(os.path.join('submissions/'+contestant, 'kernel'+str(one_target)+'true'+str(cl_est)+'pred.pkl'), 'rb') as fn:\n",
    "#                 kernel = pickle.load(fn)\n",
    "#             if kernel is not None:\n",
    "#                 data = kernel(positions)#[:, np.newaxis])\n",
    "#             else:\n",
    "#                 print('no kernel for true '+str(one_target)+' pred '+str(cl_est))\n",
    "#                 data = np.zeros_like(positions)\n",
    "#             print('evaluated '+contestant+'\\'s KDE for predicted class '+str(cl_est))\n",
    "#             np.savetxt(os.path.join('submissions/'+contestant, 'violin'+str(one_target)+'true'+str(cl_est)+'pred.txt'), data)\n",
    "            \n",
    "# # for contestant in contestants:\n",
    "# #     for one_target in label_dict:\n",
    "# #         print('evaluating '+contestant+'\\'s KDEs for true class '+str(one_target)+' with '+str(n_true)+' objects')\n",
    "# #         for cl_est in label_dict:\n",
    "# #             with open(os.path.join('submissions/validation/'+contestant, 'kernel'+str(one_target)+'true'+str(cl_est)+'pred.pkl'), 'rb') as fn:\n",
    "# #                 kernel = pickle.load(fn)\n",
    "# #             if kernel is not None:\n",
    "# #                 data = kernel(positions)#[:, np.newaxis])\n",
    "# #             else:\n",
    "# #                 data = np.zeros_like(positions)\n",
    "# #             print('evaluated '+contestant+'\\'s KDE for predicted class '+str(cl_est))\n",
    "# #             np.savetxt(os.path.join('submissions/validation/'+contestant, 'violin'+str(one_target)+'true'+str(cl_est)+'pred.txt'), data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the probabilities\n",
    "\n",
    "Voila, snazzy violin plots!\n",
    "\n",
    "TODO: rewrite these plotting functions with helper functions to avoid repeating code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# color_dict = {'1_Kyle': '#6344EE', '2_MikeSilogram': '#E873AB', '3_MajorTom': '#FFB81F', \n",
    "#               '4_AhmetErdem': '#30362F', '5_SKZLostInTranslation': '#30362F', '6_StefanStefanov': '#30362F', \n",
    "#               '7_hklee': '#30362F', '8_rapidsai': '#30362F', '9_ThreeMusketeers': '#30362F',\n",
    "#               '10_JJ': '#30362F', '11_SimonChen': '#30362F', '12_Go_Spartans': '#30362F',\n",
    "#               'validation': '#30362F'}\n",
    "\n",
    "color_dict = {}\n",
    "j = 0\n",
    "for i, contestant in enumerate(contestants):\n",
    "    if i == 6 or i == 9:\n",
    "        color_dict[contestant] = 'k'\n",
    "    else:\n",
    "        color_dict[contestant] = mpl.colors.to_hex(plt.get_cmap('tab10')(j))\n",
    "        j += 1\n",
    "color_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class 99 performance across classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def violins99(contestants, colors):\n",
    "#     highlights = {contestants[i]: colors[i] for i in range(len(contestants))}\n",
    "    one_target = 99\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "    for j, contestant in enumerate(contestants):\n",
    "        all_prob = pd.read_csv(os.path.join('submissions/'+contestant, 'probvecs99true.csv'), index_col='object_id')\n",
    "        n_true = len(all_prob)\n",
    "        stretchfact = 2.5\n",
    "        loc = 0\n",
    "        ax.set_title(str(n_true)+' true '+str(sub_labels[one_target]))\n",
    "        ticklabels = []\n",
    "        maps = all_prob.idxmax(axis=1)\n",
    "        counts = maps.value_counts()\n",
    "        for cl_est in sub_labels.keys():\n",
    "            if str(cl_est) in counts.keys():\n",
    "#                 nmax = str(counts[str(cl_est)])#str(np.around(counts[str(cl_est)]/float(n_true), 2))\n",
    "                nmax = format(float(counts[str(cl_est)]) / float(n_true) * 100., '.0f')+'%'\n",
    "            else:\n",
    "                nmax = '0%'\n",
    "            highlight = colors[contestant]\n",
    "            data = np.exp(np.genfromtxt(os.path.join('submissions/'+contestant, 'violin99true'+str(cl_est)+'pred.txt')))\n",
    "            if not np.all(data == 1.):\n",
    "                data = data / np.max(data)\n",
    "            else:\n",
    "                data = np.zeros_like(data)\n",
    "            wheremax = np.argmax(data)\n",
    "            if cl_est == one_target or (len(str(one_target)) == 2 and str(cl_est)[:2] == '99'):\n",
    "                ax.fill_betweenx(positions, stretchfact*loc - data, stretchfact*loc + data, alpha=0.5, color=highlight, linewidth=0.1, label=contestant)#fontweight='bold', \n",
    "            else:\n",
    "                ax.fill_betweenx(positions, stretchfact*loc - data, stretchfact*loc + data, alpha=0.5, color=highlight, linewidth=0.1)\n",
    "            if wheremax < len(positions)/4:\n",
    "                whereprint = 'bottom'\n",
    "            elif wheremax > 3 * len(positions) / 4:\n",
    "                whereprint = 'top'\n",
    "            else:\n",
    "                whereprint = 'center'\n",
    "            yval = j/3.+0.05 #positions[wheremax] + 0.2*j\n",
    "            ax.text(stretchfact*(loc+0.4), yval, nmax, fontsize=12, color=highlight, ha='center', rotation=0, va='bottom')#whereprint)#fontweight='bold', \n",
    "            loc += 1\n",
    "#             ax.text(stretchfact*(loc+0.25), 0.05+positions[wheremax]+0.2*j, nmax, fontsize=12, color=highlight, ha='center', va=whereprint)#, rotation=90)#fontweight='bold', \n",
    "#             loc += 1\n",
    "            ticklabels.append(sub_labels[cl_est])\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.set_xticks(stretchfact * np.arange(loc))\n",
    "    ax.set_xticklabels(ticklabels, rotation=45, ha=\"right\")\n",
    "    ax.set_xlabel('predicted class')\n",
    "    ax.set_ylabel('probability')\n",
    "    fig.savefig(os.path.join('submissions', 'violin_99.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "violins99(contestants[:3], color_dict)#['#00A878', '#D8F1A0', '#B4D6D3'])\n",
    "# #00A878, D8F1A0, B4D6D3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### performance without true class 99 objects\n",
    "\n",
    "just between real submissions and validation classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_violins(contestants, colors):\n",
    "#     highlights = {contestants[k]: colors[k] for k in range(len(contestants))}\n",
    "    for i in label_dict.keys():\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "        legloc, ticklabels = [], []\n",
    "        for j, contestant in enumerate(contestants):\n",
    "            all_prob = pd.read_csv(os.path.join('submissions/'+contestant, 'probvecs'+str(i)+'true.csv'), index_col='object_id')\n",
    "            n_true = len(all_prob)\n",
    "            stretchfact = 2.5\n",
    "            loc = 0\n",
    "            ax.set_title(str(n_true)+' true '+str(label_dict[i]))\n",
    "            maps = all_prob.idxmax(axis=1)\n",
    "            counts = maps.value_counts()\n",
    "            for cl_est in label_dict.keys():\n",
    "                if str(cl_est) in counts.keys():\n",
    "#                     nmax = str(counts[str(cl_est)])#str(np.around(counts[str(cl_est)]/float(n_true), 2))\n",
    "                    nmax = format(float(counts[str(cl_est)]) / float(n_true) * 100., '.0f')+'%'\n",
    "                else:\n",
    "                    nmax = '0%'\n",
    "                highlight = colors[contestant]\n",
    "                data = np.genfromtxt(os.path.join('submissions/'+contestant, 'violin'+str(i)+'true'+str(cl_est)+'pred.txt'))\n",
    "                if contestant == '':\n",
    "#                     print((cl_est, data))\n",
    "                    return\n",
    "                if not np.all(np.isclose(data, 0.)):\n",
    "                    data = data / np.max(data)\n",
    "                else:\n",
    "                    data = np.zeros_like(data)\n",
    "                wheremax = np.argmax(data)\n",
    "#                 alpha=0.2*(j+1)\n",
    "                if cl_est == i:\n",
    "                    ax.fill_betweenx(positions, stretchfact*loc - data, stretchfact*loc + data, alpha=0.4, color=highlight, linewidth=0.1, label=contestant)#fontweight='bold', \n",
    "                else:\n",
    "                    ax.fill_betweenx(positions, stretchfact*loc - data, stretchfact*loc + data, alpha=0.4, color=highlight, linewidth=0.1)\n",
    "                if wheremax < len(positions)/4:\n",
    "                    whereprint = 'bottom'\n",
    "                elif wheremax > 3 * len(positions) / 4:\n",
    "                    whereprint = 'top'\n",
    "                else:\n",
    "                    whereprint = 'center'\n",
    "#                 yval = 0.05+positions[wheremax]+0.2*j\n",
    "#                 if contestant != 'validation':\n",
    "                yval = j/5.+0.05 #positions[wheremax] + 0.2*j\n",
    "                ax.text(stretchfact*(loc+0.4), yval, nmax, fontsize=12, color=highlight, ha='center', rotation=0, va='bottom')#whereprint)#fontweight='bold', \n",
    "                loc += 1\n",
    "        midpoint = bisect.bisect(range(len(label_dict.keys())), len(label_dict.keys())/2.)\n",
    "        for cl_est in label_dict.keys():\n",
    "            ticklabels.append(label_dict[cl_est])\n",
    "        if list(label_dict.keys()).index(i) < midpoint:\n",
    "            legloc = 'upper right'\n",
    "        else:\n",
    "            legloc = 'upper left'\n",
    "            \n",
    "        ax.legend(loc=legloc)\n",
    "        ax.set_xticks(stretchfact * np.arange(loc))\n",
    "        ax.set_xticklabels(ticklabels, rotation=45, ha=\"right\")\n",
    "        ax.set_xlabel('predicted class')\n",
    "        ax.set_ylabel('estimated probability')\n",
    "        fig.savefig(os.path.join('submissions/validation', 'compare_violin_'+true_labels[i]+'.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_violins(all_contestants, color_dict)\n",
    "val_violins(contestants[:3]+['validation'], color_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### overall performance\n",
    "\n",
    "This is a violin plot of the diagonal of the confusion matrix, i.e. the distribution of each object's probability of being labeled as its true class.\n",
    "This is closely related to what our final metric probed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diag_violin(contestant):\n",
    "    stretchfact = 2.5\n",
    "    loc = 0\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "    ax.set_title(contestant+' diagonal')\n",
    "    ticklabels = []\n",
    "    others = np.zeros_like(positions)\n",
    "    for one_target in true_labels.keys():\n",
    "        for cl_est in sub_labels.keys():\n",
    "#             print((one_target, cl_est))\n",
    "            if ((len(str(one_target)) == 3) and (str(cl_est) == 99)):\n",
    "#                 print(one_target)\n",
    "                other = np.exp(np.genfromtxt(os.path.join('submissions/'+contestant, 'violin'+str(one_target)+'true'+str(cl_est)+'pred.txt')))\n",
    "                others = others + other\n",
    "                print(others)\n",
    "            elif cl_est == one_target:\n",
    "                data = np.exp(np.genfromtxt(os.path.join('submissions/'+contestant, 'violin'+str(one_target)+'true'+str(cl_est)+'pred.txt')))\n",
    "                data = data / np.max(data)\n",
    "                ax.fill_betweenx(positions, stretchfact*loc - data, stretchfact*loc + data, alpha=0.75, linewidth=0.1, color='k')\n",
    "                ticklabels.append(sub_labels[cl_est])\n",
    "                loc += 1\n",
    "    data = others / np.max(others)\n",
    "    ax.fill_betweenx(positions, stretchfact*loc - data, stretchfact*loc + data, alpha=0.75, linewidth=0.1, color='k')\n",
    "    ticklabels.append(sub_labels[99])\n",
    "    ax.set_xticks(stretchfact * np.arange(loc+1))\n",
    "    ax.set_xticklabels(ticklabels, rotation=45, ha=\"right\")\n",
    "    ax.set_xlabel('true class')\n",
    "    ax.set_ylabel('probability of true label')\n",
    "    fig.savefig(os.path.join('submissions/'+contestant, 'diagonal_violin.png'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for contestant in all_contestants:#['validation']:# + contestants:\n",
    "    diag_violin(contestant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: interpret this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like they're all gaming the leaderboard rather than really classifying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### per-class performance\n",
    "\n",
    "Warning, these are slow!\n",
    "\n",
    "TODO: The violins are normalized so they don't overlap each other in the plots, but it would be more accurate for the area of each to be the same regardless of how far out they extend.\n",
    "Is there a reasonable way to do this?\n",
    "\n",
    "TODO: Consider combining these into subplots of a multipanel plot, one panel per true class and multiple classifiers in each panel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def violins(contestant, one_target, index, class_dict, color='r'):\n",
    "    print('plotting '+contestant+' true '+str(one_target)+' color '+color)\n",
    "    all_prob = pd.read_csv(os.path.join('submissions/'+contestant, 'probvecs'+str(one_target)+'true.csv'), index_col=index)\n",
    "    n_true = len(all_prob)\n",
    "    stretchfact = 2.5\n",
    "    loc = 0\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "    ax.set_title(str(n_true)+' true '+str(true_labels[one_target])+' by '+contestant)\n",
    "    ticklabels = []\n",
    "    maps = all_prob.idxmax(axis=1)\n",
    "    counts = maps.value_counts()\n",
    "#     print(counts.keys())\n",
    "    for cl_est in class_dict.keys():\n",
    "        if str(cl_est) in counts.keys():\n",
    "            nmax = str(counts[str(cl_est)])#str(np.around(counts[str(cl_est)]/float(n_true), 2))\n",
    "        else:\n",
    "            nmax = '0'\n",
    "        if cl_est == one_target or (one_target == 99 and str(cl_est)[:2] == '99'):\n",
    "            highlight = color\n",
    "            textcol = 'k'\n",
    "            alp = 0.75\n",
    "        else:\n",
    "            highlight = 'k'\n",
    "            textcol = color\n",
    "            alp = 0.25\n",
    "#         data = np.exp(np.genfromtxt(os.path.join('submissions/'+contestant, 'violin'+str(one_target)+'true'+str(cl_est)+'pred.txt')))\n",
    "        data = np.genfromtxt(os.path.join('submissions/'+contestant, 'violin'+str(one_target)+'true'+str(cl_est)+'pred.txt'))\n",
    "        if not np.all(np.isclose(data, 0.)):\n",
    "            data = data / np.max(data)\n",
    "        else:\n",
    "            data = np.zeros_like(data)\n",
    "        wheremax = np.argmax(data)\n",
    "        ax.fill_betweenx(positions, stretchfact*loc - data, stretchfact*loc + data, alpha=alp, color=highlight, linewidth=0.1)\n",
    "        if wheremax < len(positions)/4:\n",
    "            whereprint = 'bottom'\n",
    "        elif wheremax > 3 * len(positions) / 4:\n",
    "            whereprint = 'top'\n",
    "        else:\n",
    "            whereprint = 'center'\n",
    "        ax.text(stretchfact*loc, positions[wheremax], nmax, fontsize=12, color=textcol, ha='center', va=whereprint, rotation=90)#fontweight='bold', \n",
    "        loc += 1\n",
    "        ticklabels.append(sub_labels[cl_est])\n",
    "    ax.set_xticks(stretchfact * np.arange(loc))\n",
    "    ax.set_xticklabels(ticklabels, rotation=45, ha=\"right\")\n",
    "    ax.set_xlabel('predicted class')\n",
    "    ax.set_ylabel('probability')\n",
    "#     fig.savefig(os.path.join('submissions/'+contestant, 'violin_'+true_labels[one_target]+'.png'))\n",
    "    savepath = os.path.join('../../plasticc-explorer/images/all_violins', 'violin'+contestant+true_labels[one_target]+'.svg')\n",
    "    print(savepath)\n",
    "    fig.savefig(savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for contestant in contestants:\n",
    "    def help_violins(ind):\n",
    "        one_target = truth.true_target.unique()[ind]\n",
    "#     for one_target in truth.true_target.unique():#label_dict:\n",
    "#         violins(contestant, one_target, 'objids', label_dict, color='#00A878')\n",
    "        violins(contestant, one_target, 'object_id', sub_labels, color=color_dict[contestant])\n",
    "#         return\n",
    "    nps = mp.cpu_count() - 4\n",
    "    pool = mp.Pool(nps)\n",
    "    pool.map(help_violins, range(len(truth.true_target.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for contestant in ['validation']:#contestants:\n",
    "    for one_target in label_dict.keys():#truth.true_target.unique():#label_dict:\n",
    "#         violins(contestant, one_target, 'objids', label_dict, color='#00A878')\n",
    "        violins(contestant, one_target, 'object_id', label_dict, color=color_dict[contestant])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At least for Kyle's submission, classes 6, 15, 16, 53, 64, 65, 88, look pretty darn good, like what we'd expect from the \"perfect\" classifier archetype; class 90 looks more like the \"almost perfect\" or \"noisy\" classifier archetypes.\n",
    "Classes 42 and 62 look like the \"mutually subsuming\" classifier archetype, relative to class 99; class 67 looks like a weaker form of the \"mutually subsuming\" classifier archetype relative to classes 62, 90, and 99, and class 95 is also like that, relative to only class 99.\n",
    "Meanwhile, class 99 looks like the \"mutually subsuming\" classifier archetype relative to classes 42 and 62.\n",
    "Class 52 looks like the \"uncertain\" classifier archetype, with respect to classes 42, 62, 67, 90, and 99.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "\n",
    "TODO: Consider making one plot per predicted class, which sort of conveys a probabilistic notion of false positives, whereas making one plot per true class sort of conveys a probabilistic notion of false negatives.\n",
    "~~This would require splitting up the data files quite differently.~~\n",
    "\n",
    "Besides all the \"TODO\" items, I also want to try some unsupervised clustering on the probability vectors (per true class) to get an idea of the covariances, at least for the weirdos like 42, 52, 62, 67.\n",
    "Actually, we know why this is happening!\n",
    "It's due to the way Kyle generated the probability of being in class 99, a formula that actually uses the probabilities of these classes.\n",
    "In a sense it's a bug that it draws probabilty away from true 42 (SNII) to a label of 99 (other)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCRATCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "still don't know why this cell is so slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run me only if you've never run the notebook before, or if you changed the probabilities at which to evaluate the KDEs.\n",
    "# for contestant in ['validation_DDF', 'validation_WFD']:\n",
    "#     for one_target in label_dict:\n",
    "#         print('evaluating '+contestant+'\\'s KDEs for true class '+str(one_target))#+' with '+str(n_true)+' objects')\n",
    "#         for cl_est in label_dict:\n",
    "#             with open(os.path.join('submissions/'+contestant, 'kernel'+str(one_target)+'true'+str(cl_est)+'pred.pkl'), 'rb') as fn:\n",
    "#                 kernel = pickle.load(fn)\n",
    "#             if kernel is not None:\n",
    "#                 data = kernel(positions)#[:, np.newaxis])\n",
    "#             else:\n",
    "#                 data = np.zeros_like(positions)\n",
    "#             print('evaluated '+contestant+'\\'s KDE for predicted class '+str(cl_est))\n",
    "#             np.savetxt(os.path.join('submissions/'+contestant, 'violin'+str(one_target)+'true'+str(cl_est)+'pred.txt'), data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If data is unavailable, run the following cell to make trivial mock data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M_classes = 13\n",
    "# flat_factor = 1. / M_classes\n",
    "# class_ids = range(0, M_classes)\n",
    "\n",
    "# oom = 4\n",
    "# generator = proclam.simulators.LogUnbalanced()\n",
    "# N_objects = int(10 ** oom)\n",
    "# minitruth = generator.simulate(M_classes, N_objects, base=oom)\n",
    "\n",
    "# mask_tru = det_to_prob(minitruth).astype(int)\n",
    "\n",
    "# starter = 0.5 * np.ones((M_classes, M_classes)) + 1.5 * np.eye(M_classes)\n",
    "# starter = starter / np.sum(starter, axis=1)[:, np.newaxis]\n",
    "# cm = starter\n",
    "\n",
    "# # afflicted = np.random.choice(range(0, M_classes), size=10, replace=False)\n",
    "# cruise = [-1, -2]#[0, 1]#afflicted[2:4]\n",
    "# subsumed = [-3, -4, -6, -7]#[2, 3, 5, 6]#afflicted[4:8]\n",
    "# swapped = [3, 4]#[-4, -5]\n",
    "# tunnel = [-1, -8]#[0, 7]#afflicted[8:]\n",
    "# noisy_cls = [0, 1]#[-2, -1]#afflicted[:2]\n",
    "# uncertain = [2]#[-3]\n",
    "# afflicted = cruise + subsumed + tunnel + noisy_cls\n",
    "\n",
    "# systematic_types = list(reversed([\n",
    "#     'perfect',\n",
    "#     'almost perfect',\n",
    "#     'cruise control by 11',\n",
    "#     'cruise control by 10',\n",
    "#     'almost perfect',\n",
    "#     'subsumed by 10',\n",
    "#     'subsumed by 10',\n",
    "#     'tunnel vision',\n",
    "#     'mutually subsuming',\n",
    "#     'mutually subsuming',\n",
    "#     'uncertain',\n",
    "#     'noisy',\n",
    "#     'noisy'\n",
    "# ]))\n",
    "# plot_systematic_types = list(reversed(systematic_types))\n",
    "\n",
    "# almost = 0.5 * np.ones((M_classes, M_classes)) + 1.5 * np.eye(M_classes)\n",
    "# almost = almost / np.sum(almost, axis=1)[:, np.newaxis]\n",
    "# cm = almost\n",
    "# perfect = np.eye(M_classes) + 1.e-8\n",
    "# cm[tunnel] = perfect[tunnel]\n",
    "# noisy = 0.5 * np.ones((M_classes, M_classes)) + 0.5 * np.eye(M_classes)\n",
    "# noisy = noisy / np.sum(starter, axis=1)[:, np.newaxis]\n",
    "# cm[noisy_cls] = noisy[noisy_cls]\n",
    "# cm[subsumed[-3:]] = cm[cruise[-1]]\n",
    "# cm[subsumed[:-3]] = cm[cruise[-2]]\n",
    "# cm[uncertain] = 1./float(M_classes) * np.ones(M_classes)\n",
    "# cm[swapped[-2]][swapped[-1]] = cm[swapped[-1]][swapped[-1]]\n",
    "# cm[swapped[-1]][swapped[-2]] = cm[swapped[-2]][swapped[-2]]\n",
    "# cm[:, -8] = perfect[:, -8]\n",
    "\n",
    "# cm = cm / np.sum(cm, axis=1)[:, np.newaxis]\n",
    "\n",
    "# fig = plt.figure(figsize=(5,5))\n",
    "# grid = ImageGrid(fig, 111,          # as in plt.subplot(111)\n",
    "#                  nrows_ncols=(1,1),\n",
    "#                  axes_pad=0.05,\n",
    "#                  share_all=True,\n",
    "#                  )\n",
    "# fig.subplots_adjust(wspace=0.5)\n",
    "# ax = grid[0]\n",
    "# im = ax.imshow(cm, vmin=0., vmax=1., cmap=fave_cmap)\n",
    "# ax.set_xticks(range(M_classes))\n",
    "# ax.set_xticklabels(range(1, M_classes+1))\n",
    "# ax.set_yticks(range(M_classes))\n",
    "# ax.set_yticklabels(range(1, M_classes+1))\n",
    "# ax.set_ylabel('true class')\n",
    "# ax.set_xlabel('predicted class')\n",
    "# cbar_ax = fig.add_axes([0.1, 0.89, 0.8, 0.04])\n",
    "# cbar = fig.colorbar(im, cax=cbar_ax, orientation='horizontal', pad=0.05)\n",
    "# cbar_ax.xaxis.set_ticks_position(\"top\")\n",
    "# ax.cax.toggle_label(True)\n",
    "# axp = ax.twinx()\n",
    "# axp.set_ylim(-0.5, M_classes-0.5)\n",
    "# axp.set_yticks(range(0, M_classes))\n",
    "# axp.set_yticklabels(plot_systematic_types)\n",
    "# axp.set_ylabel('systematic effect', rotation=270, labelpad=20)\n",
    "# ax.set_title('realistically complex \\n conditional probability matrix', pad=50)\n",
    "# # plt.savefig('fig/combined.png')\n",
    "# plt.show()\n",
    "# plt.close()\n",
    "\n",
    "# # delfact = -1\n",
    "# # minidelta = 10**delfact\n",
    "# # altdelfact = delfact * 2\n",
    "# # altminidelta = 10**altdelfact\n",
    "\n",
    "# classifier = FromCMDM()\n",
    "# # delta = altminidelta\n",
    "\n",
    "# temp_cm = cm\n",
    "# temp_prob = sanitize_predictions(classifier.classify(temp_cm, minitruth, delta=0.001, other=False))\n",
    "\n",
    "# dets = prob_to_det(temp_prob)\n",
    "# cm = det_to_cm(dets, minitruth)\n",
    "# norm_cm = cm.astype(float).T\n",
    "# norm_cm[norm_cm == 0] = epsilon\n",
    "\n",
    "# fig = plt.figure(figsize=(5,5))\n",
    "# grid = ImageGrid(fig, 111,          # as in plt.subplot(111)\n",
    "#                  nrows_ncols=(1,1),\n",
    "#                  axes_pad=0.05,\n",
    "#                  share_all=True,\n",
    "#                  )\n",
    "# fig.subplots_adjust(wspace=0.5)\n",
    "# ax = grid[0]\n",
    "# im = ax.imshow(norm_cm, vmin=0.01, vmax=len(temp_prob), cmap=fave_cmap, norm=LogNorm())\n",
    "# ax.set_xticks(range(M_classes))\n",
    "# ax.set_xticklabels(pred_class_inds)\n",
    "# ax.set_yticks(range(M_classes))\n",
    "# ax.set_yticklabels(pred_class_inds)\n",
    "# ax.set_ylabel('true class')\n",
    "# ax.set_xlabel('predicted class')\n",
    "# cbar_ax = fig.add_axes([0.1, 0.89, 0.8, 0.04])\n",
    "# cbar = fig.colorbar(im, cax=cbar_ax, orientation='horizontal', pad=0.05)\n",
    "# cbar_ax.xaxis.set_ticks_position(\"top\")\n",
    "# ax.cax.toggle_label(True)\n",
    "# # axp = ax.twinx()\n",
    "# # axp.set_ylim(-0.5, M_classes-0.5)\n",
    "# # axp.set_yticks(range(0, M_classes))\n",
    "# # axp.set_yticklabels(plot_systematic_types)\n",
    "# # axp.set_ylabel('systematic effect', rotation=270, labelpad=20)\n",
    "# ax.set_title('confusion matrix for true '+str(one_target), pad=50)\n",
    "# # plt.savefig('fig/combined.png')\n",
    "# plt.show()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sadly Seaborn can't handle multiple datasets on one set of axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seaborn.violinplot(probs.loc['true class' == 4].iloc[:])#data=per_class[\"true class\" == 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One target at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_target = 52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_prob = pd.read_csv('1_Kyle_'+str(one_target)+'.csv', index_col='object_id')\n",
    "# minitruth = [one_target] * len(all_prob)\n",
    "# # true_ind = list(all_prob.columns.values).index('class_'+str(one_target))\n",
    "# pred_class_inds = {i: int(i[6:]) for i in all_prob.columns.values}\n",
    "# M_classes = len(pred_class_inds)\n",
    "# all_prob.rename(columns=pred_class_inds, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A stacked histogram really doesn't cut it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.title('probability vectors for true '+str(one_target))\n",
    "# probbins = np.linspace(-2., 0., 20)\n",
    "# plt.hist([all_prob[i] for i in all_prob.columns], bins=10.**probbins, density=True, stacked=True, \n",
    "#          label=all_prob.columns, color=[fave_cmap(j/M_classes) for j in range(M_classes)])\n",
    "# plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probs = all_prob.copy()\n",
    "# print((np.min(probs), np.max(probs)))\n",
    "# probs['true class'] = minitruth\n",
    "# # probs = pd.melt(probs, value_vars=[str(i) for i in range(M_classes)], id_vars='predicted class')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PLAsTiCC (Python 3)",
   "language": "python",
   "name": "plasticc_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
